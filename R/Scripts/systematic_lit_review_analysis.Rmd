---
title: "Systematic literature review - Analysis of abstracts"
author:
- name: Vincent Bagilet
  url: https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet
  affiliation: Columbia University
  affiliation_url: https://www.columbia.edu/
- name: LÃ©o Zabrocki
  url: https://www.parisschoolofeconomics.eu/en/
  affiliation: Paris School of Economics
  affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output:
  distill::distill_article: 
    toc: true
    toc_float: true
  html_notebook: default
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = FALSE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 500,
               fig.align = "center")  
```  

```{r include=FALSE}
library(tidyverse)
library(fulltext)
library(tidytext)
library(wordcloud)
library(retrodesign)
library(mediocrethemes)
library(lubridate)
library(DT)
library(kableExtra)
library(skimr)
library(ggridges)

set_mediocre_all()
set.seed(1)
```

# Purpose of the document

<!-- For years, the empirical economic literature has been obsessed with unbiasedness. Researchers have developed empirical methods and techniques in a quest to retrieve unbiased estimates. Yet, due to their inherent constraints, these techniques limit the set of settings in which one can retrieve causal and unbiased estimates. They also often tend to focus on limited samples of the data, leading to a reduction in sample size? This can ultimately lead to underpowered studies. As underlined by Gelman and Carlin (2014), a lack of power is often associated with type M and type S error. In a quest for unbiasedness, practitioners use methods leading to smaller sample size which can ultimately create bias (type M and type S error). We thus aim at analyzing whether the literature of short term health effects of air pollution suffers from power, type M and type S error issues. -->

One of the objectives of our paper is to evaluate whether the literature of short terms health effects of air pollution suffers from power and bias issues. In this document, we carry out part of our analysis. We retrieved part of the estimates and confidence intervals of the literature in [another document](systematic_lit_review_geting_abstracts.html). To do so, we took advantage  using REGular EXPressions (regex) of a somehow standardized reporting mechanism of point estimates and confidence intervals in the abstracts. The set of articles studied in details here is therefore limited to articles describing displaying confidence intervals and point estimates in their abstracts. This convention does not exist in all disciplines. For instance, it is not common practice in the economics literature but it is much more common in the epidemiology literature. This analysis thus focuses on a selected sample of the literature.

In the present document, we first explore the characteristics of the articles considered. We only retrieve effects for a subset of papers and thus assess whether these papers are representative of the literature. We find that these papers are relatively representative of the whole literature. We then briefly explore the effects retrieved before carrying out a power analysis on these effects, implementing sensitivity tests to compute power, type M and type S error. We find a lot of heterogeneity in this literature. Some of papers do not seem to present major problems while others seem to suffer from to severe power issues. We therefore explore potential sources for this heterogeneity.

<!-- write a paragraph explaining why the robustness checks make sense -->

# Articles characteristics

Before diving into the power analysis itself, we look at the characteristics of the articles considered.

We retrieved the articles from PubMed and Scopus using the following query on May 18, 2021:

'TITLE(("air pollution" OR "air quality" OR "particulate matter" OR ozone OR "nitrogen dioxide" OR "sulfur dioxide" OR "PM10" OR "PM2.5" OR "carbon dioxide" OR "carbon monoxide") AND ("emergency" OR "mortality" OR "stroke" OR "cerebrovascular" OR "cardiovascular" OR "death" OR "hospitalization") AND NOT ("long term" OR "long-term")) AND "short term"'

```{r}
abstracts_and_metadata <- readRDS("../Outputs/abstracts_and_metadata.RDS")
estimates <- readRDS("../Outputs/estimates.RDS")
```

This query enables us to retrieve `r nrow(abstracts_and_metadata)` valid abstracts.

## Themes

We can briefly explore the main (unsurprising) themes of the articles:

```{r}
abstracts_and_metadata %>%
  unnest_tokens(word, abstract, to_lower = TRUE) %>% 
  anti_join(tidytext::stop_words, by = "word") %>%
  count(word) %>%
  with(wordcloud::wordcloud(word, n, max.words = 80, random.color = TRUE, colors = "#00313C"))
```

```{r fig.asp = 1}
abstracts_and_metadata %>%
  unnest_tokens(word, abstract, to_lower = TRUE) %>%
  anti_join(tidytext::stop_words, by = "word") %>%
  select(doi, word) %>%
  distinct() %>%
  count(word, sort = TRUE) %>%
  filter(n > 650) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(
    y = NULL,
    title = "Main themes in the abstracts",
    subtitle = "Droping usual stop words",
    x = "Number of abstracts containing this word"
  ) 
```

## Detection of effects

<!-- Not all abstracts display effects and confidence intervals. We thus want to assess whether there are noticeable differences between articles for which we retrieve confidence intervals and those for which we do not. This quick exploration will also provide additional information and descriptive statistics on the whole set of articles. -->

```{r}
string_confint <- str_c(
  "((?<!(\\d\\.|\\d))95\\s?%|(?<!(\\d\\.|\\d))95\\s(per(\\s?)cent)|",
  "\\bC(\\.)?(I|l)(\\.)?(s?)\\b|\\bPI(s?)\\b|\\b(i|I)nterval|",
  "\\b(c|C)onfidence\\s(i|I)nterval|\\b(c|C)redible\\s(i|I)nterval|", 
  "\\b(p|P)osterior\\s(i|I)nterval)"
  )

effect_detected <- abstracts_and_metadata %>%
  left_join(estimates, by = "doi") %>% 
  mutate(
    contains_CI = str_detect(abstract, string_confint)
  ) %>%
  group_by(title) %>% 
  mutate(
    has_effect = (sum(!is.na(effect), na.rm = TRUE) > 0),
    has_effect_phrase = ifelse(has_effect, "Effect retrieved", "No effect retrieved")
  ) %>% 
  ungroup() %>% 
  select(-effect, -low_CI, -up_CI) %>% 
  distinct()
```

Out of the `r nrow(abstracts_and_metadata)` valid articles returned by the query only a fraction reports estimates and confidence intervals in their abstracts. We therefore do not retrieve effects for all the articles returned by the query, mostly for the reason aforementioned. This might create some selection, making the sample of articles studied not representative of the whole literature. We investigate further the difference between articles that do and do not report confidence intervals in their abstracts in the following section. 

Here is a list of the valid articles. The last column shows whether at least one effect was retrieved for this article.

```{r}
effect_detected %>% 
  select(authors, pub_date, title, journal, has_effect_phrase) %>% 
  datatable(
    colnames = c(
      "Authors", 
      "Publication date", 
      "Title", 
      "Journal", 
      "Effect retrieved"
    )
  )
```

In total, we **detect `r nrow(estimates)` valid effects** and associated confidence intervals. We retrieve estimates for most of the articles mentioning "CI" in the abstract:  

```{r}
effect_detected %>%
  filter(contains_CI) %>% 
  count(has_effect) %>% 
  mutate(
    prop = n/sum(n),
    has_effect = ifelse(has_effect, "Yes", "No")
  ) %>% 
  arrange(desc(has_effect)) %>% 
  kable(
    col.names = c("Effect retreived", "Number of articles", "Proportion"),
    caption = "Number of articles for which at least one effect is retrieved (out of those containing the phrase 'CI')"
  ) %>%
  kable_styling(position = "center")
```

Note that a bunch of abstracts contain the phrase "CI" without actually displaying effects and confidence intervals. Our algorithm seems to make a reasonably good job at detecting effects and CI when they are indeed displayed in an abstract. In addition, there is no reason to think that our ability to detect an effect would be correlated with power issues in the paper. Hence, we feel rather confident assuming that our detection algorithm selects a random (along our dimension of interest) sample of estimates among all estimates displayed in abstracts.

## Representativity of articles for which an effect was retrieved

In this subsection, we investigate whether there are systematic differences between articles for which we retrieved an effect and articles that do not display an effect in their abstract or for which we did not detect one. We build this analysis such that it also provides general information about the entire set of articles. 

### Qualitative random analysis

First of all, we skim through a bunch of abstracts for which we retrieve an effect or not to see whether there are clear differences across study subsets. We notice that


<!-- ```{r} -->
<!-- effect_detected %>%  -->
<!--   filter(has_effect) %>%  -->
<!--   slice_sample(n = 3) %>%  -->
<!--   .$abstract %>%  -->
<!--   str_view_all("~") -->
<!-- ``` -->


### Publication date

We first look into the distribution of published articles on this topic in time. We then wonder whether displaying effects in the abstract was a particular feature of a given period.

```{r}
effect_detected %>% 
  # filter(contains_CI) %>% 
  ggplot() +
  geom_density(aes(x = year(pub_date), fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of publication year",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Publication year",
    y = "Density"
  ) + 
  scale_fill_discrete(name = "")

year_first_article <- effect_detected %>% 
  filter(has_effect) %>% 
  .$pub_date %>% 
  year() %>% 
  min(na.rm = TRUE)
```

First, we notice that the number of articles published on short-term health effects of air pollution has been increasing rather strongly since the 1980s. The first article for which an effect is detected was published in `r year_first_article`. We only found `r effect_detected %>% filter(year(pub_date) < year_first_article) %>% count() %>% .$n` articles published before `r year_first_article`. This can be explained by the fact that, in most places, air pollution has only been measured consistently since the 1990s.

Even though there are slightly more recent (2010-2020) articles for which effects are retrieved, the difference does not seem to be substantial. Distributions of articles for which an effect has been retrieved and not are rather similar.

### Journal and fields

We then look into the journals and academic fields in which articles on short term health effect of air pollution have been published. The results by journals are rather messy so we focus on journal areas and subareas. 

```{r fig.asp=1}
effect_detected %>% 
  mutate(
    subject_area = ifelse(
      is.na(subject_area), 
      "Subject unknown", 
      subject_area
    )
  ) %>% 
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(subject_area)), fill = has_effect_phrase)) +
  labs(
    x = "Number of articles published in journals from each subject",
    y = NULL,
    title = "Journals subjects in which articles have been published",
    subtitle = "Comparison between articles for which an effect is retrieved and not"
  ) +
  scale_fill_discrete(name = "") 

effect_detected %>%  
  unnest(subsubject_area) %>% 
  mutate(
    subsubject_area = ifelse(
      is.na(subsubject_area), 
      "Subsubject unknown", 
      subsubject_area
    )
  ) %>% 
  mutate(subsubject_area = fct_lump_n(subsubject_area, 20)) %>%
  filter(subsubject_area != "Other") %>%
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(subsubject_area)), fill = has_effect_phrase)) +
  labs(
    x = "Number of articles published in journals covering a given subsubject",
    y = NULL,
    title = "Main journals subsubjects in which articles have been published",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    caption = "A paper publsihed in a multi-subject journal will appear several times"
  ) +
  scale_fill_discrete(name = "") 
```

Most papers on this topic have been, unsurprisingly, published in multidisciplinary journals, health or physical science journals.

One may notice that effects are not retrieved, *ie* not reported in the abstract or not detected, for most papers published in life science and social sciences and humanities. This might not be as problematic as they constitute a small share of the sample. There does not seem to be a particularly large imbalance in terms of journal general field for the more represented fields.

### Themes

We then wonder if the words used in each sets of abstracts differ between the two sets of articles.

```{r fig.asp=1}
theme_effect <- effect_detected %>%
  unnest_tokens(word, abstract, to_lower = TRUE) %>% 
  anti_join(tidytext::stop_words, by = "word") %>% 
  group_by(word, has_effect) %>%
  mutate(n_articles_word = length(unique(doi))) %>% 
  ungroup() %>% 
  select(word, has_effect, has_effect_phrase, n_articles_word) %>% 
  distinct() %>% 
  group_by(word) %>% 
  mutate(tot_n_articles_word = sum(n_articles_word, na.rm = TRUE)) %>% 
  ungroup() %>% 
  filter(tot_n_articles_word > 600) %>% 
  ungroup() %>% 
  # mutate(has_effect = ifelse(has_effect, "Effect detected", "Effect not detected")) %>% 
  mutate(word = reorder(word, n_articles_word)) 

theme_effect %>% 
  ggplot(aes(x = word, fill = has_effect_phrase)) +
  geom_col(
    data = subset(theme_effect, has_effect), 
    aes(y = n_articles_word)
  ) +
  geom_col(
    data = subset(theme_effect, !has_effect),
    aes(y = -n_articles_word),
    position = "identity"
  ) +
  scale_y_continuous(labels = abs) +
  coord_flip() +
  labs(
    x = NULL,
    y = "Number of abstracts containing a given word",
    title = "Words appearing in the more abstracts",
    subtitle = "Comparison between articles with and without detected effect"
  ) +
  scale_fill_discrete(name = "")
```

Apart from a few key terms, such as CI, 95 for instance, there are no huge differences in the terms used in both subsets of abstracts. 

### Pollutant

We take look at the pollutants considered in each article. We consider that a pollutant is studied if it is mentioned in the abstract. It is not an exact measure as some articles may mention pollutants without actually studying them but it remains an interesting metric.

```{r fig.asp=0.7}
effect_detected %>% 
  unnest(pollutant) %>% 
  mutate(pollutant = ifelse(is.na(pollutant), "Pollutant not detected", pollutant)) %>% 
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(pollutant)), fill = has_effect_phrase)) +
  labs(
    x = "Number of article mentionning each pollutant",
    y = NULL,
    title = "Number of articles studing a given pollutant",
    subtitle = "Comparison between articles for which an effect was detected or not",
    caption = "A paper mentioning several pollutants will appear several times"
  ) +
  scale_fill_discrete(name = "")
```
First of all, we notice that a large share of papers considered here study particulate matters (PM2.5, PM10 or both). 

It seems that, when there are enough articles, the likelihood of detecting an effect does not seem to vary much with the type of pollutant. Importantly, the proportion of effects retrieved is much lower for articles for which we are not able to detect the type of pollutants studied. 

### Outcome

As for pollutants, for some articles, we were able to retrieve studied outcomes depending on the words used in an abstract. We classified them into two categories: mortality and emergency.

```{r fig.asp=0.7}
effect_detected %>% 
  unnest(outcome) %>% 
  mutate(outcome = ifelse(is.na(outcome), "Outcome not detected", outcome)) %>% 
  mutate(has_effect = ifelse(has_effect, "Effect detected", "Effect not detected")) %>% 
  ggplot() +
  geom_bar(aes(x = outcome, fill = has_effect), position = "dodge") +
  labs(
    x = NULL,
    y = "Number of articles",
    title = "Number of articles studing a given outcome",
    subtitle = "Comparison between articles for which an effect was detected or not - articles with unknown outcomes dropped"
  ) +
  scale_fill_discrete(name = "") 
```

Most articles studied here are interested in mortality. The proportion of articles for which an effect is retrieved seems to be larger for papers studying emergency admissions than mortality. 

### Subpopulation

Some articles focus on sub-populations such as infants or elderly. We are able to detect a fraction of these articles, when these terms are mentioned in the abstract. When these terms are not mentioned, either the entire population is considered or we are not able to detect the subgroup considered. The number of articles for which a subpopulation is indicated is rather small: 

```{r}
effect_detected %>% 
  unnest(subpop) %>% 
  group_by(doi) %>% 
  mutate(missing_subpop = is.na(subpop)) %>% 
  ungroup() %>% 
  select(doi, missing_subpop) %>% 
  distinct() %>% 
  count(missing_subpop = missing_subpop) %>%
  mutate(missing_subpop = ifelse(missing_subpop, "No or unknown", "Yes")) %>% 
  kable(col.names = c("Subpopulation indicated", "Number of articles")) %>%
  kable_styling(position = "center")
```

Looking more in details into the detection of effects, we get the following pattern:

```{r fig.asp=0.7}
effect_detected %>% 
  unnest(subpop) %>% 
  mutate(subpop = ifelse(is.na(subpop), "No subpopulation not detected", subpop)) %>% 
  mutate(has_effect = ifelse(has_effect, "Effect detected", "Effect not detected")) %>% 
  ggplot() +
  geom_bar(aes(x = subpop, fill = has_effect), position = "fill") +
  labs(
    x = NULL,
    y = "Proportion of articles",
    title = "Proportion of articles studing a given outcome",
    subtitle = "Comparison between articles for which an effect was detected or not - articles with unknown outcomes dropped"
  ) +
  scale_fill_discrete(name = "") 
```
There does not seem to be large variations in the proportion of articles for which an effect is detected, depending on whether a subpopulation is studied or not. Yet, this proportion is a slightly larger for elders than infants, itself larger than when when no subpopulation is detected.

### Number of observations

We then look at the number of observations, the length of the the study period and the number of cities considered. Importantly, we only retrieve this information for a very limited subset of articles. 

```{r}
len <- effect_detected %>% 
  count(Missing = is.na(length_study)) %>% 
  rename(`Length of the study` = n)

cities <- effect_detected %>% 
  count(Missing = is.na(n_cities)) %>% 
  rename(`Number of cities` = n)

obs <- effect_detected %>% 
  count(Missing = is.na(n_obs)) %>% 
  rename(`Number of observations` = n)

len %>% 
  full_join(cities) %>% 
  full_join(obs) %>% 
  mutate(Missing = str_to_title(Missing)) %>% 
  kable() %>%
  kable_styling(position = "center")
```

Our analysis is therefore to be taken with caution as there is a critical lack of information for this category.

```{r fig.asp=0.7}
effect_detected %>% 
  ggplot() +
  geom_density(aes(x = n_obs, fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of the number of observations",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated number of observations",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")

effect_detected %>% 
  ggplot() +
  geom_density(aes(x = length_study, fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of the length of the study",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated length of the study (in days)",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")

effect_detected %>% 
  ggplot() +
  geom_density(aes(x = n_cities, fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of the number of cities in the study",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated number of cities",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")
```

We notice that there are large variations in the number of observations in the studies considered. However, there does not seem to be large differences along this dimension on whether an effect is retrieved or not. There seems however to be more studies around 1000 observation and less between 10,000 and 100,000 in articles for which an effect is retrieved. This is explained by the fact effects are retrieved more for studies with rather limited study period (around 3 years).

Now that we have quickly compared the articles for which we retrieve an effect an those for which we do not, we can dig further into the analysis of the estimates retrieved.

# Analysis of the effects

In this section, we briefly analyze the effects retrieved, their statistical significance and their precision.

## Significance

First, we notice that most of the effects retrieved here are significant (at the usual 5% threshold).

```{r}
estimates_stats <- estimates %>%
  filter(!is.na(effect)) %>% 
  mutate(
    significant = (low_CI > 0 | up_CI < 0), 
    signal_noise = abs(effect/(up_CI - low_CI)),
    se = abs(up_CI - low_CI)/(2*1.96), 
    t_score = abs(effect)/se
  ) 

estimates_stats %>% 
  count(significant) %>% 
  mutate(prop = n/sum(n)) %>% 
  mutate(significant = ifelse(significant, "Yes", "No")) %>% 
  kable(col.names = c("Significant", "Number of effects", "Proportion")) %>%
  kable_styling(position = "center")
```

Researchers mention their key findings in the abstract and therefore probably do not report non statistically significant estimates for which the null hypothesis of no effect cannot be rejected in their abstracts. Only a very small proportion of articles do not report any statistically significant estimates in their abstract:

```{r}
estimates_stats %>% 
  group_by(doi) %>% 
  mutate(has_significant = (sum(significant) > 0) ) %>% 
  ungroup() %>% 
  select(has_significant, doi) %>% 
  distinct() %>% 
  count(has_significant) %>% 
  mutate(prop = n/sum(n)) %>% 
  mutate(has_significant = ifelse(has_significant, "Yes", "No")) %>% 
  kable(col.names = c("At least one significant estimate", "Number of articles", "Proportion")) %>%
  kable_styling(position = "center")
```

## T-scores

We then look into the distribution of the t-scores. 

```{r}
estimates_stats %>%
  filter(t_score < 10) %>%
  ggplot() +
  geom_histogram(aes(x = t_score), bins = 50) +
  geom_vline(xintercept = 1.96) +
  labs(
    title = "Distribution of the t-score in estimates of the literature",
    subtitle = "Only considering observations with a t-score lower than 10",
    caption = "The vertical line represents the usual 1.96 threshold",
    x = "t-score",
    y = "Count"
  ) 
```

There seems to be some sort of bunching for t-scores above 1.96. In this analysis, we only consider estimates reported in the abstracts. Authors may only report significant estimates in their abstracts even though they also report non significant estimates in the body of the article. This might explain this bunching. We need to investigate this further in order to understand whether this bunching is evidence of publication bias. To do so, we could reproduce the present analysis but analyze the full texts and not only on the abstracts. 

## Signal to noise ratio
 
We then plot the distribution of the signal to noise ratio, *ie* the ratio of the point estimate and the width of the confidence interval.

```{r}
estimates_stats %>%
  filter(signal_noise < 4) %>%
  ggplot() +
  geom_histogram(aes(x = signal_noise), bins = 50) +
  geom_vline(xintercept = 0.5)  +
  labs(
    title = "Distribution of the signal to noise ration in estimates of the literature",
    subtitle = "Only considering observations with a signal to noise ration lower than 4",
    caption = "The vertical line represents the usual 0.5 threshold",
    x = "Signal to noise ration",
    y = "Count"
  ) 
```

The graph is of course analogous to the previous one. It however underlines that in a large share of the studies, the magnitude of the noise is larger than the magnitude of the effect. We then look into the distribution of the signal to noise ratio in more details.

```{r}
quantile(estimates_stats$signal_noise, seq(0, 1, 0.1)) %>%
  tidy() %>%
  select(x, names) %>%
  kable(
    col.names = c(
      "Signal to noise ratio",
      "Percentage of estimates with a lower signal to noise ratio"
    )
  ) %>%
  kable_styling(position = "center")
```

We notice that for about 55% of the estimates considered here, the magnitude of the noise is more important than those of the signal. This is particularly concerning.

# Power analysis

We then turn to the power analysis itself. The objective is to evaluate the power, type M and type S errors for each estimate. 

To compute these values, we would need to know the true effect size. Yet, true effects are of course unknown. One solution could be to use estimates from the literature and meta-analyses as best guesses for these true values. Yet, in the setting of this systematic literature review, detecting what is exactly measured in each analysis is particularly challenging since there is no standardized way of reporting the results beyond mentioning confidence intervals. One study may for instance claim that a 10 $\mu g/m^{3}$ increase in PM2.5 concentration leads to an increase of x% in hospital admissions over the course of a year while another study may state that a 2% increase in ozone concentration increases the number of deaths by 3 over a month. Fortunately, for each estimate retrieved, even though we do not know what is measured, we can evaluate the precision with which it is estimated.

To circumvent the fact that we do not know the actual effect size, we follow a strategy suggested by Gelman and Carlin (2014). We consider a range of potential "true" effect sizes and run a sensitivity analysis. We investigate what would be the power, type M and type S error if the true effect was only a fraction of the measured effect. This enables us to assess whether the design of the study would be *good enough* to detect a smaller effect. If assuming that the true effect is 3/4 of the measured effect yields a power of 30%, there is probably a major issue with the design of this study. With this design, this (non zero) effect would only be detected 30% of the time.

Of course, there is no reason to think *a priori* that a given effect would be overestimated. The values for power, type M and type S errors are therefore only informative. 

To carry out the analysis, we use the package `retrodesign` which computes post analysis design calculations (power, type M and type S errors). We run the function `retro_desing()` for several effect sizes.

```{r}
retro_analysis <- estimates_stats %>% 
  mutate(
    retro_low = as_tibble(retro_design(low_CI, se)),
    retro_0.01 = as_tibble(retro_design(effect*0.01, se)),
    retro_0.05 = as_tibble(retro_design(effect*0.05, se)),
    retro_0.1 = as_tibble(retro_design(effect*0.1, se)),
    retro_0.33 = as_tibble(retro_design(effect*0.33, se)),
    retro_0.5 = as_tibble(retro_design(effect*0.5, se)),
    retro_0.67 = as_tibble(retro_design(effect*0.67, se)),
    retro_0.75 = as_tibble(retro_design(effect*0.75, se)),
    retro_0.9 = as_tibble(retro_design(effect*0.9, se)),
    retro_1 = as_tibble(retro_design(effect*1, se))
  ) %>% 
  pivot_longer(
    starts_with("retro"), 
    names_to = "prop_true_effect", 
    values_to = "computed"
  ) %>% 
  mutate(
    power = computed$power,
    typeS = computed$typeS,
    typeM = computed$typeM,
    prop_true_effect = as.numeric(str_sub(prop_true_effect, 7, nchar(prop_true_effect))),
    prop_true_effect_phrase = str_c(prop_true_effect*100, "% of the measured effect")
  ) %>% 
  select(-computed) %>% 
  filter(typeM < Inf) 
```

In a first part, we investigate whether the literature might be subject to design issues. We analyze the distribution of power, type M and type S errors in the whole set of abstracts, how they evolve jointly or along several variables such as the size of the true effect. We find clear evidence of heterogeneity across articles. Some of them seem to present robust designs while others seem much more problematic, yield low power and high rates of type M error, even for large hypothesized true effect sizes.
<!-- The former set of estimates display high power even for low hypothesized true effect sizes while estimates from the later set present low power for still large true effect sizes.  -->
Thus, in a second part, we investigate potential sources of heterogeneity. 

### Distribution of design statistics in the literature

#### Graphical analysis

We first explore graphically the distribution of power, type M and type S error across simulation and for different magnitudes of the true effect.

```{r}
sizes_to_display <- c(0.33, 0.5, 0.75)

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of power in the literature", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect"
  )
# geom_density()

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = typeM)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of type M error in the literature (log scale)", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect"
  ) +
  scale_x_continuous(trans = 'log10')

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = typeS)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of type S error in the literature", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect"
  )
```

A large chunk of articles display high power and low rates of type M and type S error, in each robustness check. However, a non negligible number of articles display lower power and/or some evidence of type M error. Type S error does not seem to be an important issue in this literature. 

Note that for type M errors, due to some outliers, we used a log scale. Without this log scale and restricting our sample to type M errors lower than 2.5 (95% of our sample, even when we assume that the true effect is only 1/3 of the estimated one), we get the following graph:

```{r}
retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  filter(typeM < 2.5) %>%
  ggplot(aes(x = typeM)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of type M error in the literature", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect"
  ) 
```

We find that, even if the measured effect was close to the true effect, for a good chunk of articles, there  would be a substantial risk of type M error. Replicating this study would yield to an overestimation of the effect. 

Alternatively, we can also look at what would be the power, type M and type S if the true effect was equal to the lower bound of the confidence interval. This metric may exacerbate design issues for imprecise estimates. Such estimates would be "penalized" twice for being imprecise: they have a large standard error and the hypothesized effect would be very small. Yet, this metric remains interesting as confidence intervals describe credible effect sizes. If the effect size is smaller but still credible, would the design be good enough to "detect" it?

```{r}
retro_analysis %>% 
  filter(typeM < 5) %>% 
  filter(is.na(prop_true_effect)) %>% 
  mutate(prop_true_effect = "low_CI") %>% 
  pivot_longer(power:typeM, names_to = "design_stat") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ design_stat, scales = "free") +
  labs(
    x = "", 
    y = "Number of estimates",
    title = "Distribution of design statistics in the literature", 
    subtitle = "If the magnitude of true effect is equal to the lower bound of the CI"
  ) 
```

Unsurprisingly, the conclusions remain similar: a non negligible chunk of articles seems to suffer from power issues. 

#### Table

We then look more precisely at actual numbers, looking at the median and mean power, type M and type S errors for different true effect sizes.

```{r}
retro_analysis %>% 
  # filter(typeM < Inf) %>%
  group_by(prop_true_effect, prop_true_effect_phrase) %>% 
  summarise(
    mean_power = mean(power, na.rm = TRUE),
    median_power = median(power, na.rm = TRUE),
    mean_typeM = mean(typeM, na.rm = TRUE),
    median_typeM = median(typeM, na.rm = TRUE),
    mean_typeS = mean(typeS, na.rm = TRUE),
    median_typeS = median(typeS, na.rm = TRUE), 
    .groups = "drop"
  ) %>% 
  arrange(prop_true_effect) %>% 
  select(-prop_true_effect) %>%
  mutate(
    prop_true_effect_phrase = ifelse(
      is.na(prop_true_effect_phrase), 
      "Lower bound of the CI", 
      prop_true_effect_phrase)
  ) %>% 
  kable(col.names = c("", "Mean", "Median", "Mean", "Median", "Mean", "Median")) %>%
  add_header_above(c('"True" effect' = 1, "Power" = 2, "Type M" = 2, "Type S" = 2)) %>%
  kable_styling(position = "center")
```

#### ECDF

Empirical Cumulative Distribution Functions (ECDFs) provide a broader overview of these metrics.  They inform us of the share of articles which power, type M or S errors are below a given threshold. 

```{r}
retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power)) +
  stat_ecdf(geom = "line") +
  facet_wrap( ~ prop_true_effect_phrase) +
  geom_vline(xintercept = 0.8, linetype = "dashed", size = 0.3) +
  coord_flip() +
  labs(
    title = "Empirical cumulative distribution of power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Power",
    y = "Proportion of estimates", 
    caption = "The dashed line represents the usual 80% power threshold"
  )

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  filter(typeM < 2.5) %>%
  ggplot(aes(x = typeM)) +
  stat_ecdf(geom = "line") +
  # coord_flip() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Empirical cumulative distribution of power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Type M error",
    y = "Proportion of estimates"
  )

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = typeS)) +
  stat_ecdf(geom = "line") +
  # coord_flip() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Empirical cumulative distribution of power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Type S error",
    y = "Proportion of estimates"
  ) 
```

We notice that about 40% of estimates would not reach the conventional 80% statistical power level if the true effect is 3/4 the size of the measured effect. This is rather concerning.

For ECDFs too we look at what would be the power, type M and type S error if the true effect was equal to the lower bound of the confidence interval.

```{r}
retro_analysis %>% 
  filter(typeM < 5) %>%
  filter(is.na(prop_true_effect)) %>% 
  mutate(prop_true_effect = "low_CI") %>% 
  pivot_longer(power:typeM, names_to = "design_stat") %>% 
  ggplot(aes(x = value)) +
  stat_ecdf(geom = "line") +
  # geom_hline(yintercept = 0.8) +
  # coord_flip() +
  facet_wrap( ~ design_stat, scales = "free") +
  labs(
    x = "", 
    y = "Proportion of estimates",
    title = "Empirical cumulative distribution of design statistics in the literature", 
    subtitle = "If the magnitude of true effect is equal to the lower bound of the CI"
  ) 
```

#### Joint evolution

Then, we look how type M and type S error evolve with power for the estimates considered.

```{r}
retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power, y = typeM)) +
  geom_point() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Link between type M error and power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Power",
    y = "Type M"
  )

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power, y = typeS)) +
  geom_point() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Link between type S error and power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Power",
    y = "Type S"
  )
```

There is a one-to-one relationship between power and type M and type S error. Not surprisingly, type M and type S error skyrocket in studies with low power. 

We then investigate how average power, type M and type S evolve as a proportion of the true effect size.

```{r}
retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(Power = mean(power, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = Power)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  geom_hline(yintercept = 0.8, size = 0.5, linetype = "dashed", alpha = 0.7) +
  labs(
    title = "Evolution of average power as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    caption = "The dashed line represents the usual 80% power threshold"
  ) +
  ylim(0,1)

retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(typeM = mean(typeM, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = typeM)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of average type M error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type M"
  )

retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(typeS = mean(typeS, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = typeS)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of average type S error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type S"
  )
```

Power, decreases and type M and type S errors skyrocket for small values of the true effect (as a proportion of the measured effect). In addition on average, if for each paper of the literature, the true effects are 3/4 of the measured effect, the power would be lower than the usual 80%. Type S error only seem to be an issue for small values of the true effect as a portion of the measured effect. Type M error seems to be more consistently problematic. The shoot up in the previous graph makes it difficult to read the values of type M error when the true effect is not a small portion of the measured effect. We therefore zoom in.

```{r}
retro_analysis %>%
  filter(prop_true_effect > 0.25) %>%
  group_by(prop_true_effect) %>%
  summarise(typeM = mean(typeM, na.rm = TRUE), .groups = "drop_last") %>%
  ggplot(aes(x = prop_true_effect, y = typeM)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of average type M error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type M") +
  ylim(1, 2.2)
```

We notice that, on average in the literature, the treatment effects are overestimated, even for large values of the true effect. This result might be linked to some outliers. We thus look at the evolution of the median effect with true effect size.

```{r}
retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(Power = median(power, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = Power)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  geom_hline(yintercept = 0.8, size = 0.5, linetype = "dashed", alpha = 0.7) +
  labs(
    title = "Evolution of median power as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect"
  ) +
  ylim(0,1)

retro_analysis %>% 
  filter(prop_true_effect > 0.25) %>%
  group_by(prop_true_effect) %>% 
  summarise(typeM = median(typeM, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = typeM)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of the median type M error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type M"
  ) +
  ylim(1, 2.2)
```

We notice that the issue is much less important when looking at the median. This suggests some heterogeneity in terms of power in the literature.

To confirm that, we look into the evolution of the distribution with the proportion of effect size.

```{r fig.asp=1}
retro_analysis %>% 
  ggplot(aes(x = power, y = factor(prop_true_effect))) +
  geom_density_ridges(color = "#00313C", fill = "#00313C", alpha = 0.3) +
  labs(
    title = "Evolution of distrubtion of power as a function of the 'true effect'", 
    x = "Power", 
    y = "True effect as a proportion of the measured effect"
  ) 

retro_analysis %>% 
  filter(typeM < 5) %>% 
  ggplot(aes(x = typeM, y = factor(prop_true_effect))) +
  geom_density_ridges(color = "#00313C", fill = "#00313C", alpha = 0.3) +
  labs(
    title = "Evolution of distrubtion of type M error as a function of the 'true effect'", 
    x = "Type M error", 
    y = "True effect as a proportion of the measured effect"
  )

# retro_analysis %>% 
#   ggplot(aes(x = typeS, y = factor(prop_true_effect))) +
#   geom_density_ridges(color = "#00313C", fill = "#00313C", alpha = 0.3) +
#   labs(
#     title = "Evolution of distrubtion of type M error as a function of the 'true effect'", 
#     x = "Type S error", 
#     y = "True effect as a proportion of the measured effect"
#   )
```

The overal distribution of power seems almost bimodal: either the power of most is very high or it is very low.

It might also be interesting to look at how power, type M and type S error evolved in time, *ie* with publication date.

```{r}
retro_analysis_pubdate <- retro_analysis %>% 
  left_join(abstracts_and_metadata %>% select(doi, pub_date), , by = "doi")

retro_analysis_pubdate %>% 
  filter(prop_true_effect %in% sizes_to_display) %>% 
  group_by(year = year(pub_date), prop_true_effect) %>%
  mutate(power = mean(power, na.rm = TRUE)) %>% 
  ungroup() %>% 
  ggplot(aes(x = year, y = power)) +
  geom_point() +
  geom_smooth(size = 0.3) +
  facet_wrap(~ prop_true_effect_phrase) +
  labs(
    title = "Evolution of mean power in the literature with publication date", 
    x = "Publication date", 
    y = "Power"
  )

retro_analysis_pubdate %>% 
  filter(prop_true_effect %in% sizes_to_display) %>% 
  group_by(year = year(pub_date), prop_true_effect) %>%
  mutate(typeM = mean(typeM, na.rm = TRUE)) %>% 
  ungroup() %>% 
  ggplot(aes(x = year, y = typeM)) +
  geom_point() +
  geom_smooth(size = 0.3) +
  # geom_smooth(size = 0.3, formula = yÂ ~ x + I(x^2)) +
  facet_wrap(~ prop_true_effect_phrase) +
  labs(
    title = "Evolution of mean type M error in the literature with publication date", 
    x = "Publication date", 
    y = "Type M"
  )

retro_analysis_pubdate %>% 
  filter(prop_true_effect %in% sizes_to_display) %>%  
  group_by(year = year(pub_date), prop_true_effect) %>%
  mutate(typeS = mean(typeS, na.rm = TRUE)) %>% 
  ungroup() %>% 
  ggplot(aes(x = year, y = typeS)) +
  geom_point() + 
  geom_smooth(size = 0.3) +
  facet_wrap(~ prop_true_effect_phrase) +
  labs(
    title = "Evolution of mean type S error in the literature with publication date", 
    x = "Publication date", 
    y = "Type S"
  )
```

There does not seem to be a clear trend in the evolution of power and type S error. However, type M error seems to have peaked in the 2010s and to be decreasing again recently.

# Analysis of the sources of heterogeneity

In the previous section, we noticed that a non negligible number of studies seemed to suffer from a low power issue and associated type M error. We consider that an estimate has low power if its computed power is lower than 80% if the true effect is 3/4 of the measured effect. 80% is the threshold usually used in power analyses but 3/4 is arbitrary and could be changed easily in a robustness check. Following this criterion, the number and proportion of estimates with low power is as follows:

```{r}
articles_low_adequate <- retro_analysis %>% 
  filter(prop_true_effect == 0.75) %>% 
  mutate(
    low_power = (power <= 0.8), 
    low_power_phrase = ifelse(low_power, "Low power", "Adequate power")
  ) %>% 
  left_join(abstracts_and_metadata, by = "doi")

articles_low_adequate %>% 
  count(low_power_phrase) %>% 
  mutate(prop = n/sum(n)) %>% 
  kable(col.names = c("Power", "Number of estimates", "Proportion")) %>%
  kable_styling(position = "center")
```
<!-- Non surprisingly, most of the type M and type S errors are associated with "low power" -->

<!-- ```{r} -->
<!-- retro_analysis %>%  -->
<!--   filter(prop_true_effect %in% c(0.5, 0.75, 1)) %>%   -->
<!--   filter(typeM < 2.5) %>%  -->
<!--   mutate(low_power = ifelse(low_power, "Low power", "Adequate power")) %>%  -->
<!--   ggplot(aes(x = typeM, fill = low_power)) +  -->
<!--   geom_histogram(bins = 10) + -->
<!--   facet_wrap(~ prop_true_effect) + -->
<!--   labs(title = "Distribution of type M error in the literature", subtitle = "If the magnitude of true effect is a fraction of the measured effect") + -->
<!--   scale_fill_discrete(name = "") -->
<!-- ``` -->

We investigate the particularities of the articles with low power. We start by reproducing the analyses used to compare articles for which we retrieved an effect and those for which we did not. First, we look into the distribution of publication dates.

```{r}
set_mediocre_all(second_pair = TRUE)

articles_low_adequate %>% 
  ggplot() +
  geom_density(
    aes(x = year(pub_date), fill = low_power_phrase, color = low_power_phrase),
    alpha = 0.4
  ) +
  labs(
    title = "Distribution of publication year", 
    subtitle = "Comparison between articles for with low and adequate power", 
    x = "Publication year", 
    y = "Density"
  ) +
  scale_fill_discrete(name = "")
```

It seems that less articles with low power have been published recently, in comparison to articles with adequate power. This confirms our previous finding. We then look into the distribution of articles

```{r fig.asp=1}
articles_low_adequate %>%
  mutate(subject_area = fct_lump_n(subject_area, 20)) %>% 
  filter(subject_area != "Other") %>% 
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(subject_area)), fill = low_power_phrase)) +
  labs(
    x = "Number of articles",
    y = NULL,
    title = "Main journals in which effects have been published",
    subtitle = "Comparison between effects that have adequate power or not"
  ) +
   scale_fill_discrete(name = "")

articles_low_adequate %>%
  unnest(subsubject_area) %>% 
  mutate(subsubject_area = fct_lump_n(subsubject_area, 20)) %>% 
  filter(subsubject_area != "Other") %>% 
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(subsubject_area)), fill = low_power_phrase), position = "fill") +
  labs(
    x = "Number of articles",
    y = NULL,
    title = "Main journals in which effects have been published",
    subtitle = "Comparison between effects that have adequate power or not"
  ) +
   scale_fill_discrete(name = "")
```

Interestingly, some journals, such as "Science of the Total Environment", the "International Journal of Occupational Medicine and Environmental Health", the "Chochrane Database of Systematic Reviews", "Environmental science and pollution research" and the "Journal of Exposure Science and Environmental epidemiology" publish large share of low power studies. On the contrary, BMJ Open publish very few low power studies.

Here also, grouping the journals into big main themes could be more instructive.

```{r}
articles_low_adequate %>% 
  group_by(year = year(pub_date)) %>% 
  summarise(
    prop_low_power = sum(low_power)/n(),
    .groups = "drop"
  ) %>% 
  # filter(low_power) %>%
  filter(year <= 2020) %>% 
  ggplot(aes(x = year, y = prop_low_power)) + 
  geom_point(color = "#8D0422") +
  # geom_line(linetype = "dashed", size = 0.2) +
  geom_smooth(color = "#8D0422", fill = "#8D0422") +
  ylim(c(0,1)) +
  labs(
    title = "Evolution of the proportion of articles with low power",
    x = "Year of publication",
    y = "Proportion of estimates with a low power"
  )
```

There does not seem to be a clear trend in the proportion of articles with low power. If anything it has slightly decreased in the last decade.

```{r}
articles_low_adequate %>% 
  group_by(n_obs_quant = cut_interval(n_obs, n = 10)) %>% 
  summarise(
    prop_low_power = sum(low_power)/n(),
    .groups = "drop"
  ) %>% 
  # filter(low_power) %>%
  # filter(year <= 2020) %>% 
  ggplot(aes(x = n_obs_quant, y = prop_low_power)) + 
  geom_point(color = "#8D0422") +
  # geom_line(linetype = "dashed", size = 0.2) +
  geom_smooth(color = "#8D0422", fill = "#8D0422") +
  ylim(c(0,1)) +
  labs(
    title = "Evolution of the proportion of articles with low power",
    x = "Year of publication",
    y = "Proportion of estimates with a low power"
  )
```

We also look into potential disparities in terms of pollutant

```{r fig.asp=1}
articles_low_adequate %>% 
  unnest(pollutant) %>%
  filter(!is.na(pollutant)) %>%
  ggplot() +
  geom_bar(
    aes(y = fct_rev(fct_infreq(pollutant)), fill = low_power_phrase), 
    position = "fill"
  ) +
  labs(
    x = "Proportion of articles",
    y = NULL,
    title = "Proportion of articles with low and adequate power",
    subtitle = "Comparison between articles studing a different pollutants",
    caption = "Pollutants ordered from the most frequent to the least"
  ) +
  scale_fill_discrete(name = "")
```

There does not seem to be stark differences by pollutant type. 

We then compare these outcomes in terms of outcome (mortality or hospital admissions).

```{r}
articles_low_adequate %>%
  unnest(outcome) %>%
  filter(!is.na(outcome)) %>%
  ggplot() +
  geom_bar(aes(outcome, fill = low_power_phrase), position = "fill") +
  labs(
    x = NULL,
    y = "Proportion of articles",
    title = "Number of articles considering a given outcome",
    subtitle = "Comparison between articles with low and adequate power"
  ) +
  scale_fill_discrete(name = "")
```

There is absolutely no difference along this dimension.


