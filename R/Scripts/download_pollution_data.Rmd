---
title: "Download pollution data"
output: 
  html_notebook: 
    toc: yes
    theme: simplex
    highlight: pygments
  html_document:
    theme: simplex
    highlight: pygments
author: "Vincent Bagilet"
date: "May 18, 2020"
---

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE,
               message = FALSE,
               warning = FALSE)  
```  

```{r include=FALSE}
library(tidyverse)
library(httr)
library(lubridate)
```

# Data description and access

This document contains the code used to download the data on air pollution. The data comes from the European Environment agency and is accessible through this URL: https://discomap.eea.europa.eu/map/fme/AirQualityExport.htm. The web page contains information on how to download the data. 

The process to access the data is as follows:

- Write a simple request URL taking the form: https://fme.discomap.eea.europa.eu/fmedatastreaming/AirQualityDownload/AQData_Extract.fmw?CountryCode=FR&CityName=&Pollutant=5018&Year_from=2020&Year_to=2020&Station=&Samplingpoint=&Source=E1a&Output=HTML&UpdateDate=&TimeCoverage=Year. Note that
parameters can be left blank to access all the values for this parameter (here `CityName` is left blank. 
- This URL leads to the right URL to get the data (year by year)

# Observational data

First, generate a request URL with desired parameter values. It will provides a set of URLs to access the data. Then, store the list of URLs it in a vector. Finally, access all the URLs and merge the outputs to get a unique data frame.

## Functions

```{r}
get_urls <- function(country = "", cities = "", pollutants = "", begin = "", end = "") {
 
  vector_urls <- NULL
  
  for (city in cities) {
    for (pollutant in pollutants) {
      request_url <- str_c("https://fme.discomap.eea.europa.eu/fmedatastreaming/AirQualityDownload/AQData_Extract.fmw?CountryCode=",  country, "&CityName=", city, "&Pollutant=", pollutant, "&Year_from=", begin,"&Year_to=", end, "&Station=&Samplingpoint=&Source=All&Output=TEXT&UpdateDate=&TimeCoverage=Year")
      
      vector_urls <- GET(request_url) %>% 
        content(as = "text") %>% 
        str_split("\r\n") %>% 
        as_vector() %>% 
        head(-1) %>% #last value empty: delete
        c(vector_urls, .)
    }
  }
  return(vector_urls)
}

download_pollution <- function(country = "", cities = "", pollutants = "", begin = "", end = "") {
  vector_urls <- get_urls(country, cities, pollutants, begin, end)

  data <- NULL
  for (url in vector_urls) {
    temp <- tempfile()
    download.file(url, temp)
    data <- data %>% 
      rbind(read_csv(temp))
  }
  return(data)
}
```

## Download

Note: I do not know if it would not be quicker to download the whole data set for France and to filter it out after with the cities and pollutants we want. That would avoid enable to get only one request url and not. But it would require to download a much larger number of datasets. To do that, no need to change the code, only put `cities = ""` and `pollutants = ""`.

For now, we focus on:
- Paris, Marseille, Lyon, Toulouse, Nice, Nantes 
- PM10 (5), PM2.5 (6001), NOx (9), NO2 (8), O3 (7), SO2(1). Pollutant id in parenthesis, full list here: http://dd.eionet.europa.eu/vocabulary/aq/pollutant/view.

```{r echo = T, results = 'hide'}
# For one pollutant and two cities, just to test
# data_whole <- download_pollution(country = "FR", cities = c("Paris", "Marseille"), pollutants = c(5018), begin = 2013, end = 2020)

#all pollutants considered
data_whole <- download_pollution(country = "FR", cities = c("Paris", "Marseille", "Lyon", "Toulouse", "Nice", "Nantes"), pollutants = c(1, 5, 7, 8, 9, 6001), begin = 2013, end = 2020)
```

# Cleaning the data

There are several units of measurement, I convert everything in $\mu g/m^3$.

For now, I assume that we only need a limited subset of variables (AirPollutant, AirQualityStation, Concentration, DatetimeBegin and DatetimeEnd)

```{r}
data_clean <- data_whole %>% 
  mutate(
    Concentration = ifelse(UnitOfMeasurement == "ng/m3", Concentration*1000, Concentration),
    DatetimeBegin = as_datetime(DatetimeBegin),
    DatetimeEnd = as_datetime(DatetimeEnd)
  ) %>% 
  select("AirPollutant", "AirQualityStation", "Concentration", "DatetimeBegin", "DatetimeEnd")
```


# Covariates

## Downloading covariates

The metadata file is updated daily and can be access through the data website.

```{r echo = T, results = 'hide'}
temp <- tempfile()
download.file("https://discomap.eea.europa.eu/map/fme/metadata/PanEuropean_metadata.csv", temp)
metadata_whole <- read_delim(temp, "\t", escape_double = FALSE, trim_ws = TRUE)
```

## Cleaning covariates

- For now, we are only working with France. 
- I assume that we only need a couple of covariates (AirQualityStation, Longitude, Latitude, Altitude, AirQualityStationType). 
- I still need to look at why some variables change in time.

```{r}
metadata_clean <- metadata_whole %>% 
  filter(Countrycode == "FR") %>% 
  select(AirQualityStation, Longitude, Latitude, Altitude, AirQualityStationType)
```

# Merge and export

```{r}
pollution_data <- data_clean %>% 
  left_join(metadata_clean)

save(pollution_data, file = "../Outputs/pollution_data.Rda")
```







