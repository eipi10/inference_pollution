---
title: "Power Simulation Exercise: RCT"
author:
  - name: Vincent Bagilet 
    url: https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: Léo Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: distill::distill_article
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

# Purpose of the document

In this document, we carry out a simulation exercise to evaluate the performance of randomized control trials (RCTs) in measuring the short-term effects of air pollution on health. 

# Description of the analysis

## Data

We use data from 18 cities in France over the 2013-2018 period. The data set contains records of hospital admissions and deaths, mean concentration data for various air pollutants, a bunch of weather variables and calendar control variables (such as school holidays for instance). All variables are at the daily and city level. There is therefore a unique observation per date and per city in the data set.

## Background on selected designs for air pollution analyses

### RCT

Here, we consider interventions leading to changes in air pollution levels on some random days. Examples of such interventions include transportation strikes or for instance. Of course, dates are often not defined as random and are likely to be correlated with unobserved variable. In the present setting, we first consider the golden standard case in which these days are defined at random. Then, we consider deviations from this golden standard.

The overall idea of the RCT is to compare the average number of deaths or hospital admissions on days with treatment to days with no treatment.


## Analysis

### Overall setting

To sum up the analysis, the aim is to measure the performance of different designs in recovering the true effect of a treatment and to analyze characteristics of the estimates. For simplicity, consider the daily number of death as the output variable of interest for now. To do so, we proceed as follows:

1. We define the length of the study period, *ie* the number of observations, and draw a study period randomly.
1. We define the treated days. In the case of the RCT, we draw $n_{obs} \times p_{treat}$ treated days, where $p_{treat}$ is the proportion of treated days. For the ITS, we draw the date at which the treatment starts. For the RDD, we define the concentration threshold above which observations are treated. For the DiD, we define the group of treated cities.
1. We choose a true effect size, $\beta_0$, representing the percentage change in the number of deaths in response to the treatment.
1. We create fake death counts, accounting for the "true" effect of the treatment:  
$h_{ct}^{fake} = (1 + \beta_0 T_{ct}) h_{ct}$ where $h_{ct}$ represents the daily number of deaths or emergency admissions, for a given city $c$ at time $t$, $T_{ct}$ the treatment variable which is equal to 1 if the city $c$ is treated at time $t$ and zero otherwise. 
1. We estimate our model and retrieve $\hat{\beta}$ and the associated p-value.
1. We run the steps 4 to 6 $n_{iter}$ times and compare our estimated effects $\hat{\beta}$ to the true effect $\beta_0$. We compute the average bias, type M, type S and power.

In the potential outcome framework, we have $Y_{ct}(0)$, the number of deaths if a city $c$ is not treated at time $t$ equal to $h_{ct}$ and $Y_{ct}(1)$, the number of deaths if this city is treated, equal to $(1 + \beta_0 )h_{ct}$. We build our setting, so that we can "observe" both outcomes. The estimation is however performed on the fake observed data $h^{fake}$ and we have
$h^{fake} = Y(0) × (1-T) + Y(1) × T$. 

### Varying "parameters"

Note that there are several parameters we can vary in order to evaluate the performance of such RCTs: the number of observations, the proportion of treated days, the true effect size and the treatment allocation. In order to limit the number of simulations and for clarity, we only modify one parameter at the time, keeping the others constant. We consider the following values for these parameters:

- Number of observations: 1000, 2000 **because** ...
- Proportion of treated days: 0.1, 0.5, etc. The proportion of days treated matters since, while the number of observations can be large, the number of treated days may remain very small (*eg* the number of strikes). We choose these values **because** ...
- Effect size: 0.05%, 0.1% , 0.5% and 1% **because** ...
- The definition of treated days, *ie* the drawing processes: drawn at random, correlated with covariates, autocorrelated, etc (to do that, we can use the code we wrote to create missing data).
- The estimation model

# Actual implementation

In this section, we follow the steps described in the section "analysis" and carry out our analysis. We basically define a function for each step.

## Loading and Formatting Data

We load the packages and the data and wrangle it into a format well suited for this analysis.

```{r, echo=TRUE, message = FALSE, warning = FALSE}
library(here) # for files paths organization
library(tidyverse) # for data manipulation and visualisation
library(modelr) # modeling within the tidyverse
library(retrodesign) # formulas for type-m and type-s errors
library(knitr) # for tables
library(broom)
library(furrr) # for parallel computing
future::plan(multiprocess)

source(here::here("R", "Scripts", "script_custom_ggplot_theme.R"))

pollution_data <- readRDS(here::here("R", "Outputs", "data_daily_imputed.rds")) %>% 
  ungroup() %>% 
  mutate(
    city = tolower(city),
    city = str_remove_all(city, "[\\s-]")
  )
emergency_data <- readRDS(here::here("R", "Outputs", "emergency_data.rds"))
mortality_data <- readRDS(here::here("R", "Outputs", "mortality_data.rds"))

total_data <- pollution_data %>% 
  left_join(emergency_data, by = c("city", "date")) %>% 
  left_join(mortality_data, by = c("city", "date")) %>% 
  ungroup() %>% 
  mutate(date = ymd(date))

# no_pollutant_data <- total_data %>% 
#   select(-(no2:pm2.5)) %>% 
```

## Function definitions

### Drawing the study period

First, we create a function to randomly draw a study period of a given length. For simplicity, we choose to have the same study period for each city. This also seems realistic; a study focusing on several cities would probably consider a unique study period. Note that to do so, we need to nest the data before running the function. 

This function randomly selects a starting date for the study, early enough so that the study can actually last the number of days chosen, and returns a boolean vector indicating whether each date is in the study or not.

```{r}
draw_study_period <- function(dates, n_days_study = 1200) {
  max_date <- max(dates)
  min_date <- min(dates)
  
  begin_study <- sample(seq_along(max_date - n_days_study), 1)
  
  study_period <- (dates < begin_study)
}
```

### Defining the treatment

Then, we create a function to draw the treatment. 

```{r}
draw_treated_days <- function(dates, p_treat = 0.5, treatment_type = "random") {
  
  if(treatment_type == "random") {
    treated <- rbernoulli(length(dates), p_treat)
  }
}
```

### Creating a fake output

We then create our fake output

```{r}
create_fake_output <- function(output_var, percent_effect_size = 0.5) {
  # fake_output <- output_var*(1 + percent_effect_size/100)
  fake_output <- output_var + rpois(length(output_var), output_var*percent_effect_size/100)
} 
```

### Estimate the model

We can then estimate our model and retrieve the point estimate and p-value.

```{r}
estimate_model <- function(data, formula) {
  est_results <- data %>% 
    lm(data = ., formula = formula) 
  
  nobs <- length(est_results$residuals)
  
  est_results %>% 
    broom::tidy(., conf.int = TRUE) %>% 
    filter(term == "treatedTRUE") %>%
    rename(p_value = p.value) %>% 
    select(estimate, p_value) %>% 
    mutate(n_obs = nobs)
} 
```

### Computing simulations

We then create a function running all the previous functions together and therefore performing an iteration of the simulations. We then loop this function to get a large number of replications of this simulation, with fixed parameters. It gives a data set with estimate and p-value for each run, along with information about the parameter considered.

```{r}
compute_one_simulation <- function(data, n_days_study = 1200, p_treat = 0.5, treatment_type = "random", percent_effect_size = 0.5, formula) {
  
  output_var <- str_extract(formula, ".+(?=\\s~)")
  indep_var <- str_extract(formula, "(?<=~\\s).+")
  
  data %>% 
    nest(cols = c(everything(), -date))  %>%
    mutate(study_period = draw_study_period(date, n_days_study)) %>% 
    filter(study_period) %>%
    select(-study_period) %>%
    mutate(treated = draw_treated_days(date, p_treat, treatment_type)) %>% 
    unnest(cols = everything()) %>%
    mutate(fake_output = create_fake_output(.data[[output_var]], 1)) %>%
    estimate_model(formula = str_c("fake_output ~ ", indep_var))
} 

# compute_one_simulation(total_data, formula = "deaths_all_causes ~ treated")

compute_simulations <- function(data, n_days_study = 1200, p_treat = 0.5, treatment_type = "random", percent_effect_size = 0.5, formula = "deaths_all_causes ~ treated", n_iter = 1000) {
  
  all_est <- future_map_dfr(
    1:n_iter, 
    ~ compute_one_simulation(
      data = data, 
      n_days_study = n_days_study, 
      p_treat = p_treat, 
      treatment_type = treatment_type, 
      percent_effect_size = percent_effect_size, 
      formula = formula), 
    .options = furrr_options(seed = TRUE)
    )
  
  all_est %>%
    mutate(
      formula = formula, 
      treatment_type = treatment_type, 
      n_days_study = n_days_study,
      p_treat = p_treat, 
      percent_effect_size = percent_effect_size, 
      n_iter = n_iter
    ) %>%
    select(formula:n_iter, n_obs, estimate, p_value)
} 

# test <- compute_simulations(total_data, n_iter = 3)
````

Note that we can then loop the function `compute_simulations` on the values of its parameters in order to generate different simulations.

Once we have estimates and p-values for each iteration and each set of parameters, we will be able to compute the statistics of interest (power, type M error, etc) for each set of parameters. The function `summarise_simulations` does just that. Note that it takes as input a data frame produced with `compute_simulations`.

```{r eval=FALSE}
summarise_simulations <- function(data) {
  
  data %>% 
    group_by(formula, treatment_type, n_days_study, p_treat, percent_effect_size, n_iter) %>%
    mutate(
      power = mean(p_value <= 0.05)*100, 
      bias = mean(estimate) - percent_effect_size,
      average_p_value = mean(p_value),
      average_n_obs = mean(n_obs, na.rm = TRUE)
    ) %>% 
    ungroup() %>%
    # filter(p_value <= 0.05) %>%
    group_by(formula, treatment_type, n_days_study, p_treat, percent_effect_size, n_iter) %>%
    summarise(
      average_n_obs = unique(average_n_obs),
      power = unique(power),
      type_m = mean(abs(estimate)/percent_effect_size),
      type_s = (sum(estimate < 0)/n())*100,
      bias = unique(bias),
      average_p_value = unique(average_p_value),
      .groups	= "drop"
    ) %>% 
    ungroup()
} 

# summarise_simulations(test)
```

## Running the simulations

We can then run the simulations, using the function `compute_simulations`. As mentioned, we want to loop its inputs on the set of inputs we want to test. Note that, in order to limit the number of simulations, for now, we only vary one input at the time. To do so, for each parameter, we define the list of parameters we want to test.

We store the list 

```{r eval=FALSE}
vect_n_days_study <- c(1000, 100, 500, 2000)
vect_p_treat <- c(0.5, 0.01, 0.1)
vect_treatment_type <- c("random")
vect_percent_effect_size <- c(1, 0.01, 0.5, 2)
vect_formula <- c(
  "deaths_all_causes ~ treated", 
  "deaths_all_causes ~ treated + rainfall_height + sea_level_pressure + wind_speed + temperature + public_holiday + school_holiday"
)

vect_param <- c("p_treat", "treatment_type", "percent_effect_size", "formula") #,"n_days_study")

vary_param_simulations <- function(data, varying_param) {
  
  vect_varying_param <- str_c("vect_", varying_param)
  form <- as.formula(paste(
      "~ compute_simulations(data = data,", varying_param, "= ., n_iter = 3)"))
  
  map_dfr(get(vect_varying_param), form)
} 
```

We can then vary all parameters one after the other:

```{r}
all_simulations <- map_dfr(vect_param, vary_param_simulations, data = total_data)

saveRDS(all_simulations, "../Outputs/data_simulations.RDS")
```

We then summarize our results, computing power, type M and so on for each set of parameters.

```{r}
results_all_parameters <- summarise_simulations(all_simulations)

saveRDS(results_all_parameters, "../Outputs/results_all_parameters.RDS")
```








