---
title: "Power Simulation Exercise: actual data"
author:
  - name: Vincent Bagilet 
    url: https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: LÃ©o Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

# Purpose of the document

In this document, we carry out a simulation exercise to evaluate the inference properties of different research designs aiming at measuring the short-term effects of air pollution on health. We consider different types of quasi experiments and for each associate an identification strategy. 

This document focuses on running the simulations. The results are analyzed and discussed in another document.

# Description of the analysis

## Data

We use data from 18 cities in France over the 2013-2018 period. The data set contains records of hospital admissions and deaths, mean concentration data for various air pollutants, a bunch of weather variables and calendar control variables (such as school holidays for instance). All variables are at the daily and city level. There is therefore a unique observation per date and per city in the data set.

## Background on selected quasi-experiments

Note that, for now we focus on quasi experiments for which treatment is binary and homogeneous, both in time and across individuals. It is also not correlated with covariates (apart in the case of the air pollution alerts in which it is correlated with the pollutant level).

### Treatment on "random" days

Here, we consider interventions leading to changes in air pollution levels on some random days. Examples of such interventions include transportation strikes. Of course, dates are often not defined as random and are likely to be correlated with unobserved variables. In the present setting, we first consider the golden standard case in which these days are actually defined at random. One can think of this case as a Randomized Control Trial: it represents what would happen if an  experimenter could implement a treatment increasing pollution on random days.

### National intervention

Here consider interventions leading to changes in air pollution levels in all cities after a given date. Examples of such interventions include an air pollution reduction policy at a national level or a change in regulations at a national level leading to an increase in pollution level. 

### Sub-national intervention

Here, we consider interventions leading to changes in air pollution levels in a subset of cities after a given date. Examples of such interventions include an air pollution reduction policy at a sub-national level or a change in regulations at a sub-national level leading to an increase in pollution level. 

### Rolling intervention

Here, we consider interventions leading to changes in air pollution levels in a all cities but the change happens after a different date for each city. Examples of such interventions include an air pollution reduction policy at a national level whose implementation is rolled out.  

### Air pollution alert

Here we consider interventions that affect exposure to air pollution when air pollution levels reach a given threshold. Examples of such interventions include air pollution alerts: when pollution reaches a certain level, alerts are released, inviting people to reduce their exposure.

### No quasi experiment

We finally consider a case for which there is no quasi experiment. One can consider that in this case all units are treated. To measure the effect of air pollution on health in this case, we will use methods from the epidemiology literature, as discussed in the next section.

## Identification method

In order to estimate the parameters of interest, we use several identification methods. Most quasi-experiments are associated with a given identification method.

### RCT-type

The effect of a treatment on random days is estimated using a RCT type of model. The overall idea of the RCT is to compare the average number of deaths or hospital admissions in cities with treatment to cities with no treatment on the same day, controlling for differences across cities. 

This identification method enables to estimate the Average Treatment Effect (ATE), *ie* a difference in mean between the treated and the control group ($\mathbb{E}[Y_{1i} - Y_{0i}]$). To do so, we use the following type of model:

$$Y_{ct} = \alpha + \beta D_{ct} + \epsilon_{ct}$$
where, as in the whole document, $Y_{ct}$ is the health outcome of interest (for instance mortality), in a city $c$, at date $t$. The parameter of interest is $\beta$ and $D_{ct}$ is a dummy equal to 1 if the city $c$ is treated at time $t$ and 0 otherwise.

The identification assumption here is that the the potential outcomes are independent of the treatment (independence assumption).

### Difference-In-Differences

We use a Difference-In-Differences (DiD) model to estimate the effect of a sub-national treatment or a rolling intervention. The overall idea of the DID is to compare the average number of deaths or hospital admissions in cities with treatment to cities with no treatment on the same day, controlling for differences across cities.

This identification method enables to estimate the Average Treatment Effect on the Treated (ATET), *ie* $\mathbb{E}[Y_{1i} - Y_{0i} | Treated_c = 1, Post_t = 1]$, where $Post_t$ is a dummy variable equal to one after the intervention and 0 otherwise. We thus have $D_{ct} = Treated_c \times Post_t$. To estimate this effect, we use the following type of model:

$$Y_{ct} = \alpha + \beta D_{ct} + \gamma Treated_c + \delta Post_t + \epsilon_{ct}$$
The key identification assumptions is parallel trends, *ie* the expected difference in outcome between the treatment and control group would have remained the same in the absence of the treatment.

### Interrupted time series

We use Interrupted Time Series (ITS) type of models to estimate the effect of a national treatment. The overall idea of the ITS is to compare the average number of deaths or hospital admissions before and after treatment.

This identification method enables to estimate the ATET. To do so, we use the following type of model:

$$Y_{ct} = \alpha + \beta D_{ct} + \gamma t + \delta t^{post}_t +  \epsilon_{ct}$$
where $t^{post}_t$ is a time variable representing the number of days after the intervention (equal to 0 before the intervention).

### Regression Discontinuity Design

We use a Regression Discontinuity Design (RDD or RD) to estimate the effect of an air pollution alert type of intervention. The overall idea of the RD is to compare days just below the threshold to days just above the threshold (where exposure and health impacts are thus lower). The key identification assumption is that days just below and just above the threshold are comparable. Thus, no confounders should vary discontinuously at the threshold (local independence, $(Y_{0i}, Y_{1i}) \perp D_i|Z_i$, for $Z_i \in [c-a, c+a]$ ) and the treatment should vary at threshold (relevance, $D_i = \mathbb{1} \{Z_i \geq  c\}$).

This identification method enables to estimate a Local Average Treatment Effect (LATE) at the cutoff, *ie* $\mathbb{E}[Y_{1i} - Y_{0i}|Z_i = c]$. To do so, we use the following type of model, but restricting our sample to observation just below and just above the threshold:

$$Y_{ct} = \alpha + \beta D_{ct} + \epsilon_{ct}$$

### Instrumental variable

In all previous identification methods, we only tried to estimate a reduced form, *ie* looking at the effect of a treatment directly on mortality. We did not consider the effect of the treatment on air pollution, *ie* the mechanism. In this section, we instrument the effect of pollution on the outcome with an instrument/treatment. We basically model the effect of an exogeneous instrument on air pollution and use this information to retrieve a causal estimate of the short term effect of air pollution on health. Note that the class of treatments/instruments considered here is broader than in the previous section. 

In a first step, we only consider binary instruments such as thermal inversions and high/low wind speed for instance. This type of instruments corresponds to a large share of the instruments considered in the literature. The key assumption is that these treatments only affect the health outcome variable via their effect on air pollution. 

Let's denote $Z$ this instrument. We compute a 2-Stages Least Squares (2SLS) where the first stage has the form:

$$Poll_{ct} = \gamma + \delta Z_{ct} + e_{ct}$$
and the second stage:

$$Y_{ct} = \alpha + \beta Poll_{ct} + \epsilon_{ct}$$

### Linear model

Finally, we also consider simple linear models in order to measure the correlation between the health outcome of interest and air pollution, controlling for potential confouders. The identification assumptions here are the usual OLS assumptions.

We estimate a model of the form:

$$Y_{ct} = \alpha + \beta Poll_{ct} + \epsilon_{ct}$$

## Analysis

### Overall setting

To sum up the analysis, the aim is to measure the inference properties of different research designs aiming at measuring the short-term effects of air pollution on health. For simplicity, consider the daily number of death as the output variable of interest for now. We proceed as follows:

1. We define the length of the study period, *ie* the number of observations, and a true effect size, $\beta_0$, representing the percentage change in the number of deaths in response to the treatment.
1. We draw a study period randomly
1. We define the treated days, *ie* we define a treatment variable $T_{ct}$ equal to 1 if the city $c$ is treated at time $t$ and zero otherwise. 
1. We compute the number of deaths when treated for each  observation ($Y(1)$) based on the "true" effect of the treatment. We then derive $Y_{obs} = Y(0) Ã (1-T) + Y(1) Ã T$.
1. We estimate our model with $Y_{obs}$ as a dependent variable and retrieve $\hat{\beta}$ and the associated p-value.
1. We run the steps 2 to 5 $n_{iter}$ times.
1. We compute the average bias, type M, type S and power.

<!-- In the potential outcome framework, we have $Y_{ct}(0)$, the number of deaths if a city $c$ is not treated at time $t$ equal to $h_{ct}$ and $Y_{ct}(1)$, the number of deaths if this city is treated, equal to $(1 + \beta_0)h_{ct}$. We build our setting, so that we can "observe" both outcomes. The estimation is performed on the fake observed data $Y_{obs} = h^{fake}$ and we have -->
<!-- $Y_{obs} = Y(0) Ã (1-T) + Y(1) Ã T$.  -->

### Varying "parameters"

Note that there are several parameters we can vary in order to evaluate the performance of the different methods: the identification strategy, the number of observations, the proportion of treated days, the true effect size and the model. In order to limit the number of simulations and for clarity, we only modify one parameter at the time, keeping others constant and equal to a baseline value. When we vary a parameter, we consider the following values:

- Number of observations: we actually vary the length of the study, in days. The number of observations is approximately equal to the number of days in the study times the number of cities. We consider 100, 250, 500, 750 and 1000 days **because** ...
- Proportion of treated days: 0.1, 0.5. The proportion of days treated matters since, while the number of observations can be large, the number of treated days may remain very small (*eg* the number of strikes). We choose these values **because** ...
- Effect size: 0.15%, 0.5% and 1% **because** ...
- The estimation model: we first do not include covariates, only fixed effects. We then include include different sets of covariates.

We consider this set of parameters for each quasi-experiment.

### Deviation from the ideal case

In addition to considering an "ideal" case in which treatment is allocated randomly, the effect of the treatment is homogeneous and with perfect compliance, we can consider deviations from this "ideal" case:

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
table_cases <- "
| Allocation                  | Compliance   | Effect of the treatment  |
|-----------------------------|:------------:|-------------------------:|
| Random                      | Yes          | Homogeneous              |
| Random                      | No           | Homogeneous              |
| Correlated with covariates  | Yes          | Homogeneous              |
| Random                      | Yes          | Heterogeneous            |
"
cat(table_cases) 
```

- A non-random allocation can exist in several ways. The treatment can be correlated with one covariate, another or even several covariates. It can also be correlated with this covariates in different ways: linear or a large variety of non linear ways. Even with a settled correlation form, the intensity of the correlation can vary.
- The heterogeneity in the treatment effect can arise from variation with time, with individuals or given the value of covariates.

Deviating from the ideal case therefore yields a very large number of cases. We limit ourselves to a sample of these cases.

# Actual implementation

In this section, we follow the steps described in the section "analysis" and carry out our analysis. We basically define a function for each step.

## Loading and Formatting Data

We load the packages and the data and wrangle it into a format well suited for this analysis.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(here)
library(tidyverse) 
library(knitr) 
library(broom)
library(lubridate)
library(tictoc)
library(rlang)
library(readr)
library(fixest)
library(Formula)
library(furrr) 
library(mediocrethemes)
library(progress)
library(progressr)

set_mediocre_all()

# Settings for progress bar
handlers(list(
  handler_progress(
    format   = ":spin :current/:total (:message) [:bar] :percent in :elapsed ETA: :eta"
  )
))

nmmaps_data <- readRDS(here::here("R", "Inputs", "nmmaps_data_simulations.rds")) %>% 
  mutate(temperature_squared = temperature^2)
```

## Function definitions

### Selecting the study period

First, we create a function to randomly select a study period of a given length. This function also randomly selects a given number of cities to consider. For simplicity, we choose to have the same temporal study period for each city. This also seems realistic; a study focusing on several cities would probably consider a unique study period. 

This function randomly selects a starting date for the study, early enough so that the study can actually last the number of days chosen. It then  randomly draws a given number of cities and filters out observations outside of the study period. It returns a data set with only the desired number of cities and days.

```{r}
select_study_period <- function(data, n_days = 3000, n_cities = 68) {
  dates <- data[["date"]]
  err_proof_n_days <- min(n_days, unique(dates))
  cities <- data[["city"]]
  err_proof_n_cities <- min(n_cities, length(unique(cities)))
  
  begin_study <- sample(
    seq.Date(
      min(dates), 
      max(dates) - err_proof_n_days, 
      "day"
    ), 1)
  
  dates_kept <- dplyr::between(
    dates, 
    begin_study, 
    begin_study + err_proof_n_days
  )
  
  cities_kept <- cities %in% sample(levels(cities), size = err_proof_n_cities)
  
  data_study <- data[(dates_kept & cities_kept),]
  return(data_study)
}
```

### Defining the treatment

Then, we create a function to draw the treatment. This function returns a boolean vector, stating whether each observation is in the treatment group or not.

For each treatment, we define the proportion of treated observations (`p_obs_treat`). The different treatments correspond to different types of quasi-experiments. Note that we will consider some identifications strategies that will only consider restricted samples (RDD and RDIT). For coding simplicity, to get an accurate proportion of treated units, we directly restrict the sample in this function. For the RDIT, we create another type of quasi experiment ("national_short"), even though the quasi experiment is actually the same as for a national intervention.

In this function, we made some simulations decisions: 

- For the local intervention, to simplify, we set the beginning of the treatment as the median date. To get the desired proportion of treated units, we thus need to draw 2*`p_obs_treat` cities.
- For the air pollution alert, we draw a threshold position (from a beta distribution, for no other reason than the shape of its PDF). For each city, we find the corresponding threshold and define a bandwidth such that `p_obs_treat` observations of the total sample are treated. Observations outside the bandwidth get a `NA` for `treated`. Thus, after calling this function, we need to filter out observations with a `NA` for `treated`.
- In the roll out intervention, for each city, we randomly draw a treatment starting date. To get a proportion of treated days equal to `p_obs_treat`, we do not draw this date from the whole sample. We only consider dates close enough to the end of the study period such that, on average only `p_obs_treat` of the data will be treated.
- For the short national intervention (*ie* the RDIT), we define the bandwidth such that the proportion of treated days, as compared to the overall sample size, *ie* the length of the study period, is equal to `p_obs_treat`. Note that it does not make sense to have a large `p_obs_treat` as this would correspond to the national case. We therefore limit `p_obs_treat` to be smaller than 0.5.
- For the GAM and IV, we only consider a subset of observations to create variation across iterations

```{r}
draw_treated <- function(data, p_obs_treat = 0.5, quasi_exp = "random_days") {
  
  if (quasi_exp == "random_days") {
    data[["treated"]] <- rbernoulli(length(data[["date"]]), p_obs_treat)
  } else if (str_starts(quasi_exp, "alert")) {
    pollutant <- str_extract(quasi_exp, "(?<=_).+")
    # threshold_pos <- rbeta(1, 20, 7) 
    threshold_pos <- runif(1, 0.2, 0.4) 
    data <- data %>%
      group_by(.data$city) %>%
      mutate(
        threshold = quantile(.data[[pollutant]], threshold_pos, names = FALSE),
        bw_high = quantile(.data[[pollutant]], min(threshold_pos + p_obs_treat, 1), names = FALSE),
        treated = ifelse(
          .data[[pollutant]] > bw_high | .data[[pollutant]] < threshold - (bw_high - threshold), 
          NA, 
          (.data[[pollutant]] >= threshold)
        )
      ) %>%
      select(-bw_high, -threshold) %>% 
      ungroup()
  } else {
    data[["treated"]] <- TRUE
  }
  
  return(data)
}
```

Both to verify that everything works well and for illustration, we can make quick plots:

```{r echo=FALSE}
draw_treated_exp <- function(data, p_obs_treat, quasi_exp) {
  data <- draw_treated(data, p_obs_treat, quasi_exp)
  
  data[[paste("treated", quasi_exp, sep = "_")]] <- data[["treated"]]
  
  return(data)
}

test_treatment_data <- nmmaps_data %>% 
  select_study_period(n_days = 3500, n_cities = 6) %>% 
  draw_treated_exp(0.5, "random_days")%>% 
  draw_treated(0.01, "alert_co") %>% 
  rename(treated_alert = treated)  

graph_treatment_assignment <- function(data, method) {
  data[["alert"]] <- (method == "alert")
  y_lab <- ifelse(method == "alert", "CO concentration level (in mug/m3)", "Daily number of deaths")
  subtitle <- ifelse(method == "alert", "CO concentration", "number of deaths")
  
  data %>% 
    mutate(outcome = ifelse(alert, co, death_total)) %>%
    rename(Treated = .data[[paste("treated", method, sep = "_")]]) %>% 
    ggplot() +
    geom_point(aes(x = date, y = outcome, color = Treated), size = 0.3) +
    facet_wrap(~city, scales = "free") +
    labs(
      title = paste(
        "Treatment assignment for a", 
        str_replace_all(method, "_", " "), 
        "intervention"
      ), 
      subtitle = paste("Evolution of the", subtitle, "in time for each city"),
      x = "Date", 
      y = y_lab
    )
} 

test_treatment_data %>%
  graph_treatment_assignment("random_days")

test_treatment_data %>% 
  graph_treatment_assignment("alert")
```

### Creating the output if a unit is treated

We then create the output if a unit is treated, Y(1). We use different methods to generate fake health data: 

- For binary treatments, we draw 
- For the OLS, we build a generative model that creates fake data based on the formula given.

We also create a short function to detect a pollutant among independent variables of a formula. We also use this function later.

For the IV, we include the intensity of the IV in the method name. This is not very clean coding style as we should have created a new parameter. Yet, it makes the code much simpler and ultimately much clearer.

```{r}
find_pollutant <- function(formula) {
    pollutants_list <- c("co", "pm10", "pm2.5", "no")
    pollutant <- pollutants_list[pollutants_list %in% all.vars(formula)]
    return(pollutant)
}

create_y1 <- function(data,
                      percent_effect_size = 0.5,
                      id_method = "RCT", 
                      iv_intensity = NA, 
                      formula) {
  
  fml <- Formula::as.Formula(formula)
  dep_var <- all.vars(fml)[[1]]
  
  if (id_method == "RDD") {
    
    data <- data %>%
      group_by(.data$city) %>%
      mutate(
        y1 = .data[[dep_var]] -
          rpois(
            n(),
            mean(.data[[dep_var]], na.rm = TRUE) * percent_effect_size / 100
          ) %>% suppressWarnings() #warnings when is.na(dep_var) eg rpois(1, NA)
      ) %>%
      ungroup()
    
  } else if (id_method == "RCT") {

    #Just a different sign
    data <- data %>%
      group_by(.data$city) %>%
      mutate(
        y1 = .data[[dep_var]] +
          rpois(
            n(),
            mean(.data[[dep_var]], na.rm = TRUE) * percent_effect_size / 100
          ) %>% suppressWarnings() #warnings when is.na(dep_var) eg rpois(1, NA)
      ) %>%
      ungroup()
    
  } else if (id_method %in% c("OLS", "IV")) {
    pollutant <- find_pollutant(fml)
    
    if (id_method == "IV") {
      data[[pollutant]] <- 
        data[[pollutant]] + 
        iv_intensity*data[["treated"]] +
        rnorm(data[[pollutant]], 0, 0.1)
      
      #need to withdraw the first stage from the formula and add the pollutant
      parts_formula <- str_split(formula, "\\|")[[1]]
      formula_clean <- str_c(
        parts_formula[1], 
        "+", str_extract(parts_formula[3], "\\b.+(?=~)"),
        "|", parts_formula[2]
      )
    } else {
      formula_clean <- formula
    }
    
    fml <- Formula::as.Formula(formula_clean)
    reg <- feols(data = data, fml = fml, combine.quick=FALSE)
    reg$coefficients[[pollutant]] <- mean(data[[dep_var]])*percent_effect_size/100  
    #to get beta as the increase in percent
    res <- reg$residuals
    
    data[["y1"]] <- predict(reg, data) + rnorm(res, mean(res), sd = sd(res))
    # data[["y1"]] <- ifelse(data[["y1"]] < 0, 0, data[["y1"]])
  } 
  
  return(data)
} 
```

### Estimate the model

We can then estimate our model and retrieve the point estimate and p-value.
The model should be specified in a three part formula as follows: y ~ x | fixed effects | cluster. If one does not want to set fixed effects, a 0 should be put in the second part of the formula: y ~ x | 0 | cluster.

```{r}
estimate_model <- function(data, formula) {
  fml <- Formula::as.Formula(formula)
  pasted_formula <- paste(str_c(fml)[[2]], str_c(fml)[[1]], str_c(fml)[[3]])
  
  #the param of interest varies across identification methods
  param_of_interest <- 
    ifelse(str_count(pasted_formula, "\\|") == 2, #if IV (ie 3 parts rhs in formula)
           str_c("fit", find_pollutant(fml), sep = "_"), 
           ifelse("treated" %in% all.vars(fml), "treatedTRUE", find_pollutant(fml)))
  
  #run the estimation
  est_results <- feols(data = data, fml = fml, se = "hetero") 
  
  #retrieve the useful info
  nobs <- length(est_results$residuals)
  fstat <- fitstat(est_results, type = "ivf")$ivf1 %>% as_vector() %>% .[[1]]
  
  est_results %>%
    broom::tidy(conf.int = TRUE) %>%
    filter(term == param_of_interest) %>%
    rename(p_value = p.value, se = std.error) %>%
    select(estimate, p_value, se) %>%
    mutate(
      n_obs = nobs,
      f_stat = fstat
    )
} 	
# estimate_model(draw_treated(nmmaps_data), "death_total ~  temperature + temperature_squared | city + month^year + weekday | co ~ treated")
```

### Computing simulations

We then want to compute simulations by running all the previous functions together. Yet, for the DiD and the ITS/RDIT, we first need to create additional dummy variables (`post` and `city_treated` for the DiD and `t` and `t_post` for the ITS/RDIT). We thus write a function which does so, `add_post_vars`:

We also need to compute the true effect for each simulation. The different identification methods aims to identify different estimands (ATE, ATET, etc). We 

```{r}
compute_true_effect <- function(data, id_method, percent_effect_size, dep_var) {
  
  if (id_method %in% c("OLS", "IV")) {
    true_effect <- percent_effect_size/100*mean(data[[dep_var]])
  } else {#ATE
    true_effect <- mean(data$y1 - data$y0, na.rm = TRUE)
  }
  
  return(true_effect)
}
```

We can now create a function running all the previous functions together and therefore performing an iteration of the simulation for a given set of parameters. This function returns a one row data set with estimate, p-value, number of observations and true effect size. 

```{r}
compute_simulation <- function(data,
                               n_days = 3000,
                               n_cities = 68, 
                               p_obs_treat = 0.5,
                               percent_effect_size = 1,
                               quasi_exp = "random_days",
                               id_method = "RCT", 
                               iv_intensity = NA,
                               formula = "resp_total ~ treated",
                               progressbar = progressor()#to have a progressbar when mapping
                               ) {
  
  fml <- Formula::as.Formula(formula)
  dep_var <- all.vars(fml)[1]

  sim_data <- data %>%
    select_study_period(n_days, n_cities) %>%
    mutate(y0 = .data[[dep_var]]) %>%
    draw_treated(p_obs_treat, quasi_exp) %>% 
    create_y1(percent_effect_size, id_method, iv_intensity, formula) %>% 
    mutate(
      yobs = y1 * treated + y0 * (1 - treated)
    ) %>%
    filter(!is.na(treated)) #not necessary bc dropped in lm()
  
  updated_fml <- ifelse(
    id_method %in% c("OLS", "IV"),
    str_replace(formula, dep_var, "y1"),
    str_replace(formula, dep_var, "yobs")
  )

  sim_output <- sim_data %>%
    estimate_model(formula = updated_fml) %>% 
    mutate(
      true_effect = compute_true_effect(
        sim_data, 
        id_method, 
        percent_effect_size, 
        dep_var
      ),
      n_days = n_days, 
      n_cities = n_cities,
      p_obs_treat = p_obs_treat,
      percent_effect_size = percent_effect_size,
      quasi_exp = quasi_exp,
      id_method = id_method, 
      iv_intensity = iv_intensity,
      formula = formula
    )
  
  progressbar() 
  return(sim_output)
}

nmmaps_data %>%
  compute_simulation(
    data = .,
    formula = "death_total ~ temperature + temperature_squared | city + month^year + weekday |Â co ~Â treated ",
    quasi_exp = "random_days",
    id_method = "IV",
    iv_intensity = 0.5,
    n_days = 100,
    n_cities = 68,
    percent_effect_size = 0.5,
    p_obs_treat = 0.01
  )
````

We will then loop this function to get a large number of replications of each simulation for a given set of parameters. We will also vary the values of the different parameter.

## Running the simulations

We can then run the simulations. But first, we define the set of parameters we want to consider.

### Defining baseline parameters

We create a table displaying in each row a set of parameters we want to have a simulation for, `sim_param`. We will then map our function `compute_simulation` on this table.

To build `sim_param`, we first define a set of baseline values for our parameters and store them in a data frame, `sim_param_base`.

```{r}
sim_param_base <- tibble(
  n_days = 3000,
  n_cities = 68,
  p_obs_treat = 0.5,
  percent_effect_size = 1, 
  iv_intensity = 0.5,
  formula = "death_total ~ treated + temperature + temperature_squared | city + month^year + weekday"
)

# saveRDS(sim_param_base, "R/Outputs/sim_param_base.RDS")
# write_csv(sim_param_base, "R/Outputs/sim_param_base.csv")
```

### Evolution with values of a given parameter

#### Defining parameters

We will then vary the values of the parameters one after the other. We thus create vectors containing the different values of the parameters we want to test.
 
```{r}
vect_n_days <- c(100, 500, 1000, 2000, 3000, 4000)
vect_n_cities <- c(68)
vect_p_obs_treat <- c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5)
vect_percent_effect_size <- c(0.1, 0.5, 1, 2, 5, 10)
vect_iv_intensity <- c(0.01, 0.1, 0.2, 0.5, 0.7)
vect_formula <- c(
  # "death_total ~ treated",
  "resp_total ~ treated + temperature + temperature_squared | city + month^year + weekday",
  "death_total ~ treated + temperature + temperature_squared | city + month^year + weekday",
  "copd_age_65_75 ~ treated + temperature + temperature_squared | city + month^year + weekday"
)
```


We then want to create the actual table, varying parameters one after the other. To do so, we create a simple function `add_values_param`. This function adds the values of a parameter contained in a vector. We can then map this function on all the vectors of parameters of interest.

```{r}
#adds all values in vect_param
add_values_param <- function(df, vect_param) {
  param_name <- str_remove(vect_param, "vect_")
  
  tib_param <- tibble(get(vect_param))
  names(tib_param) <- param_name
  
  df %>% 
    full_join(tib_param, by = param_name) %>% 
    fill(everything(), .direction = "downup")
}

vect_of_vect_param <- c("vect_n_days", "vect_n_cities", "vect_p_obs_treat", "vect_percent_effect_size", "vect_iv_intensity", "vect_formula")

sim_param_unique <- 
  map_dfr(
    vect_of_vect_param, 
    add_values_param, 
    df = sim_param_base
  ) %>% 
  distinct() #bc base parameters appear twice
```

We want to compute our simulations for this set of parameters for every quasi experiment. Note that, in order to identify the effect of interest, we consider different types of models, depending on the identification strategy considered for each quasi experiment.

Then, for each set of parameters we want to run many iterations of the simulation, `n_iter`. We thus modify `sim_param` so that each set of parameter appears `n_iter` times. It will enable us to loop `compute_simulation` directly on `sim_param`.

```{r}
prepare_sim_param <- function(df_sim_param, 
                              vect_quasi_exp = c("random_days", "alert_co", "none"), 
                              n_iter = 10) {
  
  sim_param_clean <- df_sim_param %>%
    crossing(vect_quasi_exp) %>%
    rename(quasi_exp = vect_quasi_exp) %>%
    mutate(
      id_method = case_when(
        str_starts(quasi_exp, "alert") ~ "RDD",
        quasi_exp == "none" ~ "OLS",
      ),
      id_method = ifelse(
        quasi_exp == "random_days", 
        list(c("RCT", "IV")), 
        id_method
      )
    ) %>% 
    unnest(id_method) %>% 
    mutate(
      formula = case_when(
        id_method == "OLS" ~ str_replace_all(formula, "treated", "co"),
        id_method == "IV" ~ paste(
          str_remove_all(formula, "(\\+\\s)?\\btreated\\b(\\s\\+)?"),
          "| co ~ treated"
          ),
        TRUE ~ formula
      )
    ) %>% 
    filter(!str_detect(formula, "~\\s{0,2}\\|")) %>% 
    #adapting parameters
    mutate(
      p_obs_treat = ifelse(id_method == "RDD", p_obs_treat/5, p_obs_treat),
      p_obs_treat = ifelse(id_method == "OLS", NA, p_obs_treat),
      iv_intensity = ifelse(id_method != "IV", NA, iv_intensity)
    ) %>% 
    distinct() %>% #to erase the duplicates due to iv_intensity in non-iv id_methods
    arrange(id_method, n_days) %>% 
    crossing(rep_id = 1:n_iter) %>%
    select(-rep_id) 
  
  return(sim_param_clean)
} 

sim_param_evol <- prepare_sim_param(sim_param_unique, n_iter = 1)

# write_csv(sim_param, "../Outputs/sim_param.csv")
```

#### Running the simulation

We can then run the simulations for each set of parameter, using a `pmap_dfr` function.

```{r eval=FALSE}
run_all_sim <- function(data, sim_param, save_every, name_save) {
  output <- NULL
  future::plan(multicore)
  
  tic()
  for (i in 1:ceiling(nrow(sim_param)/save_every)) {
    params_slice <- sim_param %>% 
      slice((1 + save_every*(i-1)):(save_every*i))
    
    with_progress({
      p <- progressor(steps = nrow(params_slice))
      
      intermediary_output <- future_pmap_dfr(
        params_slice,
        compute_simulation,
        data = data,
        progressbar = p,
        .options = furrr_options(seed = TRUE)
      )
    })
    
    print(paste("Iteration =", i*save_every))
    output <- output %>% rbind(intermediary_output)
    saveRDS(
      intermediary_output, 
      here("R", "Outputs", "Sim_save", str_c(name_save, "_intermediary_", i*save_every,".RDS"))
    )
  }
  toc()
  return(output)
}

sim_evol <- run_all_sim(nmmaps_data, sim_param_evol, 10000, "sim_evol")

# with_progress({
#   p <- progressor(steps = nrow(sim_param))
#   
#   simulations_output <- future_pmap_dfr(
#     sim_param,
#     compute_simulation,
#     data = nmmaps_data,
#     progressbar = p,
#     .options = furrr_options(seed = TRUE)
#   )
# })

# beepr::beep(1)

# saveRDS(simulations_output, "R/Outputs/simulations_output.RDS")
```

#### Summarising the results

We then summarize our results, computing power, type M and so on for each set of parameters using he function `summarise_simulations`. Note that this function can only take as input a data frame produced by `compute_simulation` (or a mapped version of this function).

```{r}
summarise_simulations <- function(data) {
  
  data %>% 
    mutate(
      CI_low = estimate + se*qnorm((1-0.95)/2),
      CI_high = estimate - se*qnorm((1-0.95)/2),
      covered = (true_effect > CI_low & true_effect < CI_high), 
      covered = ifelse(p_value > 0.05, NA, covered) #to consider only significant estimates
    ) %>% 
    group_by(formula, quasi_exp, n_days, n_cities, p_obs_treat, percent_effect_size, id_method, iv_intensity) %>%
    summarise(
      power = mean(p_value <= 0.05, na.rm = TRUE)*100, 
      # average_p_value = mean(p_value, na.rm = TRUE),
      type_m = mean(ifelse(p_value <= 0.05, abs(estimate/true_effect), NA), na.rm = TRUE),
      type_s = sum(ifelse(p_value <= 0.05, sign(estimate) != sign(true_effect), NA), na.rm = TRUE)/n()*100,
      coverage_rate = mean(covered, na.rm = TRUE)*100,
      mse = mean((estimate - true_effect)^2, na.rm = TRUE),
      mean_estimate = mean(estimate, na.rm = TRUE),
      normalized_bias = mean(abs((estimate - true_effect/true_effect)), na.rm = TRUE),
      estimate_true_ratio = mean(abs(estimate/true_effect), na.rm = TRUE),
      mean_f_stat = mean(f_stat, na.rm = TRUE),
      .groups	= "drop"
    ) %>% 
    ungroup()
} 
```

We then run this function.

```{r eval=FALSE}
summary_evol <- summarise_simulations(sim_evol)

# saveRDS(summary_simulations, "R/Outputs/summary_simulations.RDS")
```

### Decomposing the number of observations into number of cities and days

In this section, we wonder whether only the total number of observations mater or whether decreasing the number of cities studied while keeping the number of observations constant (thus increasing the number of days) affects power and type M error. We thus aim to run several simulations with an identical number of observations but different numbers of cities/days. We repeat this for 3 different number of observations (1000, 2000 and 4000) to check the robustness of our findings.

```{r}
sim_param_decomp_nobs <- 
  tibble(n_cities = c(1, 3, 5, 10, 15, 25, 34)) %>%
  crossing(n_obs = c(1000, 2000, 3000)) %>%
  mutate(n_days = round(n_obs/n_cities)) %>% 
  select(-n_obs) %>% 
  full_join(sim_param_base, by = c("n_cities", "n_days")) %>% 
  fill(everything(), .direction = "updown") %>% 
  anti_join(#because sim_param_base not in the exact set of param we want here
    sim_param_base,
    by = c("n_cities", "n_days", "p_obs_treat", "percent_effect_size", "iv_intensity", "formula")
  ) %>% 
  prepare_sim_param(n_iter = 1000) 
```

```{r eval=FALSE}
sim_decomp_nobs <- run_all_sim(nmmaps_data, sim_param_decomp_nobs, 10000, "sim_decomp_nobs")

# saveRDS(sim_decomp_nobs, here("R", "Outputs", "sim_decomp_nobs.RDS"))
```


```{r}


summary_decomp_nobs <- summarise_simulations(sim_decomp_nobs)
# saveRDS(summary_decomp_nobs, here("R", "Outputs", "summary_decomp_nobs.RDS"))

summary_decomp_nobs %>% 
  mutate(n_obs = as.factor(round(n_days*n_cities/100)*100)) %>% 
  ggplot() + 
  geom_point(aes(x = n_cities, y = power, color = n_obs)) +
  geom_line(aes(x = n_cities, y = power, color = n_obs)) +
  facet_wrap(~ id_method)
  
```

### Decomp p_treat

```{r}
sim_param_decomp_ptreat <- 
  tibble(p_obs_treat = c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5)) %>%
  crossing(n_treat = c(500, 1000, 2000)) %>%
  mutate(
    n_obs = round(n_treat/p_obs_treat),
    n_cities = 50, #needs to be big enough
    n_days = round(n_obs/n_cities)
  ) %>% 
  select(-n_obs, -n_treat) %>% 
  full_join(sim_param_base, by = c("n_cities", "n_days", "p_obs_treat")) %>% 
  fill(everything(), .direction = "updown") %>% 
  anti_join(#because sim_param_base not in the exact set of param we want here
    sim_param_base,
    by = c("n_cities", "n_days", "p_obs_treat", "percent_effect_size", "iv_intensity", "formula")
  ) %>% 
  prepare_sim_param(n_iter = 1000) %>% 
  filter(id_method != "OLS")
```


```{r}
sim_decomp_ptreat <-  run_all_sim(nmmaps_data, sim_param_decomp_ptreat, 10000, "sim_decomp_ptreat")

# saveRDS(sim_decomp_ptreat, here("R", "Outputs", "sim_decomp_ptreat.RDS"))
```


```{r}

summary_decomp_ptreat <- summarise_simulations(sim_decomp_ptreat)
# saveRDS(summary_decomp_nobs, here("R", "Outputs", "summary_decomp_nobs.RDS"))

summary_decomp_ptreat %>% 
  mutate(n_treat = as.factor(round(n_days*n_cities*p_obs_treat/100)*100)) %>% 
  ggplot() + 
  geom_point(aes(x = p_obs_treat, y = power, color = n_treat)) +
  geom_line(aes(x = p_obs_treat, y = power, color = n_treat)) +
  facet_wrap(~ id_method)
```


<!-- # Heatmap -->

<!-- ```{r} -->
<!-- n_iter <- 3 -->

<!-- vect_n_days <- seq(500, 4000, 500) -->
<!-- vect_percent_effect_size <- seq(0.5, 1.5, 0.1) -->

<!-- sim_param_heatmap <- sim_param_base %>% -->
<!--   select(-n_days, -percent_effect_size) %>% -->
<!--   crossing(vect_n_days, vect_percent_effect_size) %>% -->
<!--   rename_with( ~ str_remove(., "vect_")) %>% -->
<!--   crossing(rep_id = 1:n_iter) %>% -->
<!--   select(n_days, n_cities, p_obs_treat, percent_effect_size, formula) %>% -->
<!--   mutate(quasi_exp = "random_days", id_method = "RCT") -->

<!-- future::plan(multicore) -->
<!-- tic() -->
<!-- with_progress({ -->
<!--   p <- progressor(steps = nrow(sim_param_heatmap)) -->

<!--   simulations_output_heatmap <- future_pmap_dfr( -->
<!--     sim_param_heatmap, -->
<!--     compute_simulation, -->
<!--     data = nmmaps_data, -->
<!--     progressbar = p, -->
<!--     .options = furrr_options(seed = TRUE) -->
<!--   ) -->
<!-- }) -->
<!-- toc() -->

<!-- all_simulations_heatmap <- simulations_output_heatmap %>% -->
<!--   as_tibble() -->

<!-- # saveRDS(all_simulations_heatmap, "R/Outputs/data_simulations_heatmap.RDS") -->
<!-- # all_simulations_heatmap <- readRDS("R/Outputs/data_simulations_heatmap.RDS") -->

<!-- summary_simulations_heatmap <- summarise_simulations(all_simulations_heatmap) -->

<!-- summary_simulations_heatmap %>% -->
<!--   filter(percent_effect_size > 0.4 & n_days > 500) %>% -->
<!--   filter(percent_effect_size < 1) %>%  -->
<!--   ggplot(aes(x = n_days, y = percent_effect_size)) + -->
<!--   geom_raster(aes(fill = type_m)) + # , interpolate = TRUE) + -->
<!--   # geom_contour(aes(z = type_m)) + -->
<!--   coord_fixed(ratio = 1000) + -->
<!--   scale_fill_continuous(limits = c(1, NA)) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- nmmaps_data %>% -->
<!--   draw_treated(p_obs_treat = 0.25, quasi_exp = "local") %>% -->
<!--   create_y1(percent_effect_size = 50, id_method = "DID", formula = "death_total ~ treated") %>%  -->
<!--   add_post_vars("DID") %>%  -->
<!--   # filter(!post) %>%  -->
<!--   mutate(year_month = str_c(year, as.numeric(month), sep = "_")) %>%  -->
<!--   group_by(city_treated, year_month) %>%  -->
<!--   summarise( -->
<!--     mean_y1 = mean(y1),  -->
<!--     mean_deaths = mean(death_total) -->
<!--   ) %>%  -->
<!--   ggplot() +  -->
<!--   geom_line(aes(x = year_month, y = mean_y1, color = city_treated, group = city_treated))  -->


<!-- ``` -->



