---
title: "Power Simulation Exercise: Interrupted time series"
author:
  - name: Vincent Bagilet 
    url: https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: Léo Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: distill::distill_article
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = FALSE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

# Purpose of the document

In this document, we carry out a simulation exercise to evaluate the performance of interrupted time series analyses in measuring the short-term effects of air pollution on health. 

# Description of the analysis

## Background on interrupted time series

Interrupted time series (ITS) is a method used to estimate the effect of an intervention affecting all units at the same time and/or when no control group is available. For instance, n the case of air pollution, one can think of the implementation of emissions standards at the national level, implying a reduction in air pollution. ITS methods can then be used to measure the impact of this decrease in pollutant concentration on health outcomes such as mortality or hospital admissions. Considering a simple hypothetical example, assume that at time $t_{treat}$, a new technology is implemented, reducing ambient PM10 concentration by $10 \mu g/m^3$ for all observation units. This leads to a reduction in hospital admissions. ITS aim to recover the impact of this reduction in PM10 concentration on hospital admissions. In this very simple example, the model to estimate would be the following:

\begin{equation}
  h_{ct} = \alpha + \beta p_{ct} + \mu T_{t} + \boldsymbol{W_{ct}'\delta} +  \boldsymbol{C_{ct}'\gamma} + \epsilon_{ct} 
  \label{eq_its}
\end{equation}
    
where $h_{ct}$ represents the daily number of deaths or emergency admissions, for a given city $c$ at time $t$, $p$ a pollutant's concentration, $T$ the treatment variable which is equal to zero for $t < t_{treat}$ and 1 otherwise. $W$ is a set of weather covariates and $C$ calendar indicators. $\mu$ is the parameter of interest here. Of course, we can also consider more complex models than this basic one..For instance, $T$ might vary across cities, might vary in time in more complex ways than only a binary change, etc.

## Data

We use data from Marseille over the 2008-2018 period (*est ce qu'on utiliserait pas directement le jeu de données complet, maintenant qu'on l'a ?*). The data set contains records of hospital admissions and deaths, mean concentration data for various air pollutants, a bunch of weather variables and calendar control variables (such as school holidays for instance). All variables are at the daily level and aggregated over the city. There is therefore a unique observation per date in the data set.

## Analysis

### Overall setting

To sum up the analysis, the aim is to measure the performance of ITS methods to recover the true effect of the treatment (*ie* to recover $\mu$ in the previous equation). To do so, we proceed as follows:

1. We estimate our "true" model 
$$h_{ct} = \alpha + \beta p_{ct} + \boldsymbol{W_{ct}'\delta} +  \boldsymbol{C_{ct}'\gamma} + \epsilon_{ct}$$
*ie* assuming $T_{t} = 0 ~\ \forall t$ and recover the "true" values of our different parameters, $\alpha_0, \beta_0, \delta_0, \gamma_0$
1. We create a fake pollution shock $T_{t}$ and create fake pollution concentration $p^{fake}$ corresponding to this shock
*eg* we increase the PM10 concentration values by 10$\mu g/m^3$ after a given date, eg April 4, 2013. For instance, if the measured value of PM10 concentration on December 12, 2015 is 87$\mu g/m^3$, we create a fake concentration value of 97$\mu g/m^3$.
1. We posit an hypothetical effect of this fake pollution shock on health ($\mu_0$)  
*eg* a permanent increase of 10$\mu g/m^3$ in PM10 is associated with a 0.4% increase in the number of deaths per day
1. We create fake deaths values $h^{fake}$, predicting outcomes from our complete model
$$h_{ct}^{fake} = \alpha_0 + \beta_0 p_{ct}^{fake} + \mu_0 T_{t} + \boldsymbol{W_{ct}'\delta_0} +  \boldsymbol{C_{ct}'\gamma_0} + e_{ct}$$
We generate error terms $e_{ct}$ using the mean and standard deviation of the residuals $\epsilon_{ct}$ from the estimation of our "true" model. Note that, in the potential outcomes framework, these fake death values correspond to our Y(1). The Y(0) corresponds to the actual number of deaths per day.
1. We estimate our model to recover $\hat{\mu}$
$$h_{ct}^{fake} = \alpha + \beta p_{ct}^{fake} + \mu T_{t} + \boldsymbol{W_{ct}'\delta} +  \boldsymbol{C_{ct}'\gamma} + \epsilon_{ct}$$

1. We can then compare $\hat{\mu}$ to $\mu_0$
1. We run the previous steps 1000 times and compute the average bias, type M, type S and power of the estimate. It enable us to get an idea of the performance of ITS in recovering effects of a given size.

We can then rerun this analysis for different effect sizes and number of observations to investigate how the performance of ITS methods vary with the effect size and the number of observations. Note that, to limit the number of estimation, we do not vary effect size and number of observation for all treatments (only for the simple dummy treatment and a realistic one).

### More complex treatments

In the description of the overall setting, we considered a very simple treatment (constant across individual and time). We can consider more complex treatments:

- Varying across individuals in a deterministic or stochastic way,
- Varying in time, for instance dying out in a linear or non linear way, 
- Variations correlated with weather or calendar covariates

- We also consider non linear relationships between air pollution and health outcomes.

# Actual implementation

In this section, we follow the steps described in the section "overall setting" and carry out our analysis. (*Je ne mets pas de code là pour l'instant pour que ce soit plus lisible mais c'est la partie où il y aura vraiment le code*)

## Loading and Formatting Data

We load the packages and the data and wrangle it into a format well suited for this analysis.

## Estimation of the model

In this section we:

- We estimate the "true" model $h_{ct} = \alpha + \beta p_{ct} + \boldsymbol{W_{ct}'\delta} +  \boldsymbol{C_{ct}'\gamma} + \epsilon_{ct}$
- We store estimates of the parameters $\alpha_0, \beta_0, \delta_0, \gamma_0$ and the mean and variance of the residuals $\epsilon_{ct}$

### Definition of the treatment

Here we define:
- $n_e$ Effect sizes
- $n_{obs}$ Number of observations
- $n_T$ different treatments, *ie* one new column in the data set for each treatment, giving the new value of pollution

### Creating fake health

- We create the 1000 fake hospital admission and death time series

### Model estimation

- For each time series and each treatment, we estimate our model. We thus retrieve $1000 \times n_T$ estimates
- For the dummy treatment and a complex one, we also vary effect size. We get $2 \times 1000 \times n_e$ estimates
- For the dummy treatment and a complex one, we also vary effect size. We get $2 \times 1000 \times n_e$ estimates
- Then, for all these estimates, we compute different measures: bias, power, rate of type I, type M and type S error, signal to noise ratio, etc.

### Summary of the performance of ITS

The objective of this analysis is to compare the performance of different estimation methods. We therefore need a systematic way to compare them. In this section, we create the summary tables for ITS that will enable us to compare its performance to the other methods.

<!-- We load the packages and the data: -->

<!-- ```{r, echo=TRUE, message = FALSE, warning = FALSE} -->
<!-- library(here) # for files paths organization -->
<!-- library(tidyverse) # for data manipulation and visualisation -->
<!-- library(modelr) # modeling within the tidyverse -->
<!-- library(retrodesign) # formulas for type-m and type-s errors -->
<!-- library(knitr) # for tables -->

<!-- source(here::here("R", "Scripts", "script_custom_ggplot_theme.R")) -->

<!-- data <- readRDS(here::here("R", "Inputs", "data_marseille_daily_2008_2018.rds")) -->
<!-- ``` -->

<!-- Then, we format the data, adding relevant variables. -->

<!-- ```{r, echo=TRUE, message = FALSE, warning = FALSE} -->
<!-- data_formated <- data %>% -->
<!--   mutate_at(vars(year, holidays_dummy, bank_day_dummy), ~ as.factor(.))  %>% -->
<!--   filter(!(year %in% c("2008", "2009"))) %>% #emergency admissions data available from 2010  -->
<!--   mutate(wind_direction_categories = cut(wind_direction, breaks = seq(0, 360, by  = 90), include.lowest = TRUE) %>% -->
<!--   recode(., "[0,90]" = "North-East", "(90,180]" = "South-East", "(180,270]" = "South-West", "(270,360]" = "North-West")) %>% -->
<!--   mutate( -->
<!--     wind_speed_lag_1 = lag(wind_speed), -->
<!--     wind_direction_categories_lag_1 = lag(wind_direction_categories), -->
<!--     rainfall_dummy = ifelse(rainfall_height>0, "True", "False") -->
<!--   ) %>% -->
<!--   mutate_at(vars(mean_no2_agregate, mean_o3_l, mean_pm10_agregate, mean_pm25_l, temperature_average, humidity_average), list("01" = ~ zoo::rollmean(., k = 2, align = "right", fill = NA))) %>% -->
<!--   mutate_at(vars(mean_no2_agregate_01:humidity_average_01), ~ scale(.)) %>% -->
<!--   mutate(emergency_cv_r = log(emergency_cv_r)) %>% -->
<!--   select(date, emergency_cv_r, mean_no2_agregate, temperature_average_01, humidity_average_01, rainfall_dummy, wind_speed, wind_speed_lag_1, wind_direction_categories, wind_direction_categories_lag_1, weekday, holidays_dummy, bank_day_dummy, month, year) -->
<!-- ```       -->


<!-- # Simulations -->

<!-- ### Preparing the simulations -->

<!-- We first want to have an idea of the effect of `mean_no2_agregate` on `emergency_cv_r` in our data: -->

<!-- ```{r, echo=TRUE, message = FALSE, warning = FALSE} -->
<!-- fitted_model_real_data <- lm(emergency_cv_r ~ mean_no2_agregate +  -->
<!--               temperature_average_01 + I(temperature_average_01^2) + -->
<!--               humidity_average_01 + -->
<!--               rainfall_dummy + -->
<!--               wind_speed + wind_speed_lag_1 +  -->
<!--               wind_direction_categories + wind_direction_categories_lag_1 + -->
<!--               weekday + holidays_dummy + bank_day_dummy + month*year, data = data_formated) -->

<!-- fitted_model_real_data %>% -->
<!--   broom::tidy(., conf.int = TRUE) %>% -->
<!--   filter(term == "mean_no2_agregate") -->
<!-- ```       -->

<!-- We see that a one standard deviation in `mean_no2_agregate` lead to 0.05% (95% CI: [-0.035; 0.139]) increase in `emergency_cv_r`. -->

<!-- Using our data and this model, we can simulate fake-data for different effect sizes of `mean_no2_agregate` on -->
<!-- `emergency_cv_r`. We first create a nested tibble where store our data, variables recording the effect sizes (effect sizes = 0.05%, 0.1% , 0.5% and 1%) and the sample size (N = 1068, 1780, 2492, 3650) and the fit of the true model. -->

<!-- ```{r, echo=TRUE, message = FALSE, warning = FALSE} -->
<!-- data_simulations <- data %>%  -->
<!--   nest(data = everything()) %>% -->
<!--   crossing(effect_size = c(0.0005, 0.001, 0.005, 0.01)) %>% -->
<!--   crossing(sample_size = c(1068, 1780, 2492, 3650)) %>% -->
<!--   mutate(true_model = map(data, ~ lm(emergency_cv_r ~ mean_no2_agregate +  -->
<!--                                        temperature_average_01 + I(temperature_average_01^2) + -->
<!--                                        humidity_average_01 + -->
<!--                                        rainfall_dummy + -->
<!--                                        wind_speed + wind_speed_lag_1 +  -->
<!--                                        wind_direction_categories + wind_direction_categories_lag_1 + -->
<!--                                        weekday + holidays_dummy + bank_day_dummy + month*year, data = .))) -->
<!-- ```       -->

<!-- Once we have fitted the true model on our data, we need to modifiy the effect size of `mean_no2_agregate` in order to create new fake `emergency_cv_r` observations: -->

<!-- ```{r, echo=TRUE, message = FALSE, warning = FALSE} -->
<!-- function_model_effect_size <- function(model, effect_size){ -->
<!--   model$coefficients[2] <- effect_size -->
<!--   return(model) -->
<!-- }  -->

<!-- data_simulations <- data_simulations %>% -->
<!--   mutate(true_model = map2(true_model, effect_size, ~ function_model_effect_size(.x, .y))) -->
<!-- ```         -->

<!-- We finally need to retrieve the mean and the standard deviation of residuals found for the `fitted_model_real_data` to add some noise in our simulations: -->

<!-- ```{r, echo=TRUE, message = FALSE, warning = FALSE} -->
<!-- # get the distribution of residuals -->
<!-- mean_residuals <- mean(residuals(fitted_model_real_data)) -->
<!-- sd_residuals <- sd(residuals(fitted_model_real_data)) -->
<!-- ```       -->

<!-- ### Function to create fake-data and run the model -->

<!-- We create a function to create fake-data, run on them the model for which know the true effect size and retrieve the estimate and p-value for `mean_no2_agregate`. The function takes three arguments:the data, a sample size, and a model: -->

<!-- ```{r, eval = FALSE, message = FALSE, warning = FALSE} -->
<!-- function_power_simulation <- function(data, sample_size, true_model){ -->
<!-- simulation_results <- data %>% -->
<!--   sample_n(., sample_size, replace = TRUE) %>% -->
<!--   add_predictions(true_model, var = "predicted_emergency_cv_r") %>% -->
<!--   mutate(predicted_emergency_cv_r = predicted_emergency_cv_r + rnorm(nrow(.), mean_residuals, sd_residuals)) %>% -->
<!--   lm(predicted_emergency_cv_r ~ mean_no2_agregate +  -->
<!--        temperature_average_01 + I(temperature_average_01^2) + -->
<!--        humidity_average_01 + -->
<!--        rainfall_dummy + -->
<!--        wind_speed + wind_speed_lag_1 +  -->
<!--        wind_direction_categories + wind_direction_categories_lag_1 + -->
<!--        weekday + holidays_dummy + bank_day_dummy + month*year, data = .) %>% -->
<!--   broom::tidy() %>% -->
<!--   filter(term == "mean_no2_agregate") %>% -->
<!--   select(estimate, p.value) -->

<!--   return(simulation_results) -->
<!-- } -->
<!-- ```       -->

<!-- ### Running and Cleaning the Simulations -->

<!-- We run the simulations: -->

<!-- ```{r, eval = FALSE, message = FALSE, warning = FALSE} -->
<!-- data_simulations <- data_simulations %>% -->
<!--   crossing(id_sim = seq(1:1000)) %>% -->
<!--   mutate(simulation_results = pmap(list(data, sample_size, true_model), function_power_simulation)) -->
<!-- ```  -->

<!-- We compute the power, type m and s errors: -->

<!-- ```{r, eval = FALSE, message = FALSE, warning = FALSE} -->
<!-- data_simulation_results <- data_simulations %>% -->
<!--   select(-data, - true_model) %>% -->
<!--   unnest(simulation_results) -->

<!-- data_simulation_results_power <- data_simulation_results %>% -->
<!--   group_by(sample_size, effect_size) %>% -->
<!--   summarise(power = mean(p.value<= 0.05)*100) -->

<!-- data_simulation_results_m_error <- data_simulation_results %>% -->
<!--   group_by(sample_size, effect_size) %>% -->
<!--   filter(p.value <= 0.05) %>% -->
<!--   summarise(type_m_error = mean(abs(estimate)/effect_size)) -->

<!-- data_simulation_results_s_error <- data_simulation_results %>% -->
<!--   group_by(sample_size, effect_size) %>% -->
<!--   filter(p.value <= 0.05) %>% -->
<!--   summarise(type_s_error = (sum(estimate<0)/n())*100) -->

<!-- # join the figures together -->
<!-- data_simulation_results_all <- left_join(data_simulation_results_power, data_simulation_results_m_error, by = c("sample_size", "effect_size")) %>% -->
<!--   left_join(., data_simulation_results_s_error, by = c("sample_size", "effect_size")) -->
<!-- ```  -->


<!-- ### Simulation Results -->

<!-- We plot below the simulations results: -->

<!-- ```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width=20, fig.height=8, fig.fullwidth=TRUE, dev = "CairoPNG"} -->
<!-- # load simulation results data -->
<!-- data_simulation_results <- readRDS(here::here("1.data", "1.standard_model", "simulation_results.RDS")) -->

<!-- # make the graph -->
<!-- graph <- data_simulation_results %>% -->
<!--   mutate_at(vars(sample_size, effect_size), ~ as.factor(.)) %>% -->
<!--   mutate(effect_size = case_when(effect_size == "5e-04" ~ "+0.05%", -->
<!--                                  effect_size == "0.001" ~ "+0.1%", -->
<!--                                  effect_size == "0.005" ~ "+0.5%", -->
<!--                                  effect_size == "0.01" ~ "+1%")) %>% -->
<!--   rename("Power (%)" = power, "Type M Error (Bias)" = type_m_error, "Type S Error (%)" = type_s_error) %>% -->
<!--   pivot_longer(cols= c(`Power (%)`:`Type S Error (%)`), names_to = "variable", values_to = "value") %>% -->
<!--   ggplot(., aes(x = sample_size, y = value, group = effect_size, color = effect_size)) + -->
<!--   geom_line(linetype = "dashed") + -->
<!--   geom_point(size = 4) + -->
<!--   scale_color_manual(values=c("#f2cc8f", "#3d405b", "#81b29a", "#e07a5f")) + -->
<!--   facet_wrap(~ variable, scales = "free", nrow = 1) + -->
<!--   xlab("Sample Size") + ylab("Value") + -->
<!--   ggtitle("Power Simulations Results", subtitle = "The effect of a one standard deviation increase in NO2 on Emergency Admissions.") + -->
<!--   labs(color = "Effect Size:") + -->
<!--   custom_theme + -->
<!--   theme(legend.position = "top", legend.justification = "left", legend.direction = "horizontal") -->

<!-- # print graph -->
<!-- graph -->

<!-- # save graph -->
<!-- ggsave(graph, filename = here::here("3.outputs", "1.figures", "graph.pdf"),  -->
<!--        width = 45, height = 20, units = "cm", device = cairo_pdf) -->
<!-- ```  -->

<!-- We can compare our results with those of `retrodesign` package: -->

<!-- ```{r, eval = TRUE, message = FALSE, warning = FALSE} -->
<!-- tibble(effect_size = c(0.0005, 0.001, 0.005, 0.01), standard_error = c(rep(0.00045, 4))) %>% -->
<!--   mutate(power = map2(effect_size, standard_error, ~ retro_design(.x, .y)$power*100), -->
<!--          type_s = map2(effect_size, standard_error, ~ retro_design(.x, .y)$typeS*100), -->
<!--          type_m = map2(effect_size, standard_error, ~ retro_design(.x, .y)$typeM)) %>% -->
<!--   unnest(cols = c(power, type_s, type_m)) %>% -->
<!--   mutate_at(vars(power:type_m), ~ round(., 1)) %>% -->
<!--   mutate(effect_size = case_when(effect_size == "5e-04" ~ "+0.05%", -->
<!--                                  effect_size == "0.001" ~ "+0.1%", -->
<!--                                  effect_size == "0.005" ~ "+0.5%", -->
<!--                                  effect_size == "0.01" ~ "+1%")) %>% -->
<!--   kable() -->
<!-- ```  -->









