---
title: "Systematic literature review - Getting abstracts"
author:
- name: Vincent Bagilet
  url: https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet
  affiliation: Columbia University
  affiliation_url: https://www.columbia.edu/
- name: Léo Zabrocki
  url: https://www.parisschoolofeconomics.eu/en/
  affiliation: Paris School of Economics
  affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output:
  distill::distill_article: default
  html_notebook: default
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

```{r include=FALSE}
library(tidyverse)
library(fulltext)
library(tidytext)
library(wordcloud)
library(retrodesign)
library(mediocrethemes)
library(lubridate)
library(DT)

set_mediocre_all()
set.seed(1)
```

# Purpose of the document

In the present document, we retrieve abstracts for our systematic review of the literature on short term health effects of air pollution. 

# Selecting articles and retrieving metadata

We use the [`fulltext`](https://books.ropensci.org/fulltext/) package to get the abstract of each article corresponding to our search query. 
<!-- This set of articles might be too restrictive and we may want to broaden our scope later.  -->

We focus on articles published on Scopus and Pubmed. To access Scopus API, one needs to register to get an API key (stored in the .Renviron) for Elsevier and a Crossref TDM API key. Note that downloading of full texts may not work if one is not connected directly to their institution internet network. Pubmed articles are accessed via Entrez. An API key enables to increase the number of requests per seconds from 3 to 10. More information on authentication is available on [the `fulltext` manual](https://books.ropensci.org/fulltext/authentication.html).

## Set of articles to consider

First of all, we need to clearly define the set of articles we want to consider in this analysis. Our baseline search query is: 

'"air pollution" AND ("emergency" OR "mortality") AND ("particulate matter" OR ozone OR "nitrogen dioxyde" OR "sulfur dioxide")'. 

This returns about 57000 articles (without accounting for duplicates in between journals). This number being rather large, we want to narrow it down. To focus on the epidemiological literature, we add the query term `poisson`. To get causal analyses, we add the term `causal`. 

```{r eval = FALSE}
query <- 'TITLE(("air pollution" OR "air quality" OR "particulate matter" OR ozone OR "nitrogen dioxide" OR "sulfur dioxide" OR "PM10" OR "PM2.5" OR "carbon dioxide" OR "carbon monoxide") AND ("emergency" OR "mortality" OR "stroke" OR "cerebrovascular" OR "cardiovascular" OR "death" OR "hospitalization") AND NOT ("long term" OR "long-term")) AND "short term"'# AND (poisson OR causal)'

opts_entrez <- list(use_history = TRUE)

#Run a search
search <- ft_search(query, from = "scopus", limit = 2000)
search_entrez <- ft_search(query, from = "entrez", limit = 200, entrezopts = opts_entrez)
```

The final query is thus:

'TITLE(("air pollution"  OR "air quality" OR "particulate matter" OR ozone OR "nitrogen dioxide" OR "sulfur dioxide") AND ("emergency" OR "mortality") AND NOT ("long term" or "long-term")) AND ("particulate matter" OR ozone OR "nitrogen dioxide" OR "sulfur dioxide")'

Then, we retrieve the related metadata. The metadata from different sources having different shapes, we only select a few relevant columns to build an overall metadata set.

```{r eval = FALSE}
metadata_scopus <- search$scopus$data %>% 
  as_tibble() %>% 
  rename_all(function(x) str_remove_all(names(.), "prism:|dc:")) %>% 
  rename_all(function(x) str_replace_all(names(.), "-", "_")) %>% 
  select(doi, title, creator, publicationName, pubmed_id, coverDate) %>% 
  rename(
    authors = creator,
    journal = publicationName
  ) %>% 
  mutate(
    pubmed_id = ifelse(!str_detect(pubmed_id, "[0-9]{7}"), NA, pubmed_id),
    pub_date = ymd(coverDate)
  ) %>% 
  select(-coverDate)

saveRDS(metadata_scopus, "../Outputs/metadata_scopus.RDS")

metadata_entrez <- search_entrez$entrez$data %>% #search_entrez$entrez$data 
  as_tibble() %>% 
  # rename(id = uid) %>% 
  select(doi, title, authors, fulljournalname, pmid, pubdate) %>% 
  rename(
    journal = fulljournalname,
    pubmed_id = pmid
  ) %>% 
  mutate(
    pubmed_id = ifelse(!str_detect(pubmed_id, "[0-9]{7}"), NA, pubmed_id),
    pub_date = ymd(pubdate)
  ) %>% 
  select(-pubdate)

saveRDS(metadata_entrez, "../Outputs/metadata_entrez.RDS")

metadata_lit_review <- metadata_scopus %>% 
  rbind(metadata_entrez) %>% 
  filter(!is.na(doi)) %>% 
  mutate(pb_doi = str_detect(doi, "[<>;]")) %>% #some dois are not valid
  filter(pb_doi == FALSE) %>% 
  select(-pb_doi) %>% 
  group_by(doi) %>% 
  filter(pub_date == max(pub_date, na.rm = TRUE)) %>% #some articles have been published twice
  mutate(n_with_doi = n()) %>% 
  filter(n_with_doi == 1 | (n_with_doi > 1 & pubmed_id == max(pubmed_id, na.rm = TRUE))) %>% #two wierd articles with separate author names, I select one randomly
  select(-n_with_doi) %>% 
  ungroup()

# saveRDS(metadata_lit_review, "../Outputs/metadata_lit_review.RDS")
```

# Analysis of the power in the literature

## Retreiving abstracts

First, we focus on the abstracts and download them. 

There is no `fulltext` function to access abstracts from Entrez. Therefore, using the DOI, we get the abstracts from Semantic Scholar. We also access Scopus abstracts from Semantic Scholar since, due to my IP address being located outside of Columbia, I cannot access the texts and abstracts from Scopus.

In Semantic Scholar, there is a rate limit of 100 articles per 5 min or 20 articles per minute. We therefore need to pause the system to be able to download everything. In addition, some DOIs are not valid so we filtered them out in a previous step (`pb_doi`).^[In case any problem remains, we use tryCatch to record the DOIs corresponding to errors in order to be able to handle them later.]

```{r eval = FALSE}
get_abstracts <- function(doi) {
  vect_doi <- unique(doi)
  number_periods <- (length(vect_doi) - 1) %/% 20
  abs <- NULL

  message(str_c("Total downloading time: ", number_periods, "min"))
  
  for (i in 0:number_periods) {
    
    doi_period <- vect_doi[(20*i+1):(20*(i+1))]
    doi_period <- doi_period[!is.na(doi_period)]
    
    skip_to_next <- FALSE #to handle issues, using tryCatch
    
    possible_error <- tryCatch(
      abs_period <- doi_period %>%
        ft_abstract(from = "semanticscholar") %>%
        .$semanticscholar %>%
        as_tibble() %>%
        unnest(cols = everything()) %>%
        pivot_longer(everything(), names_to = "doi", values_to = "abstract") %>%
        filter(doi != abstract),
      error = function(e) e
    )
    
    if (inherits(possible_error, "error")) {
      warning(str_c("The abstracts for the following articles could not be downloaded: ", str_c(doi_period, collapse = ",")))
      next
    } else {
       abs <- abs %>%
        rbind(abs_period)
    }
    
    if (i < number_periods & number_periods != 0) {
      message(str_c("Remaining time: ", (number_periods - i), "min"))
      Sys.sleep(63)
    }
  } 
  
  return(abs)
}

abstracts <- metadata_lit_review %>%
  distinct(title, .keep_all = TRUE) %>% 
  .$doi %>% 
  get_abstracts()  %>% 
  left_join(metadata_lit_review, by = "doi") 

# saveRDS(abstracts, "../Outputs/abstracts.RDS")

```

## Retreiving effects and confidence interavals

```{r}
abstracts <- readRDS("../Outputs/abstracts.RDS")
```

Then we want to extract the effects, odds ratio and associated confidence intervals in the abstracts. Part of the literature, display directly effects and confidence intervals in their abstracts.^[We analyse the characteristics of articles doing soin another document.] We thus consider these abstracts (about 684 abstracts) and identify effects and CIs as follows:

- CI: any couple of numbers following by less than 4 characters a string describing a confidence interval ("95%", "CI", "confidence interval"). We also consider confidence intervals of the shape "(-8.7, 54.7)": it needs to have a shape "(number separator number)" and needs to be preceded by a number less than 5 characters away.
- Effect: the first number preceding by less than 30 characters a similar string describing a confidence interval (we do not consider "95%").


```{r}
string_confint <- "((?<!(\\.|\\d))95\\s?%|(?<!(\\.|\\d))95\\s(per(\\s?)cent)|\\bCI(s?)\\b|\\bPI(s?)\\b|\\b(i|I)nterval|\\b(c|C)onfidence\\s(i|I)nterval|\\b(c|C)redible\\s(i|I)nterval|\\b(p|P)osterior\\s(i|I)nterval)"
# string_confint_detect <- "(?<!(\\.|\\d))(95)?\\s?%?(per(\\s?)cent)?\\s?(\\bCI(s?)|(\\b(c|C)onfidence\\s)?(i|I)nterval(s?))(\\s?(\\(|\\[)?CIs?(\\)|\\])?)?"
num_confint <- "(-\\s?|−\\s?)?[\\d\\.]{1,7}[–\\s:;,%\\-to\\‐-]{1,5}(-\\s?|−\\s?)?[\\d\\.]{1,7}"
num_effect <- "(-\\s?|−\\s?)?[\\d\\.]{1,7}"

effects_CI <- abstracts %>% 
  mutate(
    abstract = str_replace_all(abstract, "·", "."),
    abstract = str_replace_all(abstract, "C.I.", "CI")
  ) %>% 
  select(doi, abstract) %>% 
  unnest_tokens(sentence, abstract, token = "sentences", to_lower = FALSE, drop = FALSE) %>% 
  mutate(
    contains_CI = str_detect(sentence, string_confint),
    sentence = str_replace_all(sentence, "(?<=(?<!\\.)(?<!\\d)\\d{1,4}),(?=(\\d{3}(?!\\.)))", "")
  ) %>%
  filter(contains_CI) %>%
  # mutate(
  #   effect_CI = str_extract_all(sentence, str_c(
  #   num_effect, "[^\\d\\.]{0,17}([^\\.\\d]",
  #   string_confint, ").{0,35}((?<=", string_confint, "[^\\d]{0,4})",
  #   num_confint, "[^\\d\\.]{0,3}[\\)\\];,])|", num_effect, "[^\\d\\.]{0,5}(\\(|\\[)", num_confint, "%?[\\)\\];](?<![^\\.\\d]95)"))
  # ) %>%
  # unnest(effect_CI, keep_empty = TRUE) %>% 
  mutate(
    CI = str_extract_all(sentence, str_c(
      "((?<=", string_confint, "[^\\d]{0,4})", num_confint, ")|",
      "(?<=", num_effect, "[^\\d\\.]{0,5})(?<=(\\(|\\[))", num_confint, "(?=%?[\\)\\];])")
    ),   
    effect = str_extract_all(sentence, str_c(
      num_effect, "(?=[^\\d\\.]{0,30}([^\\.\\d]", string_confint, "))(?<!\\b95)|",
      num_effect, "(?=[^\\d\\.]{0,5}(\\(|\\[)", num_confint, "(?=%?[\\)\\];]))(?<![^\\.\\d]95)")
    )
  ) 
  # select(-effect_CI)
```

These lines of code return a set of confidence intervals and effect for each sentence containing the phrase ("CI", "confidence interval", etc). We could improve this code by linking directly each effect to its confidence interval. For now, if we do not detect the same number of effects and confidence interval in a sentence, we drop the sentence, even though there are 5 pairs of effect-CI and only one of them is badly detected.

Note that some problems might remain with our estimates and CIs detected. Yet, a vast majority of estimates are correctly detected. Here are examples of the confidence intervals and effects detected using our current method:

```{r}
random_sentences <- sample(1:length(effects_CI$sentence), 5)
str_view_all(effects_CI$sentence[random_sentences], str_c(
    num_effect, "(?=[^\\d\\.]{0,17}([^\\.\\d]", string_confint, "))(?<!\\b95)|",
      num_effect, "(?=[^\\d\\.]{0,5}(\\(|\\[)", num_confint, "(?=%?[\\)\\];]))(?<![^\\.\\d]95)",
    "|((?<=", string_confint, "[^\\d]{0,4})", num_confint, ")|",
      "(?<=", num_effect, "[^\\d\\.]{0,5})(?<=(\\(|\\[))", num_confint, "(?=%?[\\)\\];])"))
```

Once the effects and CI are identified, some wrangling is necessary in order to get the data into a usable format. We also choose to **drop effects which do not fall into the CI** (62 estimates) in order to get rid off most of the poorly detected effects-CIs. 

```{r}
effects_CI_clean <- effects_CI %>% 
  filter(lengths(effect) == lengths(CI)) %>% #if number of effects != nb of CI for a sentence,
  #can't attribute effects to CI so, drop sentence
  unnest(c(effect, CI), keep_empty = TRUE) %>%
  mutate(CI = str_remove_all(CI, "\\s")) %>% 
  # separate(CI, into = c("low_CI", "up_CI"), "([\\s,]+)|(?<!^)[-–]") %>% 
  separate(CI, into = c("low_CI", "up_CI"), "(?<!^)[–:;,%\\-to\\‐]{1,5}") %>%
  mutate(across(c("effect", "low_CI", "up_CI"), .fns = as.numeric)) %>% 
  mutate(
    low_CI = ifelse(is.na(up_CI), NA, low_CI),
    up_CI = ifelse(is.na(low_CI), NA, up_CI),
    effect = ifelse(is.na(low_CI), NA, effect)
  ) %>% 
  select(-sentence, -abstract, -contains_CI) %>%
  filter(!is.na(effect)) %>%  
  filter(effect > low_CI & effect < up_CI)

articles_effect <- abstracts %>% 
  left_join(effects_CI_clean, by = "doi")

saveRDS(articles_effect, "../Outputs/articles_effect.RDS")
```



```{r}
articles_effect %>%
  mutate(
    contains_CI = str_detect(abstract, "(95%|\\bCI\\b|\\b(c|C)onfidence (i|I)nterval\\b)")
  ) %>%
  filter(contains_CI) %>%
  group_by(doi) %>%
  summarise(has_effect = mean(effect, na.rm = TRUE)) %>%
  count(ret = !is.nan(has_effect)) %>%
  mutate(ret = ifelse(ret, "Yes", "No")) %>%
  kable(col.names = c("Effect retreived", "Number of articles"), caption = "Number of articles for which at least one effect is retrieved (out of those containing the phrase CI)")

no_effect <- effects_CI %>%
  mutate(effect = as.character(effect)) %>%
  filter(effect == "character(0)")

str_view_all(no_effect$sentence, "lom")
```


## Retreiving additional information

It might also be interesting to information about the type of pollutant considered in the study, the study period, the number of observations or the type of outcome studied (mortality, emergency admissions, stroke, cerebrovascular or cardiovascular diseases). We also use regex to recover this information.

```{r}
#To start on the right foot, a bit of cleaning
abstracts_only <- abstracts %>% 
  mutate(
    abstract = str_replace_all(abstract, "·", ".")
  ) %>% 
  select(doi, abstract)
```


### Length of the study period

We first detect begin and end dates for the period of study and compute the length of study, after transforming the text data to a date format using the function `text_to_date`.

```{r}
month_regex <- "\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|(Nov|Dec)(?:ember)?)"
date_regex <- str_c("(", month_regex, "\\s){0,1}(19|20)\\d{2}")

text_to_date <- function(date_text) {
  year <-  str_remove_all(date_text, "[^\\d]")
  month <- match(str_remove_all(date_text, "[\\d\\s]"), month.name)
  month <- ifelse(is.na(month), "01", month)
  date <- dmy(str_c("01-", str_pad(month, width = 2, pad = 0), "-", year))
  return(date)
}

abstracts_length_study <- abstracts_only %>%
  mutate(
    dates_obs = str_extract_all(
      abstract, 
      str_c("(", date_regex, "|", month_regex, ")( to |\\s?—\\s?|\\s?-\\s?| and )", date_regex)
    )
  ) %>% 
  unnest(dates_obs, keep_empty = TRUE) %>%
  separate(dates_obs, into = c("begin_obs", "end_obs"), "( to |\\s?—\\s?|\\s?-\\s?| and )") %>% 
  mutate(#when begin_obs and end_obs are in the same year, 
    #we only get the month for begin_obs
    begin_obs = ifelse(
      !str_detect(begin_obs, "\\d"), 
      paste(begin_obs, str_remove_all(end_obs, "[^\\d]")),
      begin_obs)
  ) %>%
  mutate(
    begin_obs = text_to_date(begin_obs),
    end_obs = text_to_date(end_obs),
    length_study = time_length(end_obs - begin_obs, unit = "days"),
    length_study = ifelse(length_study < 0 | end_obs > today(), NA, length_study)
  ) %>% 
  select(doi, length_study) %>% 
  distinct()
```

### Number of cities considered

We then try to detect the number of cities considered in each abstract. To do so, we use two different techniques:

- We see whether the abstract uses phrases such as "in 34 cities" and retrieve this 34. We also convert all text-numbers (*eg* "one") into their numerical values. When we get several numbers, we take a conservative approach and only keep the largest one.
- We count the number of cities names appearing uniquely in each abstract. To do so, we use a database of cities names. We restrict our sample to large cities. 


```{r}
number_word <- tibble(
  number = 1:5000, 
  word = english::words(1:5000)
)

abstracts_numbers <- abstracts_only %>% 
  unnest_tokens(word, abstract) %>%
  left_join(number_word, by = "word") %>%
  mutate(word = ifelse(!is.na(number), number, word)) %>% 
  select(-number) %>% 
  group_by(doi) %>% 
  summarize(abstract = str_c(word, collapse = " ")) %>%
  ungroup() %>% 
  mutate(abstract = str_remove_all(abstract, "-"))

#Source: https://simplemaps.com/data/world-cities
worldcities <- read_csv("../Inputs/worldcities.csv")

worldcities_large <- worldcities %>%
  mutate(
    city_regex = str_c("\\b", city_ascii, "\\b"),
    city_regex = str_to_lower(city_regex),
    city_regex = str_remove_all(city_regex, "-")
  ) %>%
  filter(population > 500000)

abstracts_number_cities <- abstracts_numbers %>% 
   mutate(
    n_many_cities = str_extract_all(
      abstract, 
      # "(?<!more\\sthan)\\d+(?=\\s?(c|C)it(y|ies))"
      "\\d+(?=\\s?(c|C)it(y|ies))"
    ),
    abstract_ascii = stringi::stri_trans_general(abstract,"Latin-ASCII") %>% str_to_lower(),
    abstract_ascii = str_remove_all(abstract_ascii, "-")
  ) %>%
  unnest(n_many_cities, keep_empty = TRUE) %>%
  mutate(n_many_cities = as.numeric(n_many_cities)) %>% 
  group_by(doi) %>%
  mutate(n_many_cities = max(n_many_cities)) %>%
  ungroup() %>%
  distinct() %>% 
  group_by(doi) %>%
  mutate(
    n_names_cities = str_extract_all(abstract_ascii, worldcities_large[["city_regex"]]) %>% 
      as_vector() %>% 
      unique() %>% 
      length()
  ) %>%
  ungroup() %>% 
  group_by(doi) %>% 
  mutate(
    n_cities = max(n_many_cities, n_names_cities, na.rm = TRUE),
    n_cities = ifelse(n_cities == 0, NA, n_cities)
  ) %>% 
  ungroup() %>% 
  select(doi, n_cities) %>% 
  distinct()
```

### Number of observations

Finally, we combine these two information to compute the number of observations. 

```{r}
abstracts_n_obs <- abstracts %>% 
  left_join(abstracts_length_study, by = "doi") %>% 
  left_join(abstracts_number_cities, by = "doi") %>% 
  mutate(n_obs = n_cities*length_study)
```

### Pollutant and outcome studied

We then recover, when possible, the pollutant and outcome considered. 
We retrieve pollutants and outcome that are mentioned in the abstract. Importantly, we recognize that some pollutants and outcome may be mentioned in an abstract even though the corresponding study does not run any analysis on these pollutants and outcomes. We however assume that it is rather unlikely that a study on cardiovascular diseases due to particulate matter pollution will talk about ozone in its abstract. Note that there are sometimes several pollutants and outcomes per abstract. 

```{r}
abstracts_more_info <- abstracts_n_obs %>% 
  mutate(
    p_value = str_extract_all(abstract, "(?<=\\b(p|P|p-value|p value)\\s?)[=<]\\s?[\\d.]+"),
    pollutant = str_extract_all(abstract, "(\\bPM\\s?2(\\.|,)5|\\bPM\\s?10|\\bO\\s?3\\b|\\b(o|O)zone\\b|\\b(P|p)articulate\\s(M|m)atter\\b|\\bNO\\s?2|\\b(n|N)itrogen\\s?(d|D)ioxide\\b|\\bNO\\b|\\b(n|N)itrogen\\s?(o|O)xide\\b|\\bNO\\s?(x|X)\\b|\\bSO\\s?2|\\bCO\\b|\\bBC\\b|\\b(A|a)ir\\s(Q|q)uality\\s(I|i)ndex\\b)")
  ) %>% 
  unnest(pollutant, keep_empty = TRUE) %>% 
  group_by(doi) %>% 
  mutate(
    pollutant = tolower(pollutant), 
    pollutant = str_replace_all(pollutant, "\\s", ""),
    pollutant = str_replace_all(pollutant, ",", "\\."),
    pollutant = ifelse(pollutant == "nitrogendioxide", "no2", pollutant),
    pollutant = ifelse(pollutant == "nitrogenoxide", "no", pollutant),
    pollutant = ifelse(pollutant == "ozone", "o3",pollutant)
  ) %>% 
  distinct(pollutant, .keep_all = TRUE) %>% 
  ungroup() %>% 
  nest(pollutant = pollutant) %>% 
  mutate(
    # outcome = str_extract_all(title, "(\\b(M|m)ortality\\b|\\b(D|d)eath\\b|\\b(H|h)ospitalization|\\b(E|e)mergenc|\\b(S|s)troke|\\b(C|c)erebrovascular\\b|\\b(C|c)ardiovascular\\b)")
    outcome = str_extract_all(title, "(\\b(M|m)ortality\\b|\\b(D|d)eath\\b|\\b(H|h)ospitalization|\\b(E|e)mergenc)")
  ) %>% 
  unnest(outcome, keep_empty = TRUE) %>% 
  group_by(doi) %>%
   mutate(
    outcome = tolower(outcome),
    outcome = ifelse(outcome %in% c("emergenc", "hospitalization"), "Emergency",
                     ifelse(outcome %in% c("death", "mortality"), "Mortality", NA))
  ) %>%
  distinct(outcome, .keep_all = TRUE) %>%
  ungroup() %>%
  nest(outcome = outcome) %>%
  select(-title)


saveRDS(abstracts_more_info, "../Outputs/abstracts_more_info.RDS")
```


