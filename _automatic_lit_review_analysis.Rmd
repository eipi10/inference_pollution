---
title: "Automatic literature review - Analysis of abstracts"
author:
- name: Vincent Bagilet
  url: https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet
  affiliation: Columbia University
  affiliation_url: https://www.columbia.edu/
- name: LÃ©o Zabrocki
  url: https://www.parisschoolofeconomics.eu/en/
  affiliation: Paris School of Economics
  affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output:
  distill::distill_article: 
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = FALSE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 500,
               fig.align = "center")  
```  

```{r include=FALSE}
library(tidyverse)
library(here)
library(tidytext)
library(wordcloud)
library(retrodesign)
library(mediocrethemes)
library(lubridate)
library(DT)
library(kableExtra)
library(skimr)
library(ggridges)

set_mediocre_all()
set.seed(1)
```

# Purpose of the document

<!-- For years, the empirical economic literature has been obsessed with unbiasedness. Researchers have developed empirical methods and techniques in a quest to retrieve unbiased estimates. Yet, due to their inherent constraints, these techniques limit the set of settings in which one can retrieve causal and unbiased estimates. They also often tend to focus on limited samples of the data, leading to a reduction in sample size? This can ultimately lead to underpowered studies. As underlined by Gelman and Carlin (2014), a lack of power is often associated with type M and type S error. In a quest for unbiasedness, practitioners use methods leading to smaller sample size which can ultimately create bias (type M and type S error). We thus aim at analyzing whether the literature of short term health effects of air pollution suffers from power, type M and type S error issues. -->

One of our objectives in this paper is to evaluate whether the literature of short terms health effects of air pollution suffers from power and type M error issues. In this document, we carry out part of this analysis. We retrieved most of the estimates and confidence intervals of the literature in [another document](systematic_lit_review_geting_abstracts.html). To do so, using REGular EXPressions (regex), we took advantage of a somehow standardized reporting mechanism of point estimates and confidence intervals in the abstracts. The set of articles studied in details here is therefore limited to articles displaying confidence intervals and point estimates in their abstracts. This convention is not shared by all disciplines. For instance, it is not common practice in economics but much more common in the epidemiology literature. This analysis thus focuses on a selected sample of the literature.

In the present document, we first explore the characteristics of the articles considered. Since we only retrieve effects for a subset of papers, we assess whether these papers are representative of the whole literature. We find that they mostly are. We then briefly explore the effects retrieved before carrying out the actual power analysis, implementing sensitivity tests to compute power, type M and type S error. Our findings suggest important heterogeneity in this literature. Some papers do not seem to present major problems while others seem to suffer from severe power issues. We therefore explore potential sources for this heterogeneity.

Key findings and graphs are presented in the conclusion section of the present document. This section has been built to be read and understood independently. Readers who are only interested in the key findings are invited to jump straight to this section.  

<!-- write a paragraph explaining why the robustness checks make sense -->

<!-- # Key findings -->

# Articles characteristics

Before diving into the power analysis itself, we look at the characteristics of the articles considered.

We retrieved the articles from PubMed and Scopus using the following query on May 18, 2021:

'TITLE(("air pollution" OR "air quality" OR "particulate matter" OR ozone OR "nitrogen dioxide" OR "sulfur dioxide" OR "PM10" OR "PM2.5" OR "carbon dioxide" OR "carbon monoxide") AND ("emergency" OR "mortality" OR "stroke" OR "cerebrovascular" OR "cardiovascular" OR "death" OR "hospitalization") AND NOT ("long term" OR "long-term")) AND "short term"'

```{r}
abstracts_and_metadata <- readRDS(here("R", "Outputs", "abstracts_and_metadata.RDS"))
estimates <- readRDS(here("R", "Outputs", "estimates.RDS"))
```

This query enables us to retrieve `r nrow(abstracts_and_metadata)` valid abstracts.

## Themes

We can briefly explore the main (unsurprising) themes of the articles:

```{r}
abstracts_and_metadata %>%
  unnest_tokens(word, abstract, to_lower = TRUE) %>% 
  anti_join(tidytext::stop_words, by = "word") %>%
  count(word) %>%
  with(wordcloud::wordcloud(word, n, max.words = 80, random.color = TRUE, colors = "#00313C"))
```

```{r fig.asp = 1}
abstracts_and_metadata %>%
  unnest_tokens(word, abstract, to_lower = TRUE) %>%
  anti_join(tidytext::stop_words, by = "word") %>%
  select(doi, word) %>%
  distinct() %>%
  count(word, sort = TRUE) %>%
  filter(n > 650) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(
    y = NULL,
    title = "Main themes in the abstracts",
    subtitle = "Droping usual stop words",
    x = "Number of abstracts containing this word"
  ) 
```

## Detection of effects

<!-- Not all abstracts display effects and confidence intervals. We thus want to assess whether there are noticeable differences between articles for which we retrieve confidence intervals and those for which we do not. This quick exploration will also provide additional information and descriptive statistics on the whole set of articles. -->

```{r}
string_confint <- str_c(
  "((?<!(\\d\\.|\\d))95\\s?%|(?<!(\\d\\.|\\d))95\\s(per(\\s?)cent)|",
  "\\bC(\\.)?(I|l)(\\.)?(s?)\\b|\\bPI(s?)\\b|\\b(i|I)nterval|",
  "\\b(c|C)onfidence\\s(i|I)nterval|\\b(c|C)redible\\s(i|I)nterval|", 
  "\\b(p|P)osterior\\s(i|I)nterval)"
  )

effect_detected <- abstracts_and_metadata %>%
  left_join(estimates, by = "doi") %>% 
  mutate(
    contains_CI = str_detect(abstract, string_confint)
  ) %>%
  group_by(title) %>% 
  mutate(
    has_effect = (sum(!is.na(effect), na.rm = TRUE) > 0),
    has_effect_phrase = ifelse(has_effect, "Effect retrieved", "No effect retrieved")
  ) %>% 
  ungroup() %>% 
  select(-effect, -low_CI, -up_CI) %>% 
  distinct()
```

Out of the `r nrow(abstracts_and_metadata)` valid articles returned by the query, only a fraction reports estimates and confidence intervals in their abstracts. We therefore do not retrieve effects for all the articles returned by the query, mostly for the reason aforementioned. This might create some selection, making the sample of articles studied not representative of the whole literature. We investigate further the difference between articles that do and do not report confidence intervals in their abstracts in the following section. 

Here is a list of the valid articles. The last column shows whether at least one effect was retrieved for this article.

```{r}
effect_detected %>% 
  select(authors, pub_date, title, journal, has_effect_phrase) %>% 
  datatable(
    colnames = c(
      "Authors", 
      "Publication date", 
      "Title", 
      "Journal", 
      "Effect retrieved"
    )
  )
```

In total, we **detect `r nrow(estimates)` valid effects** and associated confidence intervals. We retrieve estimates for most of the articles mentioning "confidence interval" in some form in the abstract:  

```{r}
effect_detected %>%
  filter(contains_CI) %>% 
  count(has_effect) %>% 
  mutate(
    prop = n/sum(n),
    has_effect = ifelse(has_effect, "Yes", "No")
  ) %>% 
  arrange(desc(has_effect)) %>% 
  kable(
    col.names = c("Effect retreived", "Number of articles", "Proportion"),
    caption = "Number of articles for which at least one effect is retrieved (out of those containing the phrase 'CI')"
  ) %>%
  kable_styling(position = "center")
```

Note that a bunch of abstracts contain the phrase "CI" without actually displaying effects and confidence intervals. Our algorithm seems to make a reasonably good job at detecting effects and CI when they are indeed displayed in an abstract. In addition, there is no reason to think that our ability to detect an effect would be correlated with power issues in the paper. Hence, we feel rather confident assuming that our detection algorithm selects a random (along our dimension of interest) sample of estimates among all estimates displayed in abstracts.

## Representativity of articles for which an effect was retrieved

In this subsection, we investigate whether there are systematic differences between articles for which we retrieved an effect and articles that do not display an effect in their abstract or for which we did not detect one. We build this analysis such that it also provides general information about the entire set of articles. 

### Qualitative random analysis

First of all, we skim through a bunch of abstracts for which we retrieve an effect or not to see whether there are clear differences across study subsets. 

First of all, we notice that we did not select some papers displaying effects in their abstracts because they do not mention confidence intervals. We need standard errors to compute power, type M and type S error, hence why we did not select these papers. A small part of them seem to however mention p-values. We could have build on this to increase slightly our set of articles considered but it would not have drastically increased the set of articles considered. As discussed previously, we also acknowledge that our algorithm fails to detect a small share of otherwise valid estimates.

We then noticed that a non negligible share of articles for which we do not detect an effect seem to to be off-topic. Our quick exploration did not show evidence of such an issue in articles for which we detected an effect. This is rather reassuring; we are filtering out non well suited papers.

Finally, part of the studies for which we do not detect an effect are metanalyses (based on a quick regex search, there are about twice has much metanalyses in articles with an effect).
<!-- effect_detected %>% mutate(meta = str_detect(abstract, "meta(\\s|-)?anal")) %>% count(meta, has_effect) -->


<!-- ```{r} -->
<!-- effect_detected %>%  -->
<!--   filter(has_effect) %>%  -->
<!--   slice_sample(n = 3) %>%  -->
<!--   .$abstract %>%  -->
<!--   str_view_all("~") -->
<!-- ``` -->


### Publication date

Getting back to our automatic analysis, we first look into the distribution of published articles on this topic in time. We also wonder whether displaying effects in the abstract was a particular feature of a given period.

```{r}
effect_detected %>% 
  # filter(contains_CI) %>% 
  ggplot() +
  geom_density(aes(x = year(pub_date), fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of publication year",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Publication year",
    y = "Density"
  ) + 
  scale_fill_discrete(name = "")

year_first_article <- effect_detected %>% 
  filter(has_effect) %>% 
  .$pub_date %>% 
  year() %>% 
  min(na.rm = TRUE)
```

First, we notice that the number of articles published on short-term health effects of air pollution has been increasing rather strongly since the 1980s. The first article for which an effect is detected was published in `r year_first_article`. We only found `r effect_detected %>% filter(year(pub_date) < year_first_article) %>% count() %>% .$n` articles published before `r year_first_article`. This can be explained by the fact that, in most places, air pollution has only been measured consistently since the 1990s.

Even though there are slightly more recent (2010-2020) articles for which effects are retrieved, the difference does not seem to be substantial. Distributions of articles for which an effect has been retrieved and not are rather similar.

### Journal and fields

We then look into the journals and academic fields in which articles on short term health effect of air pollution have been published. The results by journals are rather messy so we focus on journal areas and subareas. 

```{r fig.asp=1}
effect_detected %>% 
  mutate(
    subject_area = ifelse(
      is.na(subject_area), 
      "Subject unknown", 
      subject_area
    )
  ) %>% 
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(subject_area)), fill = has_effect_phrase)) +
  labs(
    x = "Number of articles published in journals from each field",
    y = NULL,
    title = "Journals fields in which articles have been published",
    subtitle = "Comparison between articles for which an effect is retrieved and not"
  ) +
  scale_fill_discrete(name = "") 

effect_detected %>%  
  unnest(subsubject_area) %>% 
  mutate(
    subsubject_area = ifelse(
      is.na(subsubject_area), 
      "Subsubject unknown", 
      subsubject_area
    )
  ) %>% 
  mutate(subsubject_area = fct_lump_n(subsubject_area, 20)) %>%
  filter(subsubject_area != "Other") %>%
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(subsubject_area)), fill = has_effect_phrase)) +
  labs(
    x = "Number of articles published in journals covering a given subsubject",
    y = NULL,
    title = "Main journals subsubjects in which articles have been published",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    caption = "A paper publsihed in a multi-subject journal will appear several times"
  ) +
  scale_fill_discrete(name = "") 
```

Most papers on this topic have been, unsurprisingly, published in multidisciplinary journals, health or physical science journals.

One may notice that effects are not retrieved, *ie* not reported in the abstract or not detected, for most papers published in life science and social sciences and humanities. This is probably due to reporting practices peculiar to these fields. This might not be as problematic as they constitute a small share of the sample. There does not seem to be a particularly large imbalance in terms of journal general field for the more represented fields.

### Themes

We then wonder if the words used in each sets of abstracts differ between the two sets of articles.

```{r fig.asp=1}
theme_effect <- effect_detected %>%
  unnest_tokens(word, abstract, to_lower = TRUE) %>% 
  anti_join(tidytext::stop_words, by = "word") %>% 
  group_by(word, has_effect) %>%
  mutate(n_articles_word = length(unique(doi))) %>% 
  ungroup() %>% 
  select(word, has_effect, has_effect_phrase, n_articles_word) %>% 
  distinct() %>% 
  group_by(word) %>% 
  mutate(tot_n_articles_word = sum(n_articles_word, na.rm = TRUE)) %>% 
  ungroup() %>% 
  filter(tot_n_articles_word > 600) %>% 
  ungroup() %>% 
  # mutate(has_effect = ifelse(has_effect, "Effect detected", "Effect not detected")) %>% 
  mutate(word = reorder(word, n_articles_word)) 

theme_effect %>% 
  ggplot(aes(x = word, fill = has_effect_phrase)) +
  geom_col(
    data = subset(theme_effect, has_effect), 
    aes(y = n_articles_word)
  ) +
  geom_col(
    data = subset(theme_effect, !has_effect),
    aes(y = -n_articles_word),
    position = "identity"
  ) +
  scale_y_continuous(labels = abs) +
  coord_flip() +
  labs(
    x = NULL,
    y = "Number of abstracts containing a given word",
    title = "Words appearing in the more abstracts",
    subtitle = "Comparison between articles with and without detected effect"
  ) +
  scale_fill_discrete(name = "")
```

Apart from a few key terms, such as CI, 95 for instance, there are no huge differences in the terms used in both subsets of abstracts. Noticeably, the term "increase" seems to appear more in abstracts where we detect an effect. This is not surprising as these papers often use sentences similar to "a 10 $\mu g/m^{3}$ increase in PM10 concentration lead to ...".

### Pollutant

We then look at the pollutants considered in each article. We consider that a pollutant is studied if it is mentioned in the abstract. It is not an exact measure as some articles may mention pollutants without actually studying them but it remains an interesting metric.

```{r fig.asp=0.7}
effect_detected %>% 
  unnest(pollutant) %>% 
  mutate(pollutant = ifelse(is.na(pollutant), "Pollutant not detected", pollutant)) %>% 
  ggplot() +
  geom_bar(aes(y = fct_rev(fct_infreq(pollutant)), fill = has_effect_phrase)) +
  labs(
    x = "Number of article mentionning each pollutant",
    y = NULL,
    title = "Number of articles studing a given pollutant",
    subtitle = "Comparison between articles for which an effect was detected or not",
    caption = "A paper mentioning several pollutants will appear several times"
  ) +
  scale_fill_discrete(name = "")
```
First of all, we notice that a large share of papers considered here study particulate matters (PM2.5, PM10 or both). 

It seems that, when the number of papers sudying a given pollutant is large, the likelihood of detecting an effect does not seem to vary much with the type of pollutant. Importantly, the proportion of effects retrieved is much lower for articles for which we are not able to detect the type of pollutants studied. 

### Outcome

As for pollutants, for some articles, we were able to retrieve studied outcomes depending on the words used in an abstract. We classified them into two categories: mortality and emergency.

```{r fig.asp=0.7}
effect_detected %>% 
  unnest(outcome) %>% 
  mutate(outcome = ifelse(is.na(outcome), "Outcome not detected", outcome)) %>% 
  mutate(has_effect = ifelse(has_effect, "Effect detected", "Effect not detected")) %>% 
  ggplot() +
  geom_bar(aes(x = outcome, fill = has_effect), position = "dodge") +
  labs(
    x = NULL,
    y = "Number of articles",
    title = "Number of articles studing a given outcome",
    subtitle = "Comparison between articles for which an effect was detected or not - articles with unknown outcomes dropped"
  ) +
  scale_fill_discrete(name = "") 
```

Most articles studied here are interested in mortality. The proportion of articles for which an effect is retrieved seems to be larger for papers studying emergency admissions than mortality. There might therefore be some kind of selection issue along this dimension. 

### Subpopulation

Some articles focus on sub-populations such as infants or elders. When these terms are not mentioned, either the entire population is considered or we are not able to detect the subgroup considered. The number of articles for which a subpopulation is indicated is rather small: 

```{r}
effect_detected %>% 
  unnest(subpop) %>% 
  group_by(doi) %>% 
  mutate(missing_subpop = is.na(subpop)) %>% 
  ungroup() %>% 
  select(doi, missing_subpop) %>% 
  distinct() %>% 
  count(missing_subpop = missing_subpop) %>%
  mutate(missing_subpop = ifelse(missing_subpop, "No or unknown", "Yes")) %>% 
  kable(col.names = c("Subpopulation indicated", "Number of articles")) %>%
  kable_styling(position = "center")
```

Looking more in details into the detection of effects, we get the following pattern:

```{r fig.asp=0.7}
effect_detected %>% 
  unnest(subpop) %>% 
  mutate(subpop = ifelse(is.na(subpop), "No subpopulation not detected", subpop)) %>% 
  mutate(has_effect = ifelse(has_effect, "Effect detected", "Effect not detected")) %>% 
  ggplot() +
  geom_bar(aes(x = subpop, fill = has_effect), position = "fill") +
  labs(
    x = NULL,
    y = "Proportion of articles",
    title = "Proportion of articles studing a given outcome",
    subtitle = "Comparison between articles for which an effect was detected or not - articles with unknown outcomes dropped"
  ) +
  scale_fill_discrete(name = "") 
```

There does not seem to be large variations in the proportion of articles for which an effect is detected, depending on whether a subpopulation is studied or not. Yet, this proportion is a slightly larger for elders than infants, itself larger than when when no subpopulation is detected.

### Number of observations

We then look at the number of observations, the length of the the study period and the number of cities considered. Importantly, we only retrieve this information for a very limited subset of articles. 

```{r}
len <- effect_detected %>% 
  count(Missing = is.na(length_study)) %>% 
  rename(`Length of the study` = n)

cities <- effect_detected %>% 
  count(Missing = is.na(n_cities)) %>% 
  rename(`Number of cities` = n)

obs <- effect_detected %>% 
  count(Missing = is.na(n_obs)) %>% 
  rename(`Number of observations` = n)

len %>% 
  full_join(cities) %>% 
  full_join(obs) %>% 
  mutate(Missing = str_to_title(Missing)) %>% 
  kable() %>%
  kable_styling(position = "center")
```

Our analysis is therefore to be taken with caution as there is a critical lack of information for this category.

```{r fig.asp=0.7}
effect_detected %>% 
  ggplot() +
  geom_density(aes(x = n_obs, fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of the number of observations",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated number of observations",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")

effect_detected %>% 
  ggplot() +
  geom_density(aes(x = length_study, fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of the length of the study",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated length of the study (in days)",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")

effect_detected %>% 
  ggplot() +
  geom_density(aes(x = n_cities, fill = has_effect_phrase, color = has_effect_phrase)) +
  labs(
    title = "Distribution of the number of cities in the study",
    subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated number of cities",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")
```

We notice that there are large variations in the number of observations in the studies considered. However, there does not seem to be large differences along this dimension on whether an effect is retrieved or not. There seems however to be more studies around 1000 observations and less between 10,000 and 100,000 in articles for which an effect is retrieved. This is explained by the fact effects are retrieved more for studies with rather limited study period (around 3 years).

### Summary

The articles for which an effect was detected, unsurprisingly, seem to be slightly different from those for which we did not detect one, based on our quick qualitative analysis. Yet, there does not seem to be important divides along the dimensions studied.

We can thus now dig further into the analysis of the estimates retrieved.

# Analysis of the effects

In this section, we briefly analyze the effects retrieved, their statistical significance and their precision.

## Significance

First, we notice that most of the effects retrieved here are significant (at the usual 5% threshold).

```{r}
estimates_stats <- estimates %>%
  filter(!is.na(effect)) %>% 
  mutate(
    significant = (low_CI > 0 | up_CI < 0), 
    signal_noise = abs(effect/(up_CI - low_CI)),
    se = abs(up_CI - low_CI)/(2*1.96), 
    t_score = abs(effect)/se
  ) 

estimates_stats %>% 
  count(significant) %>% 
  mutate(prop = n/sum(n)) %>% 
  mutate(significant = ifelse(significant, "Yes", "No")) %>% 
  kable(col.names = c("Significant", "Number of effects", "Proportion")) %>%
  kable_styling(position = "center")
```

Researchers mention their key findings in the abstract and therefore probably do not report in their abstracts non statistically significant estimates for which the null hypothesis of no effect cannot be rejected. Only a very small proportion of articles do not report any statistically significant estimates in their abstract:

```{r}
estimates_stats %>% 
  group_by(doi) %>% 
  mutate(has_significant = (sum(significant) > 0) ) %>% 
  ungroup() %>% 
  select(has_significant, doi) %>% 
  distinct() %>% 
  count(has_significant) %>% 
  mutate(prop = n/sum(n)) %>% 
  mutate(has_significant = ifelse(has_significant, "Yes", "No")) %>% 
  kable(col.names = c("At least one significant estimate", "Number of articles", "Proportion")) %>%
  kable_styling(position = "center")
```

## T-scores

We then look into the distribution of the t-scores. 

```{r}
estimates_stats %>%
  filter(t_score < 10) %>%
  ggplot() +
  geom_histogram(aes(x = t_score), bins = 50) +
  geom_vline(xintercept = 1.96) +
  labs(
    title = "Distribution of the t-score in estimates of the literature",
    subtitle = "Only considering observations with a t-score lower than 10",
    caption = "The vertical line represents the usual 1.96 threshold",
    x = "t-score",
    y = "Count"
  ) 
```

There seems to be some sort of bunching for t-scores above 1.96. In this analysis, we only consider estimates reported in the abstracts. Authors may only report significant estimates in their abstracts even though they also report non significant estimates in the body of the article. This might explain this bunching. We need to investigate this further in order to understand whether this bunching is evidence of publication bias. To do so, we could reproduce the present analysis but analyze the full texts and not only on the abstracts. Other techniques could be used to analyze reporting bias but are not available to us, considering the lack on information we have access to using this automatic method.

## Signal to noise ratio
 
We then plot the distribution of the signal to noise ratio, *ie* the ratio of the point estimate and the width of the confidence interval.

```{r}
estimates_stats %>%
  filter(signal_noise < 4) %>%
  ggplot() +
  geom_histogram(aes(x = signal_noise), bins = 50) +
  geom_vline(xintercept = 0.5)  +
  labs(
    title = "Distribution of the signal to noise ration in estimates of the literature",
    subtitle = "Only considering observations with a signal to noise ration lower than 4",
    caption = "The vertical line represents the usual 0.5 threshold",
    x = "Signal to noise ration",
    y = "Count"
  ) 
```

The graph is of course analogous to the previous one. It however underlines that in a large share of the studies, the magnitude of the noise is larger than the magnitude of the effect. We then look into the distribution of the signal to noise ratio in more details.

```{r}
quantile(estimates_stats$signal_noise, seq(0, 1, 0.1)) %>%
  tidy() %>%
  select(x, names) %>%
  kable(
    col.names = c(
      "Signal to noise ratio",
      "Percentage of estimates with a lower signal to noise ratio"
    )
  ) %>%
  kable_styling(position = "center")
```

We notice that *for about 55% of the estimates considered here, the magnitude of the noise is more important than those of the signal*. This is particularly concerning.

# Power analysis

We then turn to the power analysis itself. The objective is to evaluate the power, type M and type S errors for each estimate. 

To compute these values, we would need to know the true effect size. Yet, true effects are of course unknown. One solution could be to use estimates from the literature and meta-analyses as best guesses for these true values. Yet, in the setting of this automatic literature review, detecting what is exactly measured in each analysis is particularly challenging since there is no standardized way of reporting the results beyond mentioning confidence intervals. One study may for instance claim that a 10 $\mu g/m^{3}$ increase in PM2.5 concentration is associated with an increase of x% in hospital admissions over the course of a day while another study may state that a 2% increase in ozone concentration increases the number of deaths by 3 over a month. Fortunately, for each estimate retrieved, even though we do not know what is measured, we can evaluate the precision with which it is estimated.

To circumvent the fact that we do not know the actual effect size, we follow a strategy suggested by Gelman and Carlin (2014). We consider a range of potential "true" effect sizes and run a sensitivity analysis. We investigate what would be the power, type M and type S error if the true effect was only a fraction of the measured effect. This enables us **to assess whether the design of the study would be *good enough* to detect a smaller effect**. If assuming that the true effect is 3/4 of the measured effect yields a power of 30%, there is probably a major issue with the design of this study. With this design, this (non zero) effect would only be detected 30% of the time.

Of course, there is no reason to think *a priori* that a given effect would be overestimated. The values for power, type M and type S errors are therefore only informative. 

To carry out the analysis, we use the package `retrodesign` which computes post analysis design calculations (power, type M and type S errors). We run the function `retro_desing()` for several effect sizes.

```{r}
retro_analysis <- estimates_stats %>% 
  mutate(
    retro_low = as_tibble(retro_design(low_CI, se)),
    retro_0.01 = as_tibble(retro_design(effect*0.01, se)),
    retro_0.05 = as_tibble(retro_design(effect*0.05, se)),
    retro_0.1 = as_tibble(retro_design(effect*0.1, se)),
    retro_0.33 = as_tibble(retro_design(effect*0.33, se)),
    retro_0.5 = as_tibble(retro_design(effect*0.5, se)),
    retro_0.67 = as_tibble(retro_design(effect*0.67, se)),
    retro_0.75 = as_tibble(retro_design(effect*0.75, se)),
    retro_0.9 = as_tibble(retro_design(effect*0.9, se)),
    retro_1 = as_tibble(retro_design(effect*1, se))
  ) %>% 
  pivot_longer(
    starts_with("retro"), 
    names_to = "prop_true_effect", 
    values_to = "computed"
  ) %>% 
  mutate(
    power = computed$power,
    typeS = computed$typeS,
    typeM = computed$typeM,
    prop_true_effect = as.numeric(str_sub(prop_true_effect, 7, nchar(prop_true_effect))),
    prop_true_effect_phrase = str_c(prop_true_effect*100, "% of the measured effect"), 
    prop_true_effect_phrase = ifelse(
      is.na(prop_true_effect_phrase), 
      "Lower bound of the CI", 
      prop_true_effect_phrase
    )
  ) %>% 
  select(-computed) %>% 
  filter(typeM < Inf) 
```

In a first part, we investigate whether the literature might be subject to design issues. We analyze the distribution of power, type M and type S errors in the whole set of abstracts, how they evolve jointly or along several variables such as the size of the true effect. We find clear evidence of heterogeneity across articles. Some of them seem to present robust designs while others seem much more problematic, yielding low power and high rates of type M error, even for large hypothesized true effect sizes.
<!-- The former set of estimates display high power even for low hypothesized true effect sizes while estimates from the later set present low power for still large true effect sizes.  -->
Thus, in a second part, we investigate potential sources of heterogeneity. 

### Graphical analysis

We first explore graphically the distribution of power, type M and type S error across simulations and for different magnitudes of the true effect.

```{r}
sizes_to_display <- c(0.33, 0.5, 0.75)

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of power in the literature", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    y = "Number of estimates"
  )
# geom_density()

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = typeM)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of type M error in the literature (log scale)", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    y = "Number of estimates"
  ) +
  scale_x_continuous(trans = 'log10')

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = typeS)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of type S error in the literature", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    y = "Number of estimates"
  )
```

A large chunk of articles display high power and low rates of type M and type S error, in each robustness check. This is reassuring: **a good chunk of the literature does not seem to suffer from low power issues!** 
**Type S error does not seem to be an important issue in this literature.** 
However, a non negligible number of articles display low power and/or some evidence of type M error. 

Note that for type M errors, due to some outliers, we used a log scale. Without this log scale and restricting our sample to type M errors lower than 2.5, we get the following graph:

```{r}
retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  filter(typeM < 2.5) %>%
  ggplot(aes(x = typeM)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Distribution of type M error in the literature", 
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    y = "Number of estimates"
  ) 
```

We find that, **even if the measured effect was close to the true effect, for a good chunk of articles, there  would be a substantial risk of type M error**. That means that, replicating each of these studies would yield, on average, a smaller effect than the reported one. 

Alternatively, we can also look at what would be the power, type M and type S if the true effect was equal to the lower bound of the confidence interval. This metric may exacerbate design issues for imprecise estimates. Such estimates would be "penalized" twice for being imprecise: they have a large standard error and the hypothesized true effect would be very small. Yet, this metric remains interesting as confidence intervals describe credible effect sizes. If the effect size is smaller but still credible, would the design be good enough to "detect" it?

```{r}
retro_analysis %>% 
  filter(typeM < 5) %>% 
  filter(str_starts(prop_true_effect_phrase, "Lower")) %>% 
  pivot_longer(power:typeM, names_to = "design_stat") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10) +
  facet_wrap( ~ design_stat, scales = "free") +
  labs(
    x = "", 
    y = "Number of estimates",
    title = "Distribution of design statistics in the literature", 
    subtitle = "If the magnitude of true effect is equal to the lower bound of the CI",
    y = "Number of estimates"
  ) 
```

Unsurprisingly, the conclusions remain similar: most articles do not seems to suffer from power issues but a non negligible chunk of articles does.

### Table

We then look more precisely at actual numbers, looking at the median and mean power, type M and type S errors for different true effect sizes.

```{r}
retro_analysis %>% 
  # filter(typeM < Inf) %>%
  group_by(prop_true_effect, prop_true_effect_phrase) %>% 
  summarise(
    mean_power = mean(power, na.rm = TRUE),
    median_power = median(power, na.rm = TRUE),
    mean_typeM = mean(typeM, na.rm = TRUE),
    median_typeM = median(typeM, na.rm = TRUE),
    mean_typeS = mean(typeS, na.rm = TRUE),
    median_typeS = median(typeS, na.rm = TRUE), 
    .groups = "drop"
  ) %>% 
  arrange(prop_true_effect) %>% 
  select(-prop_true_effect) %>%
  kable(col.names = c("", "Mean", "Median", "Mean", "Median", "Mean", "Median")) %>%
  add_header_above(c('"True" effect' = 1, "Power" = 2, "Type M" = 2, "Type S" = 2)) %>%
  kable_styling(position = "center")
```

### ECDF

Empirical Cumulative Distribution Functions (ECDFs) provide a broader perspective on these metrics.  They inform us of the share of articles for which power, type M or S errors are below a given threshold. 

```{r}
retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power)) +
  stat_ecdf(geom = "line") +
  facet_wrap( ~ prop_true_effect_phrase) +
  geom_vline(xintercept = 0.8, linetype = "dashed", size = 0.3) +
  # coord_flip() +
  labs(
    title = "Empirical cumulative distribution of power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Power",
    y = "Cumulative proportion of estimates", 
    caption = "The dashed line represents the usual 80% power threshold"
  )

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  filter(typeM < 2.5) %>%
  ggplot(aes(x = typeM)) +
  stat_ecdf(geom = "line") +
  # coord_flip() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Empirical cumulative distribution of power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Type M error",
    y = "Cumulative proportion of estimates"
  )

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = typeS)) +
  stat_ecdf(geom = "line") +
  # coord_flip() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Empirical cumulative distribution of power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Type S error",
    y = "Cumulative proportion of estimates"
  ) 
```

We notice that **about 40% of estimates would not reach the conventional 80% statistical power level if the true effect is 3/4 the size of the measured effect**. This is rather concerning.

For ECDFs too we look at what would be the power, type M and type S error if the true effect was equal to the lower bound of the confidence interval or to the estimated effect.

```{r}
retro_analysis %>% 
  filter(typeM < 5) %>%
  filter(str_starts(prop_true_effect_phrase, "Lower")) %>% 
  pivot_longer(power:typeM, names_to = "design_stat") %>% 
  ggplot(aes(x = value)) +
  stat_ecdf(geom = "line") +
  # geom_hline(yintercept = 0.8) +
  # coord_flip() +
  facet_wrap( ~ design_stat, scales = "free") +
  labs(
    x = "", 
    y = "Proportion of estimates",
    title = "Empirical cumulative distribution of design statistics in the literature", 
    subtitle = "If the magnitude of true effect is equal to the lower bound of the CI"
  ) 

retro_analysis %>% 
  filter(typeM < 5) %>%
  filter(prop_true_effect == 1) %>% 
  pivot_longer(power:typeM, names_to = "design_stat") %>% 
  ggplot(aes(x = value)) +
  stat_ecdf(geom = "line") +
  # geom_hline(yintercept = 0.8) +
  # coord_flip() +
  facet_wrap( ~ design_stat, scales = "free") +
  labs(
    x = "", 
    y = "Proportion of estimates",
    title = "Empirical cumulative distribution of design statistics in the literature", 
    subtitle = "If the magnitude of true effect is equal to the estimated effect"
  ) 
```

This might be rather concerning: even if the true effect was equal to the estimated effect (and only considering statistically significant estimates), `r retro_analysis %>% filter(prop_true_effect == 1) %>% filter(t_score >= 1.96) %>% mutate(above_80 = (power > 0.8)) %>% count(above_80) %>% mutate(prop = n/sum(n)) %>% filter(!above_80) %>% .$prop*100`% of the estimates would not reach the conventional a 80% power threshold. This is rather concerning since, due to type M error, significant estimates likely over-estimate on average the true effects. Indeed, the median type M for these estimates is `r retro_analysis %>% filter(prop_true_effect == 1) %>% filter(t_score >= 1.96) %>% mutate(above_80 = (power > 0.8)) %>% filter(!above_80) %>% .$typeM %>% median`. Considering that the true effect is equal to the estimated effect is therefore very conservative. If we assume that the true effect is equal to 3/4 of the measured effect (which is still rather conservative), the proportion of significant estimates not reaching the 80% power conventional threshold is still as high as `r retro_analysis %>% filter(prop_true_effect == 0.75) %>% filter(t_score >= 1.96) %>% mutate(above_80 = (power > 0.8)) %>% count(above_80) %>% mutate(prop = n/sum(n)) %>% filter(!above_80) %>% .$prop*100`%

### Link between power, type M and type S error

We then look how type M and type S error evolve with power for the estimates considered.

```{r}
retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power, y = typeM)) +
  geom_point() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Link between type M error and power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Power",
    y = "Type M"
  )

retro_analysis %>%
  filter(prop_true_effect %in% sizes_to_display) %>%
  ggplot(aes(x = power, y = typeS)) +
  geom_point() +
  facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Link between type S error and power in the literature",
    subtitle = "If the magnitude of true effect is a fraction of the measured effect",
    x = "Power",
    y = "Type S"
  )
```

There is a one-to-one relationship between power and type M and type S error. Not surprisingly, type M and type S errors skyrocket in studies with low power. Restricting the sample to a maximum type M of 4, we notice how quickly type M error rises when power decreases. In general, and not restricting ourselves to this literature, **type M error starts becoming a preoccupying issue even for rather high values of power**. Unfortunately, there is no conventional limit for which type M error is considered to be problematic. We should probably define one. Interestingly, when power is equal to the conventional 80% threshold, type M error is around 1.125.

```{r}
retro_analysis %>%
  filter(typeM < 4) %>% 
  filter(prop_true_effect == 1) %>%
  ggplot(aes(x = power, y = typeM)) +
  geom_line() +
  # facet_wrap( ~ prop_true_effect_phrase) +
  labs(
    title = "Relationship between type M error and power in the literature",
    subtitle = "Restricting the sample to a maximum type M of 4",
    x = "Power",
    y = "Type M"
  )
```

### Design calculations and effect size

We then investigate how average power, type M and type S evolve as a proportion of the true effect size.

```{r}
retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(Power = mean(power, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = Power)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  geom_hline(yintercept = 0.8, size = 0.5, linetype = "dashed", alpha = 0.7) +
  labs(
    title = "Evolution of average power as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    caption = "The dashed line represents the usual 80% power threshold"
  ) +
  ylim(0,1)

retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(typeM = mean(typeM, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = typeM)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of average type M error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type M"
  )

retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(typeS = mean(typeS, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = typeS)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of average type S error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type S"
  )
```

Power decreases and type M and type S errors skyrocket for small values of the true effect (as a proportion of the measured effect). In addition, the average power in the literature would be lower than 80% in the true effect was about 0.85 the measured effect for each study. This is rather preoccupying since, if the true effect was about 0.85 the measured effect for each study, on average, effect would be overestimated by more than 30%. Type S error only seems to be an issue for very small values of the true effect as a portion of the measured effect. Type M error seems to be more consistently problematic. The shoot up in the previous graph makes it difficult to read the values of type M error when the true effect is not a small portion of the measured effect. We therefore zoom in.

```{r}
retro_analysis %>%
  filter(prop_true_effect > 0.25) %>%
  group_by(prop_true_effect) %>%
  summarise(typeM = mean(typeM, na.rm = TRUE), .groups = "drop_last") %>%
  ggplot(aes(x = prop_true_effect, y = typeM)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of average type M error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type M") +
  ylim(1, 2.2)
```

We notice that, on average in the literature, the treatment effects are overestimated, even for large values of the true effect. This result might be linked to some outliers. We thus look at the evolution of the median power and type M with true effect size.

```{r}
retro_analysis %>% 
  group_by(prop_true_effect) %>% 
  summarise(Power = median(power, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = Power)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  geom_hline(yintercept = 0.8, size = 0.5, linetype = "dashed", alpha = 0.7) +
  labs(
    title = "Evolution of median power as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect"
  ) +
  ylim(0,1)

retro_analysis %>% 
  filter(prop_true_effect > 0.25) %>%
  group_by(prop_true_effect) %>% 
  summarise(typeM = median(typeM, na.rm = TRUE), .groups = "drop_last") %>% 
  ggplot(aes(x = prop_true_effect, y = typeM)) +
  geom_point() +
  geom_line(linetype = "dotted", alpha = 0.7) +
  labs(
    title = "Evolution of the median type M error as a function of the 'true effect'", 
    x = "True effect as a proportion of the measured effect", 
    y = "Type M"
  ) +
  ylim(1, 2.2)
```

The issue is much less important when looking at the median. This also highlights the presence of heterogeneity in terms of power in the literature. It is likely that some papers suffer from important issues and drive up the mean while others are not to prone to this type of issues.

To confirm that, we look into the evolution of the distribution of power and type M error with the proportion of effect size.

```{r fig.asp=1}
retro_analysis %>% 
  mutate(prop_true_effect = ifelse(is.na(prop_true_effect), "Low CI", prop_true_effect)) %>% 
  ggplot(aes(x = power, y = factor(prop_true_effect))) +
  geom_density_ridges(color = "#00313C", fill = "#00313C", alpha = 0.3) +
  labs(
    title = "Evolution of distrubtion of power as a function of the 'true effect'", 
    x = "Power", 
    y = "True effect as a proportion of the measured effect"
  ) 

retro_analysis %>% 
  filter(typeM < 5) %>% 
  mutate(prop_true_effect = ifelse(is.na(prop_true_effect), "Low CI", prop_true_effect)) %>% 
  ggplot(aes(x = typeM, y = factor(prop_true_effect))) +
  geom_density_ridges(color = "#00313C", fill = "#00313C", alpha = 0.3) +
  labs(
    title = "Evolution of distrubtion of type M error as a function of the 'true effect'", 
    x = "Type M error", 
    y = "True effect as a proportion of the measured effect"
  )

# retro_analysis %>% 
#   ggplot(aes(x = typeS, y = factor(prop_true_effect))) +
#   geom_density_ridges(color = "#00313C", fill = "#00313C", alpha = 0.3) +
#   labs(
#     title = "Evolution of distrubtion of type M error as a function of the 'true effect'", 
#     x = "Type S error", 
#     y = "True effect as a proportion of the measured effect"
#   )
```

The overall distribution of power may seem surprising: for "large" effect sizes, the power of most studies is high while it suddenly shifts to very low as effect size decreases.

### Summary

To sum up, the bulk of the estimates in this literature does not seem to suffer from low power and type M error issues. For most estimates, type S error does not seem to be an important issue in this literature. Yet, even if the measured effect was close to the true effect, a non negligible chunk of articles would display low power and a substantial risk of type M error. The power of a large chunk of estimates, at least one quarter of significant ones, might be problematically low. This low power is associated with non negligible type M error. Part of the estimates are thus likely overestimated. 

We need to investigate further into the sources of this heterogeneity.

# Analysis of the sources of heterogeneity

To simplify and clarify our analysis, we consider that an estimate has low power if its computed power is lower than 80% if the true effect is 3/4 of the measured effect. These numbers are arbitrary. 80% is the threshold usually used in power analyses and 3/4 is purely arbitrary and could be changed easily in a robustness check. Following this criterion, the number and proportion of estimates with low power is as follows:

```{r}
articles_low_adequate <- retro_analysis %>% 
  filter(prop_true_effect == 0.75) %>% 
  mutate(
    low_power = (power <= 0.8), 
    low_power_phrase = ifelse(low_power, "Low power", "Adequate power")
  ) %>% 
  left_join(abstracts_and_metadata, by = "doi")

articles_low_adequate %>% 
  count(low_power_phrase) %>% 
  mutate(prop = n/sum(n)) %>% 
  kable(col.names = c("Power", "Number of estimates", "Proportion")) %>%
  kable_styling(position = "center")
```

Based this criterion, **the number of studies with low power is concernedly high**. 

Such articles are also associated with large type M errors:

```{r}
articles_low_adequate %>% 
  group_by(low_power_phrase) %>% 
  summarise(
   mean_typeM = mean(typeM),
   median_typeM = median(typeM)
  ) %>% 
  kable(col.names = c("Power", "Mean type M", "Median type M")) %>%
  kable_styling(position = "center")
```


<!-- Non surprisingly, most of the type M and type S errors are associated with "low power" -->

<!-- ```{r} -->
<!-- retro_analysis %>%  -->
<!--   filter(prop_true_effect %in% c(0.5, 0.75, 1)) %>%   -->
<!--   filter(typeM < 2.5) %>%  -->
<!--   mutate(low_power = ifelse(low_power, "Low power", "Adequate power")) %>%  -->
<!--   ggplot(aes(x = typeM, fill = low_power)) +  -->
<!--   geom_histogram(bins = 10) + -->
<!--   facet_wrap(~ prop_true_effect) + -->
<!--   labs(title = "Distribution of type M error in the literature", subtitle = "If the magnitude of true effect is a fraction of the measured effect") + -->
<!--   scale_fill_discrete(name = "") -->
<!-- ``` -->

We investigate the particularities of articles with seemingly low power. We basically reproduce the same analyses as the ones used to compare articles for which we retrieved an effect or not.

## Publication date

First, we look into the evolution of the proportion of articles with low power with publication dates.

```{r}
set_mediocre_all(second_pair = TRUE)

# articles_low_adequate %>% 
#   ggplot() +
#   geom_density(
#     aes(x = year(pub_date), fill = low_power_phrase, color = low_power_phrase),
#     alpha = 0.4
#   ) +
#   labs(
#     title = "Distribution of publication year", 
#     subtitle = "Comparison between articles for with low and adequate power", 
#     x = "Publication year", 
#     y = "Density"
#   ) +
#   scale_fill_discrete(name = "")
# 
# articles_low_adequate %>% 
#   ggplot() +
#   geom_bar(
#     aes(x = year(pub_date) %/% 3 * 3, fill = low_power_phrase),
#     position = "fill"
#   ) +
#   labs(
#     title = "Temporal evolution of the share of papers with low power", 
#     x = "Publication year", 
#     y = "Share of estimates"
#   ) +
#   scale_fill_discrete(name = "")

articles_low_adequate %>% 
  group_by(year = year(pub_date)) %>% 
  summarise(
    n = n(),
    prop_low_power = sum(low_power)/n(),
    .groups = "drop"
  ) %>% 
  # filter(low_power) %>%
  filter(year <= 2020) %>% 
  ggplot() + 
  geom_point(aes(x = year, y = prop_low_power, size = n), color = "#8D0422", alpha = 0.6) +
  # geom_line(linetype = "dashed", size = 0.2) +
  # geom_smooth(color = "#8D0422", fill = "#8D0422") +
  ylim(c(0,1)) +
  labs(
    title = "Evolution of the proportion of articles with low power",
    size = "Number of articles published per year",
    x = "Year of publication",
    y = "Proportion of estimates with a low power"
  )
```

**The proportion of articles with low power does not seem to have drastically evolved in time**. One may have expected that this proportion would have decreased in time as researchers have access to more and more data, computational power and as estimation methods improve. 
<!-- It seems that articles with adequate power have been published more recently than articles with adequate power but the difference is not extreme. -->

We can also look more generally at how power, type M and type S error evolved in time.

```{r}
retro_analysis_pubdate <- retro_analysis %>% 
  left_join(abstracts_and_metadata %>% select(doi, pub_date), by = "doi")

retro_analysis_pubdate %>% 
  filter(prop_true_effect == 0.75) %>% 
  group_by(year = year(pub_date), prop_true_effect) %>%
  summarise(
    power = mean(power, na.rm = TRUE),
    n = n(),
    groups = ".drop"
  ) %>% 
  ggplot() +
  geom_point(aes(x = year, y = power, size = n), alpha = 0.6) +
  # geom_smooth(size = 0.3) +
  # facet_wrap(~ prop_true_effect_phrase) +
  labs(
    title = "Evolution of mean power in the literature with publication date", 
    subtitle = "If the magnitude of true effect is 3/4 of the measured effect",
    size = "Number of articles published per year",
    x = "Publication date", 
    y = "Power"
  )

retro_analysis_pubdate %>% 
  filter(prop_true_effect == 0.75) %>% 
  group_by(year = year(pub_date), prop_true_effect) %>%
  summarise(
    typeM = mean(typeM, na.rm = TRUE),
    n = n(),
    groups = ".drop"
  ) %>% 
  ggplot() +
  geom_point(aes(x = year, y = typeM, size = n), alpha = 0.6) +
  # geom_smooth(size = 0.3) +
  # geom_smooth(size = 0.3, formula = yÂ ~ x + I(x^2)) +
  # facet_wrap(~ prop_true_effect_phrase) +
  labs(
    title = "Evolution of mean type M error in the literature with publication date", 
    subtitle = "If the magnitude of true effect is 3/4 of the measured effect",
    size = "Number of articles published per year",
    x = "Publication date", 
    y = "Type M"
  )

retro_analysis_pubdate %>% 
  filter(prop_true_effect == 0.75) %>%  
  group_by(year = year(pub_date), prop_true_effect) %>%
  summarise(
    typeS = mean(typeS, na.rm = TRUE),
    n = n(),
    groups = ".drop"
  ) %>% 
  ggplot(aes(x = year, y = typeS, size = n)) +
  geom_point(alpha = 0.6) + 
  # geom_smooth(size = 0.3) +
  labs(
    title = "Evolution of mean type S error in the literature with publication date", 
    subtitle = "If the magnitude of true effect is 3/4 of the measured effect",
    size = "Number of articles published per year",
    x = "Publication date", 
    y = "Type S"
  )
```

There does not seem to be a clear trend in the evolution of power and type S error. However, type M error seems to increase slightly in time. This is rather concerning. One may potentially explain that by the fact that the literature is chasing smaller and smaller effects, leading to more type M error. This hypothesis is however difficult to verify since the effects we retrieved are not standardized, as mentioned previously.

Carrying out an analysis by quantile clearly shows that this evolution is driven by outliers with vary large type M errors. The yearly maximum type M error rates have increased in recent years.

```{r}
retro_analysis_pubdate %>% 
  filter(prop_true_effect == 0.75) %>% 
  group_by(year = year(pub_date), prop_true_effect) %>%
  summarise(
    typeM = quantile(typeM, na.rm = TRUE),
    quantile =  paste("Quantile:", seq(0, 1, 0.25)),
    n = n(),
    groups = ".drop"
  ) %>% 
  # filter(quantile > 0.8) %>%
  ggplot() +
  geom_point(aes(x = year, y = typeM)) +
  # geom_smooth(size = 0.3) +
  # geom_smooth(size = 0.3, formula = yÂ ~ x + I(x^2)) +
  facet_wrap(~ quantile, scales = "free_y") +
  labs(
    title = "Evolution type M error with publication date, by quantile", 
    subtitle = "If the magnitude of true effect is 3/4 of the measured effect",
    x = "Publication date", 
    y = "Type M"
  )
```


## Journal and fields

We then look into the distribution of articles in journals and fields. We filter out articles from life sciences and social sciences & humanities journals since only a very limited number of articles have been published in such journals.

```{r fig.asp=1}
articles_low_adequate %>%
  filter(!(subject_area %in% c("Life Sciences", "Social Sciences & Humanities"))) %>% 
  filter(!is.na(subject_area)) %>% 
  group_by(subject_area, low_power) %>% 
  mutate(n_low_power = sum(low_power)/sum(n())) %>% 
  ungroup() %>% 
  ggplot() +
  geom_bar(
    aes(y = fct_reorder(subject_area, n_low_power, mean), fill = low_power_phrase),
    position = "fill",
    ) +
  labs(
    x = "Proportion of articles",
    y = NULL,
    title = "Journals fields in which articles have been published",
    subtitle = "Comparison between effects that have adequate power or not"
  ) +
   scale_fill_discrete(name = "")

articles_low_adequate %>%
  unnest(subsubject_area) %>% 
  mutate(subsubject_area = fct_lump_n(subsubject_area, 20)) %>% 
  filter(subsubject_area != "Other") %>% 
  group_by(subsubject_area, low_power) %>% 
  mutate(n_low_power = sum(low_power)/sum(n())) %>% 
  ungroup() %>% 
  ggplot() +
  geom_bar(
    aes(y = fct_reorder(subsubject_area, n_low_power, mean), fill = low_power_phrase),
    position = "fill"
  ) +
  labs(
    x = "Proportion of articles",
    y = NULL,
    title = "Main journal subjects in which effects have been published",
    subtitle = "Comparison between effects that have adequate power or not"
  ) +
   scale_fill_discrete(name = "")
```

It seems that articles published in health sciences journals are less prone to power issues than articles published in other types of journals. This is somehow confirmed when looking at more precise subjects: the proportion of articles displaying low power is much lower in medicine journals than in "environment" and "chemistry" journals.

Some of the heterogeneity may come from the leniency of the journals with regards to these issues. Interestingly, not many fields have a very low rate of articles with low power. This might reveal an across the board lack of awareness of power issues. Yet, while our knowledge of the medicine literature is extremely limited, we seem to remember that, when applying for randomized experiments, researchers in medicine have to perform power calculations in order to properly design their study. Some of them therefore pay carefull attention to power issues and this might explain the lower rate of articles with low power in such journals.

## Themes

We then investigate whether the themes appearing in the abstracts are different in articles with adequate or low power.

```{r fig.asp=1}
theme_effect <- articles_low_adequate %>%
  unnest_tokens(word, abstract, to_lower = TRUE) %>% 
  anti_join(tidytext::stop_words, by = "word") %>% 
  group_by(word, low_power) %>%
  mutate(n_articles_word = length(unique(doi))) %>% 
  ungroup() %>% 
  select(word, low_power, low_power_phrase, n_articles_word) %>% 
  distinct() %>% 
  group_by(word) %>% 
  mutate(tot_n_articles_word = sum(n_articles_word, na.rm = TRUE)) %>% 
  ungroup() %>% 
  filter(tot_n_articles_word > 450) %>% 
  ungroup() %>% 
  # mutate(has_effect = ifelse(has_effect, "Effect detected", "Effect not detected")) %>% 
  mutate(word = reorder(word, n_articles_word)) 

theme_effect %>% 
  ggplot(aes(x = word, fill = low_power_phrase)) +
  geom_col(
    data = subset(theme_effect, low_power), 
    aes(y = n_articles_word)
  ) +
  geom_col(
    data = subset(theme_effect, !low_power),
    aes(y = -n_articles_word),
    position = "identity"
  ) +
  scale_y_continuous(labels = abs) +
  coord_flip() +
  labs(
    x = NULL,
    y = "Number of abstracts containing a given word",
    title = "Words appearing in the more abstracts",
    subtitle = "Comparison between articles with and without detected effect"
  ) +
  scale_fill_discrete(name = "")
```

There does not seem to be a clear difference in the terms used in abstracts for articles with low and adequate power. The only noticeable difference, not visible in this graph, is the use of the term "relative". Articles with low power mention it much less that articles with adequate power. This difference is rather unclear.

## Pollutant

We also look into potential disparities in terms of pollutant:

```{r fig.asp=1}
articles_low_adequate %>% 
  unnest(pollutant) %>%
  filter(!is.na(pollutant)) %>% 
  group_by(pollutant, low_power) %>% 
  mutate(n_low_power = sum(low_power)/sum(n())) %>% 
  ungroup() %>% 
  ggplot() +
  geom_bar(
    aes(y = fct_reorder(pollutant, n_low_power, mean), fill = low_power_phrase), 
    position = "fill"
  ) +
  labs(
    x = "Proportion of articles",
    y = NULL,
    title = "Power of studies depending on the pollutant considered",
    subtitle = "Comparison between articles studing a different pollutants"
  ) +
  scale_fill_discrete(name = "")
```

There does not seem to be stark differences by pollutant type. While some differences seem to appear for BC and Air Quality Index, these pollutants are only studied in a limited sample of studies.

## Outcome

We then compare the power as a function of the outcome studied (mortality or hospital admissions).

```{r}
articles_low_adequate %>%
  unnest(outcome) %>%
  filter(!is.na(outcome)) %>%
  ggplot() +
  geom_bar(aes(outcome, fill = low_power_phrase), position = "fill") +
  labs(
    x = NULL,
    y = "Proportion of articles",
    title =  "Power of studies depending on the outcome considered",
    subtitle = "Comparison between articles with low and adequate power"
  ) +
  scale_fill_discrete(name = "")
```

There does not seem to be any heterogeneity along this dimension.

## Subpopulation

We then turn to the sub-population studied.

```{r}
articles_low_adequate %>%
  unnest(subpop) %>%
  # filter(!is.na(subpop)) %>%
  mutate(subpop = ifelse(is.na(subpop), "Unknown", subpop)) %>% 
  ggplot() +
  geom_bar(aes(subpop, fill = low_power_phrase), position = "fill") +
  labs(
    x = NULL,
    y = "Proportion of articles",
    title = "Number of articles considering a given subpopulation",
    subtitle = "Comparison between articles with low and adequate power"
  ) +
  scale_fill_discrete(name = "")
```

There does not seem to be stark variations along this dimension either.

## Number of observations

Finally, we explore the link between power and the number of observations. Remember that, for a lot of articles, we were not able to recover information about the number of observations (number of cities or length of the study). Hence, these results are purely suggestive.

```{r fig.asp=0.7}
articles_low_adequate %>% 
  ggplot() +
  geom_density(aes(x = n_obs, fill = low_power_phrase, color = low_power_phrase)) +
  labs(
    title = "Distribution of the number of observations",
    # subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated number of observations",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")

articles_low_adequate %>% 
  ggplot() +
  geom_density(aes(x = length_study, fill = low_power_phrase, color = low_power_phrase)) +
  labs(
    title = "Distribution of the length of the study",
    # subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated length of the study (in days)",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")

articles_low_adequate %>% 
  ggplot() +
  geom_density(aes(x = n_cities, fill = low_power_phrase, color = low_power_phrase)) +
  labs(
    title = "Distribution of the number of cities in each study",
    # subtitle = "Comparison between articles for which an effect is retrieved and not",
    x = "Estimated number of cities",
    y = "Density"
  ) + 
  scale_x_log10() +
  scale_fill_discrete(name = "")
```

There does not seem to be substantial heterogeneity in terms of number of observations or length of the study period (at least in articles for which we were able to retrieve this information). We may not observe any difference in terms of number of observations because we do not have this information for many studies. We have retrieved more data about the number of cities considered and it seems that more studies with low power only consider one city. The data are imperfect but this result may come from the fact that estimates from studies focusing on one city are likely to be less precise than those from studies focusing on several cities. This would be due to a smaller number of observations.

## Summary

We have seem that mean and median power do not seem to have greatly evolved in time but the average type M seems to have increased in time. This pattern seems to be driven by the fact that more and more papers with very large type M errors have been published recently. We have also seen that health science journals seem to be more prone to power issues than other journals. However, there seems to be a shared lack of concern for low power issues across all fields. Finally, we did not find evidence of obvious heterogeneity in outcome, pollutant or sub-population studied but noticed that more studies with low power only seem to be considering only one city.

# Conclusion and summary {#concl}

In this document, we analyze the statistical power and inference properties of the literature on short term health effects of air pollution. To do so, we retrieved `r nrow(estimates)` valid estimates from abstracts of this literature. This sample of estimates is likely rather representative of the whole set of estimates in abstracts of this literature.

Our results are both reassuring and worrying. A large chunk of the estimates in this literature does not seem to suffer from low power and type M error issues. 
Yet, even if the measured effect was close to the true effect, a non negligible chunk of articles would display low power and a substantial risk of type M error. Type S error does not seem to be an important issue in this literature. The following graphs summarise part of this information:

```{r}
retro_analysis %>%
  filter(typeM < 5) %>% 
  filter(prop_true_effect == 0.75) %>% 
  pivot_longer(power:typeM, names_to = "design_stat") %>% 
  ggplot() +
  geom_histogram(aes(x = value), bins = 10) +
  facet_wrap( ~ design_stat, scales = "free") +
  labs(
    x = "", 
    y = "Number of estimates",
    title = "Distribution of design statistics in the literature", 
    subtitle = "If the magnitude of true effect is equal 3/4 of the estimated effect"
  ) +
  scale_fill_discrete(name = "")

articles_low_adequate %>% 
  filter(typeM < 3) %>%
  pivot_longer(power:typeM, names_to = "design_stat") %>% 
  filter(design_stat != "typeS") %>% 
  ggplot(aes(x = value)) +
  stat_ecdf(geom = "line") +
  # geom_hline(yintercept = 0.8) +
  # coord_flip() +
  facet_wrap( ~ design_stat, scales = "free") +
  labs(
    x = "", 
    y = "Proportion of estimates",
    title = "Empirical cumulative distribution of design statistics in the literature", 
    subtitle = "If the magnitude of true effect is equal 3/4 of the estimated effect",
    caption = "Only considering estimates with a type M lower than 3"
  ) 
```

About 40% of estimates would not reach the conventional 80% statistical power level threshold if the true effect is 3/4 the size of the measured effect. If for all studies, the true effect was 3/4 of the measured effect, the mean type M would be about 1.4. Again, this average hides a lot of heterogeneity. For the 40% of estimates with low power mentioned above, the average type M is 1.93, the median is 1.47. 

We then investigated the sources of heterogeneity. Concernedly, we find that in recent years, more and more articles display very large type M errors. In addition, health science journals seem to be more prone to power issues than other journals. However, there seems to be a shared lack of concern for low power issues across all fields. Finally, we did not find evidence of obvious heterogeneity in outcome, pollutant or sub-population studied but noticed that more studies with low power only seem to be considering only one city.
