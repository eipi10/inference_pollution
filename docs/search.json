{
  "articles": [
    {
      "path": "automatic_lit_review_getting_abstracts.html",
      "title": "Automatic literature review - Getting abstracts",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nPurpose of the document\nSelecting articles and retrieving metadataSet of articles to consider\n\nRetreiving abstracts\nRetreiving effects and confidence interavals\nRetreiving additional informationNumber of obsrvationsLength of the study period\nNumber of cities considered\nNumber of observations\n\nPollutant studied\nOutcome considered\nSub-population considered\nAdditional information on journals\n\nAgregating the information\n\n\nbody {\ntext-align: justify}\nPurpose of the document\nIn the present document, we retrieve abstracts for our automatic review of the literature on short term health effects of air pollution, focusing mainly on the epidemiology literature. We take advantage of a somehow standardized reporting mechanism to retrieve point estimates and confidence intervals from the abstracts using REGular EXPressions (regex).\nThe algorithm we wrote is not perfect: it probably does not detect all estimates and may pick up some incorrect point estimates and/or standard error. However, based on quick non-automatic checks we ran, these potential issues seem very limited. Retrieving all this information via careful reading of the abstracts would have been extremely cumbersome. This automatic analysis is an important time saver. Importantly, the analysis carried out for this literature could be easily replicated for another literature, based on the code used and described in this document.\nSelecting articles and retrieving metadata\nWe use the fulltext package to get the abstract of each article corresponding to our search query. \nWe focus on articles published on Scopus and Pubmed. To access Scopus API, one needs to register to get an API key (stored in the .Renviron) for Elsevier and a Crossref TDM API key.  Pubmed articles are accessed via Entrez. An API key enables to increase the number of requests per seconds from 3 to 10. More information on authentication is available on the fulltext manual.\nSet of articles to consider\nFirst of all, we need to clearly define the set of articles we want to consider in this analysis. Our search query is:\n‘TITLE((“air pollution” OR “air quality” OR “particulate matter” OR ozone OR “nitrogen dioxide” OR “sulfur dioxide” OR “PM10” OR “PM2.5” OR “carbon dioxide” OR “carbon monoxide”) AND (“emergency” OR “mortality” OR “stroke” OR “cerebrovascular” OR “cardiovascular” OR “death” OR “hospitalization”) AND NOT (“long term” OR “long-term”)) AND “short term”’\n\n\nquery <- \n  paste('TITLE((\"air pollution\" OR \"air quality\" OR \"particulate matter\" OR \"ozone\"', \n        'OR \"nitrogen dioxide\" OR \"sulfur dioxide\" OR \"PM10\" OR \"PM2.5\" OR', \n        ' \"carbon dioxide\" OR \"carbon monoxide\")', \n        'AND (\"emergency\" OR \"mortality\" OR \"stroke\" OR \"cerebrovascular\" OR', \n        '\"cardiovascular\" OR \"death\" OR \"hospitalization\")' ,\n        'AND NOT (\"long term\" OR \"long-term\")) AND \"short term\"'\n  )\n\nopts_entrez <- list(use_history = TRUE)\n\n#Run a search\nsearch <- ft_search(query, from = \"scopus\", limit = 2000)\nsearch_entrez <- ft_search(\n  str_replace(query, \"AND NOT\", \"NOT\"), \n  from = \"entrez\", \n  limit = 300, \n  entrezopts = opts_entrez\n)\n\n\n\nWe then retrieve and wrangle the related metadata. The metadata from different sources having different shapes, we only select a few relevant columns to build an overall metadata set.\n\n\nmetadata_scopus <- search$scopus$data %>% \n  as_tibble() %>% \n  rename_all(function(x) str_remove_all(names(.), \"prism:|dc:\")) %>% \n  rename_all(function(x) str_replace_all(names(.), \"-\", \"_\")) %>% \n  select(doi, title, creator, publicationName, pubmed_id, coverDate) %>% \n  rename(\n    authors = creator,\n    journal = publicationName\n  ) %>% \n  mutate(\n    pubmed_id = ifelse(!str_detect(pubmed_id, \"[0-9]{7}\"), NA, pubmed_id),\n    pub_date = ymd(coverDate)\n  ) %>% \n  select(-coverDate)\n\nsaveRDS(metadata_scopus, \"../Outputs/metadata_scopus.RDS\")\n\nmetadata_entrez <- search_entrez$entrez$data %>% #search_entrez$entrez$data \n  as_tibble() %>% \n  # rename(id = uid) %>% \n  select(doi, title, authors, fulljournalname, pmid, pubdate) %>% \n  rename(\n    journal = fulljournalname,\n    pubmed_id = pmid\n  ) %>% \n  mutate(\n    pubmed_id = ifelse(!str_detect(pubmed_id, \"[0-9]{7}\"), NA, pubmed_id),\n    pub_date = ymd(pubdate)\n  ) %>% \n  select(-pubdate)\n\nsaveRDS(metadata_entrez, \"../Outputs/metadata_entrez.RDS\")\n\nmetadata_lit_review <- metadata_scopus %>% \n  rbind(metadata_entrez) %>% \n  filter(!is.na(doi)) %>% \n  mutate(pb_doi = str_detect(doi, \"[<>;]\")) %>% #some dois are not valid\n  filter(pb_doi == FALSE) %>% \n  select(-pb_doi) %>% \n  group_by(doi) %>% \n  filter(pub_date == max(pub_date, na.rm = TRUE)) %>% #some articles have been published twice\n  mutate(n_with_doi = n()) %>% \n  filter(n_with_doi == 1 | (n_with_doi > 1 & pubmed_id == max(pubmed_id, na.rm = TRUE))) %>%\n  #two weird articles with separate author names, I select one randomly\n  select(-n_with_doi) %>% \n  ungroup() %>%\n  distinct(title, .keep_all = TRUE)\n\n# saveRDS(metadata_lit_review, \"../Outputs/metadata_lit_review.RDS\")\n\n\n\nRetreiving abstracts\nThere is no fulltext function to access abstracts from Entrez. Therefore, using the DOI, we get the abstracts from Semantic Scholar. We also access Scopus abstracts from Semantic Scholar since, due to an IP address constraint we cannot access the texts and abstracts from Scopus.\nIn Semantic Scholar, there is a rate limit of 100 articles per 5 min or 20 articles per minute. We therefore need to pause the system to be able to download everything. In addition, some DOIs are not valid so we filtered them out in a previous step (pb_doi).1\n\n\nget_abstracts <- function(doi) {\n  vect_doi <- unique(doi)\n  number_periods <- (length(vect_doi) - 1) %/% 20\n  abs <- NULL\n\n  message(str_c(\"Total downloading time: \", number_periods, \"min\"))\n  \n  for (i in 0:number_periods) {\n    \n    doi_period <- vect_doi[(20*i+1):(20*(i+1))]\n    doi_period <- doi_period[!is.na(doi_period)]\n    \n    skip_to_next <- FALSE #to handle issues, using tryCatch\n    \n    possible_error <- tryCatch(\n      abs_period <- doi_period %>%\n        ft_abstract(from = \"semanticscholar\") %>%\n        .$semanticscholar %>%\n        as_tibble() %>%\n        unnest(cols = everything()) %>%\n        pivot_longer(everything(), names_to = \"doi\", values_to = \"abstract\") %>%\n        filter(doi != abstract),\n      error = function(e) e\n    )\n    \n    if (inherits(possible_error, \"error\")) {\n      warning(\n        str_c(\"The abstracts for the following articles could not be downloaded: \",\n              str_c(doi_period, collapse = \",\")))\n      next\n    } else {\n       abs <- abs %>%\n        rbind(abs_period)\n    }\n    \n    if (i < number_periods & number_periods != 0) {\n      message(str_c(\"Remaining time: \", (number_periods - i), \"min\"))\n      Sys.sleep(63)\n    }\n  } \n  \n  return(abs)\n}\n\n#run this from the console to see the time remaining (copy/paste it)\nabstracts <- metadata_lit_review %>%\n  .$doi %>% \n  get_abstracts()  %>% \n  left_join(metadata_lit_review, by = \"doi\") \n\n# saveRDS(abstracts, \"../Outputs/abstracts.RDS\")\n\n\n\nRetreiving effects and confidence interavals\nNow that we have retrieved the abstracts, we want to extract the effects and associated confidence intervals. Part of the literature, displays directly effects and 95% confidence intervals in their abstracts.2 We identify effects and CIs as follows:\nCI: any couple of numbers following by less than 4 characters a string describing a confidence interval (“95%”, “CI”, “confidence interval”). We also consider confidence intervals of the shape “(-8.7, 54.7)”: it needs to have a shape “(number separator number)” and needs to be preceded by a number, less than 5 characters away.\nEffect: the first number preceding by less than 30 characters a similar string describing a confidence interval (apart numbers used to describe the “95” of a 95% CI).\n\n\nstring_confint <- str_c(\n  \"((?<!(\\\\d\\\\.|\\\\d))95\\\\s?%|(?<!(\\\\d\\\\.|\\\\d))95\\\\s(per(\\\\s?)cent)|\",\n  \"\\\\bC(\\\\.)?(I|l)(\\\\.)?(s?)\\\\b|\\\\bPI(s?)\\\\b|\\\\b(i|I)nterval|\",\n  \"\\\\b(c|C)onfidence\\\\s(i|I)nterval|\\\\b(c|C)redible\\\\s(i|I)nterval|\", \n  \"\\\\b(p|P)osterior\\\\s(i|I)nterval)\"\n  )\nnum_confint <- \n  \"(-\\\\s?|−\\\\s?)?[\\\\d\\\\.]{1,7}[–\\\\s:\\\\~;,%\\\\-to\\\\‐-]{1,5}(-\\\\s?|−\\\\s?)?[\\\\d\\\\.]{1,7}\"\nnum_effect <- \"(-\\\\s?|−\\\\s?)?[\\\\d\\\\.]{1,7}\"\n\ndetected_effects <- abstracts %>%\n  mutate(abstract = str_replace_all(abstract, \"·\", \".\")) %>%\n  select(doi, abstract) %>%\n  unnest_tokens(\n    sentence,\n    abstract,\n    token = \"sentences\",\n    to_lower = FALSE,\n    drop = FALSE\n  ) %>%\n  mutate(\n    # contains_CI = str_detect(sentence, string_confint),\n    sentence = str_replace_all(\n      sentence,\n      \"(?<=(?<!\\\\.)(?<!\\\\d)\\\\d{1,4}),(?=(\\\\d{3}(?!\\\\.)))\",\n      \"\"\n    )\n  ) %>%\n  # filter(contains_CI) %>%\n  mutate(\n    CI = str_extract_all(\n      sentence,\n      str_c(\n        \"((?<=\", string_confint, \"[^\\\\d]{0,4})\", num_confint,\")|\",\n        \"(?<=\", num_effect,\"[^\\\\d\\\\.]{0,5})\", \n        \"(?<=(\\\\(|\\\\[))\", num_confint,\"(?=%?[\\\\)\\\\];])\"\n      )\n    ),\n    effect = str_extract_all(\n      sentence,\n      str_c(\n        num_effect,\n        \"(?=[^\\\\d\\\\.]{0,30}([^\\\\.\\\\d]\", string_confint,\"))(?<![^\\\\.\\\\d]95)|\",\n        num_effect,\n        \"(?=[^\\\\d\\\\.]{0,5}(\\\\(|\\\\[)\", num_confint, \"(?=%?[\\\\)\\\\];]))(?<![^\\\\.\\\\d]95)\"\n      )\n    )\n  ) \n\n\n\nThese lines of code return a set of confidence intervals and effects for each sentence containing the phrase (“CI”, “confidence interval”, etc).  For now, if we do not detect the same number of effects and confidence interval in a sentence, we drop the sentence, even though there are 5 pairs of effect-CI and only one of them is badly detected.\nNote that some problems might remain with our estimates and CIs detected. Yet, a vast majority of estimates seems to be correctly detected. Here are examples of the confidence intervals and effects detected using our current method:\n\n\nsentences_with_CI <- detected_effects %>% \n  filter(CI != \"character(0)\" & effect != \"character(0)\")\nrandom_sentences <- sample(1:length(sentences_with_CI$sentence), 5)\nstr_view_all(\n  sentences_with_CI$sentence[random_sentences],\n  str_c(\n    num_effect,\n    \"(?=[^\\\\d\\\\.]{0,17}([^\\\\.\\\\d]\", string_confint, \"))(?<![^\\\\.\\\\d]95)|\",\n    num_effect,\n    \"(?=[^\\\\d\\\\.]{0,5}(\\\\(|\\\\[)\", num_confint, \"(?=%?[\\\\)\\\\];]))(?<![^\\\\.\\\\d]95)\",\n    \"|((?<=\", string_confint, \"[^\\\\d]{0,4})\", num_confint,\")|\",\n    \"(?<=\", num_effect,\"[^\\\\d\\\\.]{0,5})(?<=(\\\\(|\\\\[))\", num_confint, \"(?=%?[\\\\)\\\\];])\"\n  )\n)\n\n\n\n{\"x\":{\"html\":\"<ul>\\n  <li>Results The excess risk for non-accidental mortality was <span class='match'>1.3<\\/span>% [95% confidence interval (CI), <span class='match'>0.8–1.7<\\/span>] per 10 μg/m3 of PM10, with higher excess risks for cardiovascular and above age 65 mortality of <span class='match'>1.9<\\/span>% (95% CI, <span class='match'>0.8–3.0<\\/span>) and <span class='match'>1.5<\\/span>% (95% CI, <span class='match'>0.9–2.1<\\/span>), respectively.<\\/li>\\n  <li>After adjustment, top (hazard ratio [HR], <span class='match'>1.48<\\/span>; 95% confidence interval [CI], <span class='match'>1.08-2.04<\\/span>; P=.016) and bottom (HR, <span class='match'>1.54<\\/span>; 95% CI, <span class='match'>1.08-2.18<\\/span>; P=.017) quartiles of BE were associated with increased risk of readmission or death.<\\/li>\\n  <li>On admission days with SO2 levels above the median, mortality was higher (OR <span class='match'>1.13<\\/span>; 95% CI <span class='match'>1.10, 1.16<\\/span>) at <span class='match'>12.2<\\/span>% (95% CI <span class='match'>11.4, 13<\\/span>) compared with <span class='match'>10.7<\\/span>% (95% CI <span class='match'>10.3, 11.1<\\/span>) on days when SO2 levels were below the median.<\\/li>\\n  <li>Using these methods, we estimated <span class='match'>362000<\\/span> (95% confidence interval, <span class='match'>173000–551000<\\/span>) annual global premature cardiopulmonary deaths attributable to ozone, approximately 50% of the 700000 premature deaths we calculated in our original study (Anenberg et al.<\\/li>\\n  <li>During the period of high PM10 concentration (>89.82 μg/m3), NO2 demonstrated its strongest effect for total non-accidental mortality (ERR%: <span class='match'>0.92<\\/span>, 95% CI: <span class='match'>0.42–1.42<\\/span>) and cardiovascular disease mortality (ERR%: <span class='match'>1.20<\\/span>, 95% CI: <span class='match'>0.38–2.03<\\/span>).<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\nOnce the effects and CI are identified, some wrangling is necessary in order to get the data into a usable format. We also choose to drop effects which do not fall into the CI (62 estimates) in order to get rid off most of the poorly detected effects-CIs.\n\n\nestimates <- detected_effects %>% \n  filter(lengths(effect) == lengths(CI)) %>% #if number of effects != nb of CI for a sentence,\n  #can't attribute effects to CI so, drop sentence\n  unnest(c(effect, CI), keep_empty = TRUE) %>%\n  mutate(CI = str_remove_all(CI, \"\\\\s\")) %>% \n  # separate(CI, into = c(\"low_CI\", \"up_CI\"), \"([\\\\s,]+)|(?<!^)[-–]\") %>% \n  separate(CI, into = c(\"low_CI\", \"up_CI\"), \"(?<!^)[–:;,%\\\\-to\\\\‐]{1,5}\") %>%\n  mutate(across(c(\"effect\", \"low_CI\", \"up_CI\"), .fns = as.numeric)) %>% \n  mutate(\n    low_CI = ifelse(is.na(up_CI), NA, low_CI),\n    up_CI = ifelse(is.na(low_CI), NA, up_CI),\n    effect = ifelse(is.na(low_CI), NA, effect)\n  ) %>% \n  select(doi, effect, low_CI, up_CI) %>%\n  filter(!is.na(effect)) %>%  \n  filter(effect > low_CI & effect < up_CI)\n\n# saveRDS(estimates, \"../Outputs/estimates.RDS\")\n\n\n\nThe estimates data frame displays, in each row, a point estimate and the lower and upper band of the CI along with the DOI of the article from which this estimate is extracted. We retrieved 2666 valid point estimates and associated confidence intervals. We analyze them further in another document.\nRetreiving additional information\nIt might also be interesting to have information about the type of pollutant considered in the study, the study period, the number of observations or the type of outcome studied (mortality, emergency admissions, stroke, cerebrovascular or cardiovascular diseases). We thus use regex to recover this information. Of course, this does not enable us to retrieve data for all the abstracts considered but it provides a useful source of information; retrieving this information “by hand” is extremely cumbersome. Regex are thus very helpful here. They also make this analysis reproducible.\nNumber of obsrvations\nIn most studies in this field, observations are daily and at the city level. To compute the number of observations, we thus retrieve the length of the study period from the abstract when possible, along with the number of cities considered in the study.\nLength of the study period\nTo compute the length of the study period, we look for beginning and end dates for the period of study in the abstract. Some abstracts contain phrases such as “from January 2002 to March 2011” to indicate the study period. We take advantage of such mentions to retrieve the study period. Before anything, we transform the text data to a date format using the function text_to_date. Then, we detect, the study periods and wrangle them into a usable format. If we retrieve several length for a unique article, we take a conservative approach and only keep the longer one.\n\n\nabstracts_only <- abstracts %>% \n  mutate(abstract = str_replace_all(abstract, \"·\", \".\")) %>% \n  select(doi, abstract)\n\nmonth_regex <- str_c(\n  \"\\\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|\", \n  \"Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|(Nov|Dec)(?:ember)?)\")\ndate_regex <- str_c(\"(\", month_regex, \"\\\\s){0,1}(19|20)\\\\d{2}\")\n\ntext_to_date <- function(date_text) {\n  year <-  str_remove_all(date_text, \"[^\\\\d]\")\n  month <- match(str_remove_all(date_text, \"[\\\\d\\\\s]\"), month.name)\n  month <- ifelse(is.na(month), \"01\", month)\n  date <- dmy(str_c(\"01-\", str_pad(month, width = 2, pad = 0), \"-\", year))\n  return(date)\n}\n\narticles_length_study <- abstracts_only %>%\n  mutate(\n    dates_obs = str_extract_all(\n      abstract, \n      str_c(\n        \"(\", date_regex, \"|\", month_regex, \")\", \n        \"( to |\\\\s?—\\\\s?|\\\\s?-\\\\s?| and )\", \n        date_regex\n      )\n    )\n  ) %>% \n  unnest(dates_obs, keep_empty = TRUE) %>%\n  separate(\n    dates_obs, \n    into = c(\"begin_obs\", \"end_obs\"), \n    \"( to |\\\\s?—\\\\s?|\\\\s?-\\\\s?| and )\"\n  ) %>% \n  mutate(#when begin_obs and end_obs are in the same year, \n    #we only get the month for begin_obs\n    begin_obs = ifelse(\n      !str_detect(begin_obs, \"\\\\d\"), \n      paste(begin_obs, str_remove_all(end_obs, \"[^\\\\d]\")),\n      begin_obs)\n  ) %>%\n  mutate(\n    begin_obs = text_to_date(begin_obs),\n    end_obs = text_to_date(end_obs),\n    length_study = time_length(end_obs - begin_obs, unit = \"days\"),\n    length_study = ifelse(length_study < 0 | end_obs > today(), NA, length_study)\n  ) %>% \n  select(doi, length_study) %>% \n  group_by(doi) %>% \n  mutate(\n    length_study = max(length_study, na.rm = TRUE),\n    length_study = ifelse(length_study < 0, NA, length_study)\n  ) %>% \n  ungroup() %>% \n  distinct()\n\n\n\nWe retrieve a length of the study for 34.351145% of the articles. Note that with this method, we may miss studies which span for exactly a year, eg when only 2011 is mentioned.\nNumber of cities considered\nWe then try to detect the number of cities considered in each abstract. To do so, we use two different techniques:\nWe see whether the abstract uses phrases such as “in 34 cities” and retrieve this 34. We also convert all text-numbers (eg “one”) into their numerical values. When we get several numbers, we take a conservative approach and only keep the largest one.\nWe count the number of cities names appearing uniquely in each abstract. To do so, we use a database of cities names. We restrict our sample to large cities.\n\n\nnumber_word <- tibble(\n  number = 1:5000, \n  word = english::words(1:5000)\n)\n\nabstracts_in_numbers <- abstracts_only %>% \n  unnest_tokens(word, abstract) %>%\n  left_join(number_word, by = \"word\") %>%\n  mutate(word = ifelse(!is.na(number), number, word)) %>% \n  select(-number) %>% \n  group_by(doi) %>% \n  summarize(abstract = str_c(word, collapse = \" \")) %>%\n  ungroup() %>% \n  mutate(abstract = str_remove_all(abstract, \"-\"))\n\n#Source: https://simplemaps.com/data/world-cities\nworldcities <- read_csv(\"R/Inputs/worldcities.csv\")\n\nworldcities_large <- worldcities %>%\n  mutate(\n    city_regex = str_c(\"\\\\b\", city_ascii, \"\\\\b\"),\n    city_regex = str_to_lower(city_regex),\n    city_regex = str_remove_all(city_regex, \"-\")\n  ) %>%\n  filter(population > 500000)\n\narticles_number_cities <- abstracts_in_numbers %>% \n   mutate(\n    n_many_cities = str_extract_all(\n      abstract, \n      # \"(?<!more\\\\sthan)\\\\d+(?=\\\\s?(c|C)it(y|ies))\"\n      \"\\\\d+(?=\\\\s?(c|C)it(y|ies))\"\n    ),\n    abstract_ascii = stringi::stri_trans_general(abstract,\"Latin-ASCII\") %>% str_to_lower(),\n    abstract_ascii = str_remove_all(abstract_ascii, \"-\")\n  ) %>%\n  unnest(n_many_cities, keep_empty = TRUE) %>%\n  mutate(n_many_cities = as.numeric(n_many_cities)) %>% \n  group_by(doi) %>%\n  mutate(n_many_cities = max(n_many_cities)) %>%\n  ungroup() %>%\n  distinct() %>% \n  group_by(doi) %>%\n  mutate(\n    n_names_cities = str_extract_all(abstract_ascii, worldcities_large[[\"city_regex\"]]) %>% \n      as_vector() %>% \n      unique() %>% \n      length()\n  ) %>%\n  ungroup() %>% \n  group_by(doi) %>% \n  mutate(\n    n_cities = max(n_many_cities, n_names_cities, na.rm = TRUE),\n    n_cities = ifelse(n_cities == 0, NA, n_cities)\n  ) %>% \n  ungroup() %>% \n  select(doi, n_cities) %>% \n  distinct()\n\n\n\nWe retrieve a number of cities considered in the study for 45.3653217% of the articles.\nNumber of observations\nFinally, we combine these two information to compute the number of observations.\n\n\narticles_n_obs <- articles_length_study %>% \n  full_join(articles_number_cities, by = \"doi\") %>% \n  mutate(n_obs = n_cities*length_study)\n\n\n\nWe retrieve a number of cities considered in the study for 20.4471101% of the articles.\nPollutant studied\nWe then recover, when possible, the pollutant(s) considered in the study. We assume that only pollutants studied are mentioned in the abstract. This might be slightly inaccurate but seems to be a coherent first order approximation. We recognize that some pollutants may be mentioned in an abstract even though the corresponding study does not run any analysis on these pollutants and outcomes. We however assume that it is rather unlikely that a study on particulate matter pollution for instance will also talk about ozone in its abstract. Note that there are sometimes several pollutants mentioned in an abstract and analyzed in a study.\n\n\nabstracts_with_titles <- abstracts %>% \n  mutate(\n    abstract = str_replace_all(abstract, \"·\", \".\"),\n    abstract_title = str_c(title, abstract, sep = \". \")\n  ) %>% \n  select(doi, abstract_title)\n\narticles_pollutant <- abstracts_with_titles %>% \n  mutate(\n    pollutant = str_extract_all(\n      abstract_title,\n      str_c(\"(\\\\bPM\\\\s?2(\\\\.|,)5|\\\\bPM\\\\s?10|\\\\bO\\\\s?3\\\\b|\\\\b(o|O)zone\\\\b|\",\n            \"\\\\b(P|p)articulate(\\\\s(M|m)atter\\\\b)?|\\\\bNO\\\\s?2|\",\n            \"\\\\b(n|N)itrogen\\\\s?(d|D)ioxide\\\\b|\\\\bNO\\\\b|\",\n            \"\\\\b(n|N)itrogen\\\\s?(o|O)xide\\\\b|\\\\bNO\\\\s?(x|X)\\\\b|\\\\bSO\\\\s?2|\",\n            \"\\\\bCO\\\\b|\\\\bBC\\\\b|\\\\b(A|a)ir\\\\s(Q|q)uality\\\\s(I|i)ndex\\\\b)\")\n    )\n  ) %>% \n  unnest(pollutant, keep_empty = TRUE) %>% \n  group_by(doi) %>% \n  mutate(\n    pollutant = tolower(pollutant), \n    pollutant = str_replace_all(pollutant, \"\\\\s\", \"\"),\n    pollutant = str_replace_all(pollutant, \",\", \"\\\\.\"),\n    pollutant = case_when(\n      pollutant == \"nitrogendioxide\" ~ \"no2\",\n      pollutant == \"nitrogenoxide\" ~ \"no\", \n      pollutant == \"ozone\" ~ \"o3\",\n      pollutant == \"particulate\" ~ \"particulatematter\",\n      TRUE ~ pollutant\n    ),\n    pollutant = str_to_upper(pollutant),\n    pollutant = ifelse(pollutant == \"PARTICULATEMATTER\", \"Particulate matter\", pollutant),\n    pollutant = ifelse(pollutant == \"AIRQUALITYINDEX\", \"Air Quality Index\", pollutant)\n  ) %>% \n  distinct(pollutant, .keep_all = TRUE) %>% \n  ungroup() %>% \n  select(-abstract_title) %>% \n  nest(pollutant = pollutant)\n\n\n\nWe identify pollutants considered in the study for 82.4427481% of the articles.\nOutcome considered\nFollowing a similar methodology as for pollutants, we retrieve information about the outcomes considered.\n\n\narticles_outcome <- abstracts_with_titles %>% \n  mutate(\n    outcome = str_extract_all(\n      abstract_title, \n      \"(\\\\b(M|m)ortality\\\\b|\\\\b(D|d)eath(s)?\\\\b|\\\\b(H|h)ospitalization|\\\\b(E|e)mergenc)\"\n    )\n  ) %>% \n  unnest(outcome, keep_empty = TRUE) %>% \n  group_by(doi) %>%\n   mutate(\n    outcome = tolower(outcome),\n    outcome = ifelse(str_starts(outcome, \"emergenc|hospitalization\"), \"Emergency\",\n                     ifelse(str_starts(outcome, \"death|mortalit\"), \"Mortality\", NA))\n  ) %>%\n  distinct(outcome, .keep_all = TRUE) %>%\n  ungroup() %>%\n  nest(outcome = outcome) %>%\n  select(-abstract_title)\n\n\n\nWe identify outcomes considered in the study for 81.4067612% of the articles.\nSub-population considered\nUsing a similar methodology, we try to identify the sub-population considered (infants or elderly). Note that, when the whole population is studied, we do not recover any information. It might be a bit far fetch to consider that when no sub-population is mentioned, the whole population is studied. We therefore abstain from doing so.\n\n\narticles_subpop <- abstracts_with_titles %>% \n  mutate(\n    subpop = str_extract_all(\n      abstract_title, \n      \"(\\\\b(I|i)nfant|\\\\b(E|e)lder)\"\n    )\n  ) %>% \n  unnest(subpop, keep_empty = TRUE) %>% \n  group_by(doi) %>%\n   mutate(\n    subpop = tolower(subpop),\n    subpop = ifelse(str_starts(subpop, \"infant\"), \"Infants\",\n                     ifelse(str_starts(subpop, \"elder\"), \"Elders\", NA))\n  ) %>%\n  distinct(subpop, .keep_all = TRUE) %>%\n  ungroup() %>%\n  nest(subpop = subpop) %>%\n  select(-abstract_title)\n\n\n\nWe identify sub-population considered in the study for 12.9770992% of the articles.\nAdditional information on journals\nIt is also interesting to have access to journal fields. This will enable us to see whether some academic research fields are more subject to certain type of issues than others.\nWe retrieve information on journal fields from Scopus. In their source list, they classify all journals into approximately 330 sub-subject areas. We thus match this with journal names from our database. Scopus also provides coarser subject area categorizations, for instance one with 5 fields: Multidisciplinary, Physical Sciences, Health Sciences, Social Sciences and Life Science. They provide a correspondance table between those two classifications.\nNote that, some journals mention several of these fields as references. We choose to classify those as multidisciplinary journals.\n\n\nsubject_subsubject_corres <-\n  read_csv(\"R/Inputs/scopus_subject_corres.csv\") %>%\n  rename(\n    area_code = Code,\n    subsubject_area = Field, \n    subject_area = `Subject area`\n  ) %>%\n  drop_na()\n\njournal_subsubject_corres <- read_csv(\"R/Inputs/scopus_journal_subsubject.csv\") %>% \n  rename(\n    journal = Title,\n    subsubject_area = `Scopus Sub-Subject Area`,\n    area_code = `Scopus ASJC Code (Sub-subject Area)`\n  ) %>% \n  select(journal, subsubject_area, area_code)\n\n#To classify journals as multidisciplinary\njournal_subject_corres <- journal_subsubject_corres %>% \n  left_join(subject_subsubject_corres, by = c(\"area_code\", \"subsubject_area\")) %>% \n  select(journal, subject_area) %>% \n  distinct() %>% \n  group_by(journal) %>% \n  mutate(\n    n_subject_area = n(),\n    subject_area = ifelse(n_subject_area > 1, \"Multidisciplinary\", subject_area)\n  ) %>% \n  ungroup() %>% \n  select(-n_subject_area) %>% \n  distinct()\n\njournal_subject <- journal_subsubject_corres %>% \n  left_join(journal_subject_corres, by = c(\"journal\")) %>% \n  mutate(\n    journal_merge = str_to_lower(journal),\n    journal_merge = str_remove_all(journal_merge, \"[^\\\\w\\\\s]\")\n  )\n\narticles_journal_subject <- abstracts %>% \n  mutate(\n    journal_merge = str_to_lower(journal),\n    journal_merge = str_remove_all(journal_merge, \"[^\\\\w\\\\s]\")\n  ) %>% \n  left_join(journal_subject, by = \"journal_merge\") %>% \n  select(doi, subject_area, subsubject_area) %>% \n  distinct() %>% \n  nest(subsubject_area = c(subsubject_area)) \n\n\n\nWe retrieve information about the subject area for 87.7317339% articles and about the subsubject area for 94.1668846% of articles.\nAgregating the information\nFinally, we build the overall metadata set, by combining all the previous information.\n\n\nabstracts_and_metadata <- abstracts %>% \n  full_join(articles_n_obs, by = \"doi\") %>% \n  full_join(articles_pollutant, by = \"doi\") %>% \n  full_join(articles_outcome, by = \"doi\") %>% \n  full_join(articles_subpop, by = \"doi\") %>% \n  full_join(articles_journal_subject, by = \"doi\") %>% \n  group_by(doi) %>% \n  filter(\n    pub_date == max(pub_date, na.rm = TRUE) | \n      is.na(pub_date)\n  ) %>% #some articles have been published twice\n  ungroup()\n\n# saveRDS(abstracts_and_metadata, \"../Outputs/abstracts_and_metadata.RDS\")\n\n\n\n\n\n\n\n\nIn case any problem remains, we use tryCatch to record the DOIs corresponding to errors in order to be able to handle them later.↩︎\nWe analyze the characteristics of articles doing so in another document.↩︎\n",
      "last_modified": "2021-06-23T13:39:47+02:00"
    },
    {
      "path": "index.html",
      "title": "Inference Design in Studies of Acute Health Effects of Air Pollution",
      "description": "This website gathers code and additional material for the paper [Inference Design in Studies of Acute Health Effects of Air Pollution](paper.pdf) by Vincent Bagilet and Léo Zabrocki.\n",
      "author": [],
      "contents": "\nAll the (R) code for this project is available in its GitHub directory.\nAbstract\nWe explore statistical power issues of various empirical strategies implemented to estimate the short-term health effect of air pollution. Through an extensive literature review, we retrieve the estimates and standard errors of nearly all studies published on this topic, which rely both on standard outcomes regressions and causal inference methods. We find that a non-negligible share of studies may suffer from low power issues and could thereby exaggerate effect sizes. The analysis of published results highlights potential shortcomings of the literature but does not enable to precisely identify drivers of theses issues. We therefore run realistic simulations to investigate how statistical power varies with the treatment effect size, the number of observations, the proportion of treated units as well as the distribution of the outcome. Instrumental variable estimates, when statistically significant, are more likely to overestimate true effect sizes than naive estimates. Researchers should also pay attention to the number of treated units and the average count of health outcomes as they are important drivers of statistical power and may lead to greatly overestimated true effect sizes.\n\n\n\n",
      "last_modified": "2021-06-23T13:39:52+02:00"
    }
  ],
  "collections": []
}
